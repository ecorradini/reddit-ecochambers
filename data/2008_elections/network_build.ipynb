{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724ba0a5-2d3a-4930-a369-976649bf94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shared/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import multinetx as mnet\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9430c353-6569-4f43-9349-2ee60f8ac4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PARAMETERS & THRESHOLDS\n",
    "# --------------------------\n",
    "TAU_C = 0.39        # Threshold for content similarity (CSL)\n",
    "TAU_A = 0.99      # Threshold for affective similarity (ASL)\n",
    "OMEGA = 1.0        # Uniform interlayer coupling weight\n",
    "DELTA_T = 3600     # Time window (in seconds) for temporal burst calculation (e.g., 1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e231944-1dec-4359-8f19-1e1a501bfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "    \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16475f94-2f09-4797-aa81-f686b5b9c026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author_flair_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c05nibf</td>\n",
       "      <td>RAISEStheQuestion</td>\n",
       "      <td>1222819954</td>\n",
       "      <td>Dontcha just love the two-party system?</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_74i3j</td>\n",
       "      <td>politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c05nidf</td>\n",
       "      <td>JonAce</td>\n",
       "      <td>1222820188</td>\n",
       "      <td>http://www.senate.gov/general/contact_informat...</td>\n",
       "      <td>3</td>\n",
       "      <td>t3_74i4u</td>\n",
       "      <td>politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c05nif4</td>\n",
       "      <td>IrishJoe</td>\n",
       "      <td>1222820370</td>\n",
       "      <td>And McCain took credit for it passing before i...</td>\n",
       "      <td>1</td>\n",
       "      <td>t3_74i3j</td>\n",
       "      <td>politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c05nihm</td>\n",
       "      <td>starbork</td>\n",
       "      <td>1222820607</td>\n",
       "      <td>Wait, how can this possibly be?  Spending bill...</td>\n",
       "      <td>3</td>\n",
       "      <td>t1_c05nidf</td>\n",
       "      <td>politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c05nime</td>\n",
       "      <td>mutatron</td>\n",
       "      <td>1222821155</td>\n",
       "      <td>&amp;gt; Find out on October 29, when the Weekly R...</td>\n",
       "      <td>2</td>\n",
       "      <td>t3_74i69</td>\n",
       "      <td>politics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             author  created_utc  \\\n",
       "0  c05nibf  RAISEStheQuestion   1222819954   \n",
       "1  c05nidf             JonAce   1222820188   \n",
       "2  c05nif4           IrishJoe   1222820370   \n",
       "3  c05nihm           starbork   1222820607   \n",
       "4  c05nime           mutatron   1222821155   \n",
       "\n",
       "                                                body  score   parent_id  \\\n",
       "0            Dontcha just love the two-party system?      1    t3_74i3j   \n",
       "1  http://www.senate.gov/general/contact_informat...      3    t3_74i4u   \n",
       "2  And McCain took credit for it passing before i...      1    t3_74i3j   \n",
       "3  Wait, how can this possibly be?  Spending bill...      3  t1_c05nidf   \n",
       "4  &gt; Find out on October 29, when the Weekly R...      2    t3_74i69   \n",
       "\n",
       "  subreddit  permalink  author_flair_text  \n",
       "0  politics        NaN                NaN  \n",
       "1  politics        NaN                NaN  \n",
       "2  politics        NaN                NaN  \n",
       "3  politics        NaN                NaN  \n",
       "4  politics        NaN                NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1504266-9867-48d8-a6f7-26451ad721ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16474\n"
     ]
    }
   ],
   "source": [
    "all_authors = list(submissions['author'])\n",
    "all_authors.extend(list(comments['author']))\n",
    "all_authors = list(set(all_authors))\n",
    "print(len(all_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6151c422-d05f-4c79-97c8-504e76760265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIL nodes: 16474\n",
      "UIL edges: 46827\n"
     ]
    }
   ],
   "source": [
    "### Build UIL\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            if edge in edges:\n",
    "                edges[edge] += 1\n",
    "            else:\n",
    "                edges[edge] = 1    \n",
    "\n",
    "    return edges\n",
    "\n",
    "UIL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    UIL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    UIL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"UIL nodes: {len(UIL.nodes())}\")\n",
    "print(f\"UIL edges: {len(UIL.edges())}\")\n",
    "print(f\"UIL density: {nx.density(UIL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(UIL)\n",
    "print(f\"Average UIL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('uil.pkl', 'wb') as f:\n",
    "    pickle.dump(UIL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f9bc5a0-4447-4d07-938e-abea1df69253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDL nodes: 16474\n",
      "TDL edges: 46827\n"
     ]
    }
   ],
   "source": [
    "### Build TDL\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    \"\"\"\n",
    "    Given a sorted list of timestamps (in seconds), compute the maximum number of events\n",
    "    occurring within any window of length `window`.\n",
    "    \"\"\"\n",
    "    if len(timestamps) == 0:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            timestamp = comment.created_utc\n",
    "            \n",
    "            if edge in edges:\n",
    "                edges[edge].append(timestamp)\n",
    "            else:\n",
    "                edges[edge] = [timestamp]   \n",
    "\n",
    "    return edges\n",
    "\n",
    "TDL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    TDL.add_node(author)\n",
    "\n",
    "for edge, times in get_edges().items():\n",
    "    times_sorted = sorted(times)\n",
    "    burst_weight = compute_sliding_window_max(times_sorted, window=DELTA_T)\n",
    "    TDL.add_edge(edge[0], edge[1], weight=burst_weight)\n",
    "\n",
    "print(f\"TDL nodes: {len(TDL.nodes())}\")\n",
    "print(f\"TDL edges: {len(TDL.edges())}\")\n",
    "print(f\"TDL density: {nx.density(TDL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(TDL)\n",
    "print(f\"Average TDL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('tdl.pkl', 'wb') as f:\n",
    "    pickle.dump(TDL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c5f744-0263-4d82-bf4c-4dc1d85b8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLEANING FUNCTION\n",
    "# --------------------------\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------\n",
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "\n",
    "submissions = pd.read_csv(\"submissions.csv.zst\")\n",
    "comments = pd.read_csv(\"comments.csv.zst\")\n",
    "\n",
    "\n",
    "# CLS/ASL Preprocessing\n",
    "# --------------------------\n",
    "# AGGREGATE TEXTS PER AUTHOR\n",
    "# --------------------------\n",
    "def aggregate_texts(df, text_fields):\n",
    "    \"\"\"Concatenate selected fields into a single string per row.\"\"\"\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions[\"full_text\"] = aggregate_texts(submissions, [\"title\", \"selftext\"])\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "submissions[\"selftext\"] = submissions[\"selftext\"].fillna(\"\")\n",
    "\n",
    "contents_per_author = {}\n",
    "\n",
    "# Process submissions\n",
    "for submission in submissions.itertuples():\n",
    "    author = submission.author\n",
    "    full_text = clean(f\"{submission.title} {submission.selftext}\")\n",
    "    contents_per_author.setdefault(author, []).append(full_text)\n",
    "\n",
    "# Process comments\n",
    "for comment in comments.itertuples():\n",
    "    author = comment.author\n",
    "    body = clean(comment.body)\n",
    "    contents_per_author.setdefault(author, []).append(body)\n",
    "\n",
    "# Merge all texts for each author\n",
    "for author, contents in contents_per_author.items():\n",
    "    contents_per_author[author] = \" \".join(contents)\n",
    "\n",
    "authors = list(contents_per_author.keys())\n",
    "contents = list(contents_per_author.values())\n",
    "\n",
    "os.makedirs('cslasl-pre', exist_ok=True)\n",
    "with open('cslasl-pre/authors.pkl', 'wb') as f:\n",
    "    pickle.dump(authors, f)\n",
    "with open('cslasl-pre/contents.pkl', 'wb') as f:\n",
    "    pickle.dump(contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ecdf08-c55d-44b3-9c50-f36ff4421e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSL edges and with tau_c at 0.3, then compute estimated tau_c\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True, device=\"cuda:1\")\n",
    "embeddings = model.encode(contents, normalize=True)\n",
    "with open('cslasl-pre/csl_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4f39ab-6db8-4f51-8dfc-62ea468c34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:  60%|███████████████████████▎               | 153/256 [02:41<01:48,  1.05s/batch]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Compute cosine similarity in **batches** to prevent MemoryError\n",
    "num_users = len(authors)\n",
    "batch_size = 1000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "total_batches = (num_users // batch_size) ** 2  # Total iterations (i, j) pairs\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Computing Similarities\", unit=\"batch\")\n",
    "\n",
    "for i in range(0, num_users, batch_size):\n",
    "    edges = {}\n",
    "    end_i = min(i + batch_size, num_users)\n",
    "    \n",
    "    for j in range(i, num_users, batch_size):\n",
    "        end_j = min(j + batch_size, num_users)\n",
    "        \n",
    "        # Compute batch-wise similarity\n",
    "        similarity_batch = np.dot(embeddings[i:end_i], embeddings[j:end_j].T)\n",
    "        \n",
    "        # Extract upper triangle indices to avoid duplicate calculations\n",
    "        if i == j:\n",
    "            upper_tri_indices = np.triu_indices(end_i - i, k=1)\n",
    "            similarity_values = similarity_batch[upper_tri_indices]\n",
    "        else:\n",
    "            similarity_values = similarity_batch.flatten()\n",
    "\n",
    "        # Store only meaningful similarities\n",
    "        for x, sim in enumerate(similarity_values):\n",
    "            if sim > 0.3:  # Adjust threshold as needed\n",
    "                author_1 = authors[i + x // (end_j - j)]\n",
    "                author_2 = authors[j + x % (end_j - j)]\n",
    "                edges[(author_1, author_2)] = sim\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "                \n",
    "    # Save edges to a file\n",
    "    os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "    with open(f\"cslasl-pre/edges/edges_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges, f)\n",
    "    del edges\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc31b6b5-df21-4f18-8c29-3210bd7ab620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_c (90th percentile): 0.51\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "\n",
    "def custom_round(num):\n",
    "    \"\"\"\n",
    "    Ceil if the third digit after the decimal is >= 5, otherwise floor.\n",
    "    Always returns a number with two decimal places.\n",
    "    \"\"\"\n",
    "    # Convert to string and split on '.'\n",
    "    num_str = f\"{num:.10f}\"  # Ensure precision to check third decimal\n",
    "    integer_part, decimal_part = num_str.split('.')\n",
    "\n",
    "    # Extract the relevant digits\n",
    "    third_digit = int(decimal_part[2]) if len(decimal_part) > 2 else 0\n",
    "\n",
    "    # Apply ceil or floor rounding\n",
    "    if third_digit >= 5:\n",
    "        return math.ceil(num * 100) / 100  # Round up\n",
    "    else:\n",
    "        return math.floor(num * 100) / 100\n",
    "\n",
    "\n",
    "# Compute empirical threshold\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"*.pkl\"))\n",
    "\n",
    "similarity_values = []\n",
    "\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    similarity_values.extend(current.values())\n",
    "    del current\n",
    "\n",
    "TAU_C = custom_round(np.percentile(similarity_values, 90))  # 90th percentile\n",
    "print(\"Estimated tau_c (90th percentile):\", TAU_C)\n",
    "\n",
    "all_edges = {}\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    for key, value in current.items():\n",
    "        if value > TAU_C:\n",
    "            all_edges[key] = value\n",
    "    del current\n",
    "\n",
    "with open(f\"cslasl-pre/edges/all_edges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4898f898-0a3d-4e06-98b6-f8c9df290395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSL nodes: 16474\n",
      "CSL edges: 3764849\n",
      "CSL density: 0.02774634601157842\n",
      "Average CSL Edge Weight: 0.5642\n"
     ]
    }
   ],
   "source": [
    "### Build CSL\n",
    "with open(\"cslasl-pre/edges/all_edges.pkl\", \"rb\") as f:\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "CSL = nx.Graph()\n",
    "for author in authors:\n",
    "    CSL.add_node(author)\n",
    "\n",
    "for edge, weight in edges.items():\n",
    "    CSL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"CSL nodes: {len(CSL.nodes())}\")\n",
    "print(f\"CSL edges: {len(CSL.edges())}\")\n",
    "print(f\"CSL density: {nx.density(CSL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(CSL)\n",
    "print(f\"Average CSL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('csl.pkl', 'wb') as f:\n",
    "    pickle.dump(CSL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adf0f08f-b8cf-4da9-9b42-22021e41f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ASL edges and with tau_a at 0.5, then compute estimated tau_a\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "\n",
    "def get_sentiment_vector(text):\n",
    "    \"\"\"Compute sentiment vector from RoBERTa model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).logits  # Get logits\n",
    "    return output.squeeze().cpu().numpy()\n",
    "\n",
    "authors_sentiments = {}\n",
    "for i in range(0, len(authors)):\n",
    "    authors_sentiments[authors[i]] = get_sentiment_vector(contents[i])\n",
    "\n",
    "with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'wb') as f:\n",
    "    pickle.dump(authors_sentiments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4291b1ef-873d-4b94-a58a-4490a688e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment vectors to NumPy\n",
    "sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "\n",
    "with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'wb') as f:\n",
    "    pickle.dump(normalized_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28c6bc1a-c1fa-46a8-b014-fad656a436ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:   2%|▉                                        | 6/256 [00:07<05:30,  1.32s/batch]\n",
      "Computing Similarities:  60%|███████████████████████▎               | 153/256 [04:39<02:09,  1.25s/batch]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Compute cosine similarity in **batches** to prevent MemoryError\n",
    "num_users = len(authors)\n",
    "batch_size = 1000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "total_batches = (num_users // batch_size) ** 2  # Total iterations (i, j) pairs\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Computing Similarities\", unit=\"batch\")\n",
    "\n",
    "# Compute cosine similarity in **batches** for sentiment analysis\n",
    "for i in range(0, num_users, batch_size):\n",
    "    edges_sentiment = {}\n",
    "    end_i = min(i + batch_size, num_users)\n",
    "    \n",
    "    for j in range(i, num_users, batch_size):\n",
    "        end_j = min(j + batch_size, num_users)\n",
    "        \n",
    "        # Compute batch-wise similarity\n",
    "        similarity_batch = np.dot(normalized_matrix[i:end_i], normalized_matrix[j:end_j].T)\n",
    "        \n",
    "        # Extract upper triangle indices to avoid duplicate calculations\n",
    "        if i == j:\n",
    "            upper_tri_indices = np.triu_indices(end_i - i, k=1)\n",
    "            similarity_values = similarity_batch[upper_tri_indices]\n",
    "        else:\n",
    "            similarity_values = similarity_batch.flatten()\n",
    "\n",
    "        # Store only meaningful similarities\n",
    "        for x, sim in enumerate(similarity_values):\n",
    "            if sim > 0.5:  # Adjust threshold as needed\n",
    "                author_1 = authors[i + x // (end_j - j)]\n",
    "                author_2 = authors[j + x % (end_j - j)]\n",
    "                edges_sentiment[(author_1, author_2)] = sim\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "                \n",
    "    # Save edges to a file\n",
    "    os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "    with open(f\"cslasl-pre/edges/edges_sentiments_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges_sentiment, f)\n",
    "    del edges_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cf6a7e5-60d5-4997-8572-e97aa6a64e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_a (90th percentile): 0.997\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "\n",
    "def custom_round(num):\n",
    "    \"\"\"\n",
    "    Ceil if the fourth digit after the decimal is >= 5, otherwise floor.\n",
    "    Always returns a number with three decimal places.\n",
    "    \"\"\"\n",
    "    # Convert number to string for precise control\n",
    "    num_str = f\"{num:.10f}\"  # Ensure precision to check the fourth decimal\n",
    "    integer_part, decimal_part = num_str.split('.')\n",
    "\n",
    "    # Extract digits\n",
    "    third_digit = int(decimal_part[2]) if len(decimal_part) > 2 else 0\n",
    "    fourth_digit = int(decimal_part[3]) if len(decimal_part) > 3 else 0  # Fourth digit\n",
    "\n",
    "    # Compute the base rounded value (3 decimal places)\n",
    "    base_value = math.floor(num * 1000) / 1000  # Default floor rounding\n",
    "\n",
    "    # Only increase within the 3 decimal boundary\n",
    "    if fourth_digit >= 5:\n",
    "        adjusted_value = base_value + 0.001  # Move up without crossing integer boundary\n",
    "        return min(adjusted_value, math.floor(num * 100) / 100)  # Prevent rounding to unintended values\n",
    "\n",
    "    return base_value  # Default floored value\n",
    "\n",
    "\n",
    "# Compute empirical threshold\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"edges_sentiments_*.pkl\"))\n",
    "\n",
    "similarity_values = []\n",
    "\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)    \n",
    "    similarity_values.extend(current.values())\n",
    "    del current\n",
    "\n",
    "TAU_A = custom_round(np.percentile(p, 90))  # 90th percentile\n",
    "print(\"Estimated tau_a (90th percentile):\", TAU_A)\n",
    "\n",
    "del similarity_values\n",
    "\n",
    "all_edges = {}\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    for key, value in current.items():\n",
    "        if value > TAU_A:\n",
    "            all_edges[key] = value\n",
    "    del current\n",
    "\n",
    "with open(f\"cslasl-pre/edges/all_edges_asl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d719f477-e476-4338-ae8a-e7d5ff134546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL nodes: 16474\n",
      "ASL edges: 8917269\n",
      "ASL density: 0.06571887243082575\n",
      "Average ASL Edge Weight: 1.0000\n"
     ]
    }
   ],
   "source": [
    "### Build ASL\n",
    "with open(\"cslasl-pre/edges/all_edges_asl.pkl\", \"rb\") as f:\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "ASL = nx.Graph()\n",
    "for author in authors:\n",
    "    ASL.add_node(author)\n",
    "\n",
    "for edge, weight in edges.items():\n",
    "    ASL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"ASL nodes: {len(ASL.nodes())}\")\n",
    "print(f\"ASL edges: {len(ASL.edges())}\")\n",
    "print(f\"ASL density: {nx.density(ASL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(ASL)\n",
    "print(f\"Average ASL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('asl.pkl', 'wb') as f:\n",
    "    pickle.dump(ASL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cef323bb-c585-4710-8144-bc51678a23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uil.pkl', 'rb') as f:\n",
    "    UIL = pickle.load(f)\n",
    "with open('csl.pkl', 'rb') as f:\n",
    "    CSL = pickle.load(f)\n",
    "with open('tdl.pkl', 'rb') as f:\n",
    "    TDL = pickle.load(f)\n",
    "with open('asl.pkl', 'rb') as f:\n",
    "    ASL = pickle.load(f)\n",
    "\n",
    "layers = {\n",
    "    \"UIL\": UIL,\n",
    "    \"CSL\": CSL,\n",
    "    \"TDL\": TDL,\n",
    "    \"ASL\": ASL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22868134-26c6-4b09-a0dc-41767d32419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network constructed with 98844 interlayer edges.\n"
     ]
    }
   ],
   "source": [
    "multilayer = mnet.MultilayerGraph()\n",
    "\n",
    "# Add nodes from UIL (the same for all layers).\n",
    "for layer_name, G in layers.items():\n",
    "    for node in G.nodes():\n",
    "        multilayer.add_node((node, layer_name))\n",
    "\n",
    "# Add intralayer edges.\n",
    "for layer_name, G in layers.items():\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        multilayer.add_edge((u, layer_name), (v, layer_name), weight=data.get('weight', 1))\n",
    "\n",
    "# Add interlayer edges.\n",
    "all_users = set()\n",
    "for G in layers.values():\n",
    "    all_users.update(G.nodes())\n",
    "layer_names = list(layers.keys())\n",
    "for user in all_users:\n",
    "    for i in range(len(layer_names)):\n",
    "        for j in range(i+1, len(layer_names)):\n",
    "            multilayer.add_edge((user, layer_names[i]), (user, layer_names[j]), weight=OMEGA)\n",
    "\n",
    "interlayer_edges = [e for e in multilayer.edges() if e[0][1] != e[1][1]]\n",
    "num_interlayer_edges = len(interlayer_edges)\n",
    "\n",
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afc82b34-93ec-47af-a6f0-e49f8f6cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network saved to 'multilayer_network.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# SAVE THE MULTILAYER NETWORK\n",
    "# --------------------------\n",
    "with open('network.pkl', 'wb') as f:\n",
    "    pickle.dump(multilayer, f)\n",
    "print(\"Multilayer network saved to 'multilayer_network.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
