{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724ba0a5-2d3a-4930-a369-976649bf94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shared/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import multinetx as mnet\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430c353-6569-4f43-9349-2ee60f8ac4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PARAMETERS & THRESHOLDS\n",
    "# --------------------------\n",
    "TAU_C = 0.39        # Threshold for content similarity (CSL)\n",
    "TAU_A = 0.99      # Threshold for affective similarity (ASL)\n",
    "OMEGA = 1.0        # Uniform interlayer coupling weight\n",
    "DELTA_T = 3600     # Time window (in seconds) for temporal burst calculation (e.g., 1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e231944-1dec-4359-8f19-1e1a501bfd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691390/102445958.py:7: DtypeWarning: Columns (2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  submissions = pd.read_csv('submissions.csv.zst')\n"
     ]
    }
   ],
   "source": [
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "    \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1504266-9867-48d8-a6f7-26451ad721ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573839\n"
     ]
    }
   ],
   "source": [
    "all_authors = list(submissions['author'])\n",
    "all_authors.extend(list(comments['author']))\n",
    "all_authors = list(set(all_authors))\n",
    "print(len(all_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151c422-d05f-4c79-97c8-504e76760265",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build UIL\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            if edge in edges:\n",
    "                edges[edge] += 1\n",
    "            else:\n",
    "                edges[edge] = 1    \n",
    "\n",
    "    return edges\n",
    "\n",
    "UIL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    UIL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    UIL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"UIL nodes: {len(UIL.nodes())}\")\n",
    "print(f\"UIL edges: {len(UIL.edges())}\")\n",
    "print(f\"UIL density: {nx.density(UIL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(UIL)\n",
    "print(f\"Average UIL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('uil.pkl', 'wb') as f:\n",
    "    pickle.dump(UIL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c566071-e447-4c4c-ad87-98c3a839efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSL nodes: 16474\n",
      "CSL edges: 12208150\n"
     ]
    }
   ],
   "source": [
    "### Build CSL\n",
    "\n",
    "def aggregate_texts(df, text_fields):\n",
    "    # Concatenate selected fields into a single string per row.\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions['full_text'] = aggregate_texts(submissions, ['title', 'selftext'])\n",
    "comments['body'] = comments['body'].fillna(\"\")\n",
    "\n",
    "def get_edges():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    def clean(text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    edges = {}\n",
    "    \n",
    "    contents_per_author = {}\n",
    "    for submission in submissions.itertuples():\n",
    "        author = submission.author\n",
    "        title = submission.title\n",
    "        selftext = submission.selftext\n",
    "        full_text = f\"{title} {selftext}\"\n",
    "        full_text = clean(full_text)\n",
    "        \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(full_text)\n",
    "        else:\n",
    "            contents_per_author[author] = [full_text]\n",
    "    \n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        body = comment.body\n",
    "        body = clean(body)\n",
    "    \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(body)\n",
    "        else:\n",
    "            contents_per_author[author] = [body]\n",
    "    \n",
    "    for author, contents in contents_per_author.items():\n",
    "        contents_per_author[author] = \" \".join(contents_per_author[author])\n",
    "    \n",
    "    authors = list(contents_per_author.keys())\n",
    "    contents = list(contents_per_author.values())\n",
    "    \n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', trust_remote_code=True, device=\"cuda:1\")\n",
    "    embeddings = model.encode(contents, normalize=True)\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)  # Dot product of normalized vectors\n",
    "    indices = np.triu_indices(len(authors), k=1)  # Get indices of the upper triangle (excluding diagonal)\n",
    "    \n",
    "    author_pairs = [(authors[i], authors[j]) for i, j in zip(indices[0], indices[1])]\n",
    "    similarities = similarity_matrix[indices]  # Fetch corresponding similarity values\n",
    "\n",
    "    # Store edges where similarity > TAU_C\n",
    "    edges = {pair: sim for pair, sim in zip(author_pairs, similarities) if sim > TAU_C}\n",
    "    \n",
    "    return edges\n",
    "\n",
    "CSL = nx.Graph()\n",
    "for author in all_authors:\n",
    "    CSL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    CSL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"CSL nodes: {len(CSL.nodes())}\")\n",
    "print(f\"CSL edges: {len(CSL.edges())}\")\n",
    "print(f\"CSL density: {nx.density(CSL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(CSL)\n",
    "print(f\"Average CSL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('csl.pkl', 'wb') as f:\n",
    "    pickle.dump(CSL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f9bc5a0-4447-4d07-938e-abea1df69253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDL nodes: 16474\n",
      "TDL edges: 46827\n"
     ]
    }
   ],
   "source": [
    "### Build TDL\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    \"\"\"\n",
    "    Given a sorted list of timestamps (in seconds), compute the maximum number of events\n",
    "    occurring within any window of length `window`.\n",
    "    \"\"\"\n",
    "    if len(timestamps) == 0:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            timestamp = comment.created_utc\n",
    "            \n",
    "            if edge in edges:\n",
    "                edges[edge].append(timestamp)\n",
    "            else:\n",
    "                edges[edge] = [timestamp]   \n",
    "\n",
    "    return edges\n",
    "\n",
    "TDL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    TDL.add_node(author)\n",
    "\n",
    "for edge, times in get_edges().items():\n",
    "    times_sorted = sorted(times)\n",
    "    burst_weight = compute_sliding_window_max(times_sorted, window=DELTA_T)\n",
    "    TDL.add_edge(edge[0], edge[1], weight=burst_weight)\n",
    "\n",
    "print(f\"TDL nodes: {len(TDL.nodes())}\")\n",
    "print(f\"TDL edges: {len(TDL.edges())}\")\n",
    "print(f\"TDL density: {nx.density(TDL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(TDL)\n",
    "print(f\"Average TDL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('tdl.pkl', 'wb') as f:\n",
    "    pickle.dump(TDL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c65f027-3d59-43d0-b494-aefc3db0a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL nodes: 16474\n",
      "ASL edges: 15248971\n"
     ]
    }
   ],
   "source": [
    "### Build ASL\n",
    "\n",
    "def aggregate_texts(df, text_fields):\n",
    "    # Concatenate selected fields into a single string per row.\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions['full_text'] = aggregate_texts(submissions, ['title', 'selftext'])\n",
    "comments['full_text'] = comments['body'].fillna(\"\")\n",
    "\n",
    "def get_edges():\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    \n",
    "    def clean(text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip().lower()\n",
    "        \n",
    "    edges = {}\n",
    "    \n",
    "    contents_per_author = {}\n",
    "    for submission in submissions.itertuples():\n",
    "        author = submission.author\n",
    "        title = submission.title\n",
    "        selftext = submission.selftext\n",
    "        full_text = f\"{title} {selftext}\"\n",
    "        full_text = clean(full_text)\n",
    "        \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(full_text)\n",
    "        else:\n",
    "            contents_per_author[author] = [full_text]\n",
    "    \n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        body = comment.body\n",
    "        body = clean(body)\n",
    "    \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(body)\n",
    "        else:\n",
    "            contents_per_author[author] = [body]\n",
    "    \n",
    "    for author, contents in contents_per_author.items():\n",
    "        contents_per_author[author] = \" \".join(contents_per_author[author])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "\n",
    "    def get_sentiment_vector(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")  # Move input to GPU\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits  # Get logits\n",
    "        return output.squeeze().cpu().numpy()\n",
    "\n",
    "    authors_sentiments = {}\n",
    "    for author, contents in contents_per_author.items():\n",
    "        authors_sentiments[author] = get_sentiment_vector(contents)\n",
    "         \n",
    "    sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "    norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "    valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "    normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "    normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "    similarity_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "\n",
    "    edges = {}\n",
    "    indices = np.triu_indices(len(authors), k=1)  # Get upper triangle indices\n",
    "    \n",
    "    author_pairs = [(authors[i], authors[j]) for i, j in zip(indices[0], indices[1])]\n",
    "    similarities = similarity_matrix[indices]  # Fetch similarity values\n",
    "    \n",
    "    # Store Edges Where Similarity > TAU_A\n",
    "    edges = {pair: sim for pair, sim in zip(author_pairs, similarities) if sim > TAU_A}\n",
    "    \n",
    "    return edges\n",
    "\n",
    "ASL = nx.Graph()\n",
    "for author in all_authors:\n",
    "    ASL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    ASL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"ASL nodes: {len(ASL.nodes())}\")\n",
    "print(f\"ASL edges: {len(ASL.edges())}\")\n",
    "print(f\"ASL density: {nx.density(ASL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(ASL)\n",
    "print(f\"Average ASL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('asl.pkl', 'wb') as f:\n",
    "    pickle.dump(ASL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cef323bb-c585-4710-8144-bc51678a23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uil.pkl', 'rb') as f:\n",
    "    UIL = pickle.load(f)\n",
    "with open('csl.pkl', 'rb') as f:\n",
    "    CSL = pickle.load(f)\n",
    "with open('tdl.pkl', 'rb') as f:\n",
    "    TDL = pickle.load(f)\n",
    "with open('asl.pkl', 'rb') as f:\n",
    "    ASL = pickle.load(f)\n",
    "\n",
    "layers = {\n",
    "    \"UIL\": UIL,\n",
    "    \"CSL\": CSL,\n",
    "    \"TDL\": TDL,\n",
    "    \"ASL\": ASL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "22868134-26c6-4b09-a0dc-41767d32419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multilayer = mnet.MultilayerGraph()\n",
    "\n",
    "# Add nodes from UIL (the same for all layers).\n",
    "for layer_name, G in layers.items():\n",
    "    for node in G.nodes():\n",
    "        multilayer.add_node((node, layer_name))\n",
    "\n",
    "# Add intralayer edges.\n",
    "for layer_name, G in layers.items():\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        multilayer.add_edge((u, layer_name), (v, layer_name), weight=data.get('weight', 1))\n",
    "\n",
    "# Add interlayer edges.\n",
    "all_users = set()\n",
    "for G in layers.values():\n",
    "    all_users.update(G.nodes())\n",
    "layer_names = list(layers.keys())\n",
    "for user in all_users:\n",
    "    for i in range(len(layer_names)):\n",
    "        for j in range(i+1, len(layer_names)):\n",
    "            multilayer.add_edge((user, layer_names[i]), (user, layer_names[j]), weight=OMEGA)\n",
    "\n",
    "interlayer_edges = [e for e in multilayer.edges() if e[0][1] != e[1][1]]\n",
    "num_interlayer_edges = len(interlayer_edges)\n",
    "\n",
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "afc82b34-93ec-47af-a6f0-e49f8f6cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network saved to 'multilayer_network.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# SAVE THE MULTILAYER NETWORK\n",
    "# --------------------------\n",
    "with open('network.pkl', 'wb') as f:\n",
    "    pickle.dump(multilayer, f)\n",
    "print(\"Multilayer network saved to 'multilayer_network.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6f0dd23c-1521-4950-92a2-773bf2d9775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network constructed with 98844 interlayer edges.\n"
     ]
    }
   ],
   "source": [
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d019-7ef6-40ed-98ca-a6e944f52c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
