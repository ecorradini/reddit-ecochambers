{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724ba0a5-2d3a-4930-a369-976649bf94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shared/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import multinetx as mnet\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430c353-6569-4f43-9349-2ee60f8ac4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PARAMETERS & THRESHOLDS\n",
    "# --------------------------\n",
    "TAU_C = 0.39        # Threshold for content similarity (CSL)\n",
    "TAU_A = 0.99      # Threshold for affective similarity (ASL)\n",
    "OMEGA = 1.0        # Uniform interlayer coupling weight\n",
    "DELTA_T = 3600     # Time window (in seconds) for temporal burst calculation (e.g., 1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e231944-1dec-4359-8f19-1e1a501bfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "    \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1504266-9867-48d8-a6f7-26451ad721ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380704\n"
     ]
    }
   ],
   "source": [
    "all_authors = list(submissions['author'])\n",
    "all_authors.extend(list(comments['author']))\n",
    "all_authors = list(set(all_authors))\n",
    "print(len(all_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6151c422-d05f-4c79-97c8-504e76760265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIL nodes: 380704\n",
      "UIL edges: 1409660\n",
      "UIL density: 9.726142707446654e-06\n",
      "Average UIL Edge Weight: 1.2266\n"
     ]
    }
   ],
   "source": [
    "### Build UIL\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            if edge in edges:\n",
    "                edges[edge] += 1\n",
    "            else:\n",
    "                edges[edge] = 1    \n",
    "\n",
    "    return edges\n",
    "\n",
    "UIL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    UIL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    UIL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"UIL nodes: {len(UIL.nodes())}\")\n",
    "print(f\"UIL edges: {len(UIL.edges())}\")\n",
    "print(f\"UIL density: {nx.density(UIL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(UIL)\n",
    "print(f\"Average UIL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('uil.pkl', 'wb') as f:\n",
    "    pickle.dump(UIL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9bc5a0-4447-4d07-938e-abea1df69253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDL nodes: 380704\n",
      "TDL edges: 1409660\n",
      "TDL density: 9.726142707446654e-06\n",
      "Average TDL Edge Weight: 1.0880\n"
     ]
    }
   ],
   "source": [
    "### Build TDL\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    \"\"\"\n",
    "    Given a sorted list of timestamps (in seconds), compute the maximum number of events\n",
    "    occurring within any window of length `window`.\n",
    "    \"\"\"\n",
    "    if len(timestamps) == 0:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            timestamp = comment.created_utc\n",
    "            \n",
    "            if edge in edges:\n",
    "                edges[edge].append(timestamp)\n",
    "            else:\n",
    "                edges[edge] = [timestamp]   \n",
    "\n",
    "    return edges\n",
    "\n",
    "TDL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    TDL.add_node(author)\n",
    "\n",
    "for edge, times in get_edges().items():\n",
    "    times_sorted = sorted(times)\n",
    "    burst_weight = compute_sliding_window_max(times_sorted, window=DELTA_T)\n",
    "    TDL.add_edge(edge[0], edge[1], weight=burst_weight)\n",
    "\n",
    "print(f\"TDL nodes: {len(TDL.nodes())}\")\n",
    "print(f\"TDL edges: {len(TDL.edges())}\")\n",
    "print(f\"TDL density: {nx.density(TDL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(TDL)\n",
    "print(f\"Average TDL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('tdl.pkl', 'wb') as f:\n",
    "    pickle.dump(TDL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e90dd9-119f-4acd-8ceb-1f07d0547b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLEANING FUNCTION\n",
    "# --------------------------\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------\n",
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "\n",
    "submissions = pd.read_csv(\"submissions.csv.zst\")\n",
    "comments = pd.read_csv(\"comments.csv.zst\")\n",
    "\n",
    "\n",
    "# CLS/ASL Preprocessing\n",
    "# --------------------------\n",
    "# AGGREGATE TEXTS PER AUTHOR\n",
    "# --------------------------\n",
    "def aggregate_texts(df, text_fields):\n",
    "    \"\"\"Concatenate selected fields into a single string per row.\"\"\"\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions[\"full_text\"] = aggregate_texts(submissions, [\"title\", \"selftext\"])\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "submissions[\"selftext\"] = submissions[\"selftext\"].fillna(\"\")\n",
    "\n",
    "contents_per_author = {}\n",
    "\n",
    "# Process submissions\n",
    "for submission in submissions.itertuples():\n",
    "    author = submission.author\n",
    "    full_text = clean(f\"{submission.title} {submission.selftext}\")\n",
    "    contents_per_author.setdefault(author, []).append(full_text)\n",
    "\n",
    "# Process comments\n",
    "for comment in comments.itertuples():\n",
    "    author = comment.author\n",
    "    body = clean(comment.body)\n",
    "    contents_per_author.setdefault(author, []).append(body)\n",
    "\n",
    "# Merge all texts for each author\n",
    "for author, contents in contents_per_author.items():\n",
    "    contents_per_author[author] = \" \".join(contents)\n",
    "\n",
    "authors = list(contents_per_author.keys())\n",
    "contents = list(contents_per_author.values())\n",
    "\n",
    "os.makedirs('cslasl-pre', exist_ok=True)\n",
    "with open('cslasl-pre/authors.pkl', 'wb') as f:\n",
    "    pickle.dump(authors, f)\n",
    "with open('cslasl-pre/contents.pkl', 'wb') as f:\n",
    "    pickle.dump(contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6ad017-69da-4063-a92b-600c6f80fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSL edges and with tau_c at 0.3, then compute estimated tau_c\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True, device=\"cuda:1\")\n",
    "embeddings = model.encode(contents, normalize=True)\n",
    "with open('cslasl-pre/csl_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a674a4-f3a2-4c7d-875b-4164f103abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:  50%|█████████████▌             | 1813560/3621409 [9:16:23<9:14:38, 54.33batch/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"cslasl-pre/csl_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "# Compute cosine similarity in **batches** to prevent MemoryError\n",
    "num_users = len(authors)\n",
    "batch_size = 200  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "total_batches = (num_users // batch_size) ** 2  # Total iterations (i, j) pairs\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Computing Similarities\", unit=\"batch\")\n",
    "\n",
    "for i in range(0, num_users, batch_size):\n",
    "    edges = {}\n",
    "    end_i = min(i + batch_size, num_users)\n",
    "    \n",
    "    for j in range(i, num_users, batch_size):\n",
    "        end_j = min(j + batch_size, num_users)\n",
    "        \n",
    "        # Compute batch-wise similarity\n",
    "        similarity_batch = np.dot(embeddings[i:end_i], embeddings[j:end_j].T)\n",
    "        \n",
    "        # Extract upper triangle indices to avoid duplicate calculations\n",
    "        if i == j:\n",
    "            upper_tri_indices = np.triu_indices(end_i - i, k=1)\n",
    "            similarity_values = similarity_batch[upper_tri_indices]\n",
    "        else:\n",
    "            similarity_values = similarity_batch.flatten()\n",
    "\n",
    "        # Store only meaningful similarities\n",
    "        for x, sim in enumerate(similarity_values):\n",
    "            if sim > 0.3:  # Adjust threshold as needed\n",
    "                author_1 = authors[i + x // (end_j - j)]\n",
    "                author_2 = authors[j + x % (end_j - j)]\n",
    "                edges[(author_1, author_2)] = sim\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "                \n",
    "    # Save edges to a file\n",
    "    os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "    with open(f\"cslasl-pre/edges/edges_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges, f)\n",
    "    del edges\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316ca22-8dc4-4862-8f1e-16c2ab53f97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing TAU_C:  23%|████████▎                           | 442/1904 [20:52:44<43:05:34, 106.11s/it]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "from tdigest import TDigest\n",
    "from tqdm import tqdm\n",
    "\n",
    "digest = TDigest()\n",
    "\n",
    "def custom_round(num):\n",
    "    \"\"\"\n",
    "    Ceil if the third digit after the decimal is >= 5, otherwise floor.\n",
    "    Always returns a number with two decimal places.\n",
    "    \"\"\"\n",
    "    # Convert to string and split on '.'\n",
    "    num_str = f\"{num:.10f}\"  # Ensure precision to check third decimal\n",
    "    integer_part, decimal_part = num_str.split('.')\n",
    "\n",
    "    # Extract the relevant digits\n",
    "    third_digit = int(decimal_part[2]) if len(decimal_part) > 2 else 0\n",
    "\n",
    "    # Apply ceil or floor rounding\n",
    "    if third_digit >= 5:\n",
    "        return math.ceil(num * 100) / 100  # Round up\n",
    "    else:\n",
    "        return math.floor(num * 100) / 100\n",
    "\n",
    "\n",
    "# Compute empirical threshold\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"*.pkl\"))\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Computing TAU_C\")\n",
    "\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    # Update the digest with each similarity value\n",
    "    for value in current.values():\n",
    "        digest.update(value)\n",
    "    del current\n",
    "    progress_bar.update(1)\n",
    "\n",
    "estimated_90th = digest.percentile(90)\n",
    "TAU_C = custom_round(estimated_90th)  # 90th percentile\n",
    "print(\"Estimated tau_c (90th percentile):\", TAU_C)\n",
    "\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Generating all edges\")\n",
    "current_edges = {}\n",
    "for i in range(0, len(edges_files)):\n",
    "    with open(edges_files[i], \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "        current_edges = {key: f_val for key, value in current.items() if (f_val := float(value)) > TAU_C}\n",
    "    del current\n",
    "    with open(f\"cslasl-pre/edges/all_edges_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(current_edges, f)\n",
    "    del current_edges\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1c566071-e447-4c4c-ad87-98c3a839efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSL nodes: 16474\n",
      "CSL edges: 12208150\n"
     ]
    }
   ],
   "source": [
    "### Build CSL\n",
    "\n",
    "def aggregate_texts(df, text_fields):\n",
    "    # Concatenate selected fields into a single string per row.\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions['full_text'] = aggregate_texts(submissions, ['title', 'selftext'])\n",
    "comments['body'] = comments['body'].fillna(\"\")\n",
    "\n",
    "def get_edges():\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    def clean(text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    edges = {}\n",
    "    \n",
    "    contents_per_author = {}\n",
    "    for submission in submissions.itertuples():\n",
    "        author = submission.author\n",
    "        title = submission.title\n",
    "        selftext = submission.selftext\n",
    "        full_text = f\"{title} {selftext}\"\n",
    "        full_text = clean(full_text)\n",
    "        \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(full_text)\n",
    "        else:\n",
    "            contents_per_author[author] = [full_text]\n",
    "    \n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        body = comment.body\n",
    "        body = clean(body)\n",
    "    \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(body)\n",
    "        else:\n",
    "            contents_per_author[author] = [body]\n",
    "    \n",
    "    for author, contents in contents_per_author.items():\n",
    "        contents_per_author[author] = \" \".join(contents_per_author[author])\n",
    "    \n",
    "    authors = list(contents_per_author.keys())\n",
    "    contents = list(contents_per_author.values())\n",
    "    \n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', trust_remote_code=True, device=\"cuda:1\")\n",
    "    embeddings = model.encode(contents, normalize=True)\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)  # Dot product of normalized vectors\n",
    "    indices = np.triu_indices(len(authors), k=1)  # Get indices of the upper triangle (excluding diagonal)\n",
    "    \n",
    "    author_pairs = [(authors[i], authors[j]) for i, j in zip(indices[0], indices[1])]\n",
    "    similarities = similarity_matrix[indices]  # Fetch corresponding similarity values\n",
    "\n",
    "    # Store edges where similarity > TAU_C\n",
    "    edges = {pair: sim for pair, sim in zip(author_pairs, similarities) if sim > TAU_C}\n",
    "    \n",
    "    return edges\n",
    "\n",
    "CSL = nx.Graph()\n",
    "for author in all_authors:\n",
    "    CSL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    CSL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"CSL nodes: {len(CSL.nodes())}\")\n",
    "print(f\"CSL edges: {len(CSL.edges())}\")\n",
    "print(f\"CSL density: {nx.density(CSL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(CSL)\n",
    "print(f\"Average CSL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('csl.pkl', 'wb') as f:\n",
    "    pickle.dump(CSL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7c65f027-3d59-43d0-b494-aefc3db0a968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL nodes: 16474\n",
      "ASL edges: 15248971\n"
     ]
    }
   ],
   "source": [
    "### Build ASL\n",
    "\n",
    "def aggregate_texts(df, text_fields):\n",
    "    # Concatenate selected fields into a single string per row.\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions['full_text'] = aggregate_texts(submissions, ['title', 'selftext'])\n",
    "comments['full_text'] = comments['body'].fillna(\"\")\n",
    "\n",
    "def get_edges():\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    \n",
    "    def clean(text):\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip().lower()\n",
    "        \n",
    "    edges = {}\n",
    "    \n",
    "    contents_per_author = {}\n",
    "    for submission in submissions.itertuples():\n",
    "        author = submission.author\n",
    "        title = submission.title\n",
    "        selftext = submission.selftext\n",
    "        full_text = f\"{title} {selftext}\"\n",
    "        full_text = clean(full_text)\n",
    "        \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(full_text)\n",
    "        else:\n",
    "            contents_per_author[author] = [full_text]\n",
    "    \n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        body = comment.body\n",
    "        body = clean(body)\n",
    "    \n",
    "        if author in contents_per_author:\n",
    "            contents_per_author[author].extend(body)\n",
    "        else:\n",
    "            contents_per_author[author] = [body]\n",
    "    \n",
    "    for author, contents in contents_per_author.items():\n",
    "        contents_per_author[author] = \" \".join(contents_per_author[author])\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "\n",
    "    def get_sentiment_vector(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")  # Move input to GPU\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits  # Get logits\n",
    "        return output.squeeze().cpu().numpy()\n",
    "\n",
    "    authors_sentiments = {}\n",
    "    for author, contents in contents_per_author.items():\n",
    "        authors_sentiments[author] = get_sentiment_vector(contents)\n",
    "         \n",
    "    sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "    norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "    valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "    normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "    normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "    similarity_matrix = np.dot(normalized_matrix, normalized_matrix.T)\n",
    "\n",
    "    edges = {}\n",
    "    indices = np.triu_indices(len(authors), k=1)  # Get upper triangle indices\n",
    "    \n",
    "    author_pairs = [(authors[i], authors[j]) for i, j in zip(indices[0], indices[1])]\n",
    "    similarities = similarity_matrix[indices]  # Fetch similarity values\n",
    "    \n",
    "    # Store Edges Where Similarity > TAU_A\n",
    "    edges = {pair: sim for pair, sim in zip(author_pairs, similarities) if sim > TAU_A}\n",
    "    \n",
    "    return edges\n",
    "\n",
    "ASL = nx.Graph()\n",
    "for author in all_authors:\n",
    "    ASL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    ASL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"ASL nodes: {len(ASL.nodes())}\")\n",
    "print(f\"ASL edges: {len(ASL.edges())}\")\n",
    "print(f\"ASL density: {nx.density(ASL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(ASL)\n",
    "print(f\"Average ASL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('asl.pkl', 'wb') as f:\n",
    "    pickle.dump(ASL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cef323bb-c585-4710-8144-bc51678a23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uil.pkl', 'rb') as f:\n",
    "    UIL = pickle.load(f)\n",
    "with open('csl.pkl', 'rb') as f:\n",
    "    CSL = pickle.load(f)\n",
    "with open('tdl.pkl', 'rb') as f:\n",
    "    TDL = pickle.load(f)\n",
    "with open('asl.pkl', 'rb') as f:\n",
    "    ASL = pickle.load(f)\n",
    "\n",
    "layers = {\n",
    "    \"UIL\": UIL,\n",
    "    \"CSL\": CSL,\n",
    "    \"TDL\": TDL,\n",
    "    \"ASL\": ASL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "22868134-26c6-4b09-a0dc-41767d32419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multilayer = mnet.MultilayerGraph()\n",
    "\n",
    "# Add nodes from UIL (the same for all layers).\n",
    "for layer_name, G in layers.items():\n",
    "    for node in G.nodes():\n",
    "        multilayer.add_node((node, layer_name))\n",
    "\n",
    "# Add intralayer edges.\n",
    "for layer_name, G in layers.items():\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        multilayer.add_edge((u, layer_name), (v, layer_name), weight=data.get('weight', 1))\n",
    "\n",
    "# Add interlayer edges.\n",
    "all_users = set()\n",
    "for G in layers.values():\n",
    "    all_users.update(G.nodes())\n",
    "layer_names = list(layers.keys())\n",
    "for user in all_users:\n",
    "    for i in range(len(layer_names)):\n",
    "        for j in range(i+1, len(layer_names)):\n",
    "            multilayer.add_edge((user, layer_names[i]), (user, layer_names[j]), weight=OMEGA)\n",
    "\n",
    "interlayer_edges = [e for e in multilayer.edges() if e[0][1] != e[1][1]]\n",
    "num_interlayer_edges = len(interlayer_edges)\n",
    "\n",
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "afc82b34-93ec-47af-a6f0-e49f8f6cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network saved to 'multilayer_network.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# SAVE THE MULTILAYER NETWORK\n",
    "# --------------------------\n",
    "with open('network.pkl', 'wb') as f:\n",
    "    pickle.dump(multilayer, f)\n",
    "print(\"Multilayer network saved to 'multilayer_network.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6f0dd23c-1521-4950-92a2-773bf2d9775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network constructed with 98844 interlayer edges.\n"
     ]
    }
   ],
   "source": [
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d019-7ef6-40ed-98ca-a6e944f52c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
