{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724ba0a5-2d3a-4930-a369-976649bf94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shared/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import multinetx as mnet\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9430c353-6569-4f43-9349-2ee60f8ac4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# PARAMETERS & THRESHOLDS\n",
    "# --------------------------\n",
    "TAU_C = 0.39        # Threshold for content similarity (CSL)\n",
    "TAU_A = 0.99      # Threshold for affective similarity (ASL)\n",
    "OMEGA = 1.0        # Uniform interlayer coupling weight\n",
    "DELTA_T = 3600     # Time window (in seconds) for temporal burst calculation (e.g., 1 hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e231944-1dec-4359-8f19-1e1a501bfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "    \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1504266-9867-48d8-a6f7-26451ad721ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130430\n"
     ]
    }
   ],
   "source": [
    "all_authors = list(submissions['author'])\n",
    "all_authors.extend(list(comments['author']))\n",
    "all_authors = list(set(all_authors))\n",
    "print(len(all_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6151c422-d05f-4c79-97c8-504e76760265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIL nodes: 130430\n",
      "UIL edges: 439606\n",
      "UIL density: 2.584115686947797e-05\n",
      "Average UIL Edge Weight: 1.2130\n"
     ]
    }
   ],
   "source": [
    "### Build UIL\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            if edge in edges:\n",
    "                edges[edge] += 1\n",
    "            else:\n",
    "                edges[edge] = 1    \n",
    "\n",
    "    return edges\n",
    "\n",
    "UIL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    UIL.add_node(author)\n",
    "\n",
    "for edge, weight in get_edges().items():\n",
    "    UIL.add_edge(edge[0], edge[1], weight=weight)\n",
    "\n",
    "print(f\"UIL nodes: {len(UIL.nodes())}\")\n",
    "print(f\"UIL edges: {len(UIL.edges())}\")\n",
    "print(f\"UIL density: {nx.density(UIL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(UIL)\n",
    "print(f\"Average UIL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('uil.pkl', 'wb') as f:\n",
    "    pickle.dump(UIL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9bc5a0-4447-4d07-938e-abea1df69253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDL nodes: 130430\n",
      "TDL edges: 439606\n",
      "TDL density: 2.584115686947797e-05\n",
      "Average TDL Edge Weight: 1.1241\n"
     ]
    }
   ],
   "source": [
    "### Build TDL\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    \"\"\"\n",
    "    Given a sorted list of timestamps (in seconds), compute the maximum number of events\n",
    "    occurring within any window of length `window`.\n",
    "    \"\"\"\n",
    "    if len(timestamps) == 0:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "def get_edges():\n",
    "    edges = {}\n",
    "\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        parent = comment.parent_id.split(\"_\")[1]\n",
    "    \n",
    "        reply_to = None\n",
    "    \n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except:\n",
    "            reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "    \n",
    "        if reply_to:\n",
    "            edge = (author, reply_to)\n",
    "            timestamp = comment.created_utc\n",
    "            \n",
    "            if edge in edges:\n",
    "                edges[edge].append(timestamp)\n",
    "            else:\n",
    "                edges[edge] = [timestamp]   \n",
    "\n",
    "    return edges\n",
    "\n",
    "TDL = nx.DiGraph()\n",
    "for author in all_authors:\n",
    "    TDL.add_node(author)\n",
    "\n",
    "for edge, times in get_edges().items():\n",
    "    times_sorted = sorted(times)\n",
    "    burst_weight = compute_sliding_window_max(times_sorted, window=DELTA_T)\n",
    "    TDL.add_edge(edge[0], edge[1], weight=burst_weight)\n",
    "\n",
    "print(f\"TDL nodes: {len(TDL.nodes())}\")\n",
    "print(f\"TDL edges: {len(TDL.edges())}\")\n",
    "print(f\"TDL density: {nx.density(TDL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(TDL)\n",
    "print(f\"Average TDL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('tdl.pkl', 'wb') as f:\n",
    "    pickle.dump(TDL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51c72ab-0f44-4a50-8576-1dd5739eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLEANING FUNCTION\n",
    "# --------------------------\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------\n",
    "def load_submissions(submissions_file):\n",
    "    return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "\n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "\n",
    "submissions = pd.read_csv(\"submissions.csv.zst\")\n",
    "comments = pd.read_csv(\"comments.csv.zst\")\n",
    "\n",
    "\n",
    "# CLS/ASL Preprocessing\n",
    "# --------------------------\n",
    "# AGGREGATE TEXTS PER AUTHOR\n",
    "# --------------------------\n",
    "def aggregate_texts(df, text_fields):\n",
    "    \"\"\"Concatenate selected fields into a single string per row.\"\"\"\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions[\"full_text\"] = aggregate_texts(submissions, [\"title\", \"selftext\"])\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "submissions[\"selftext\"] = submissions[\"selftext\"].fillna(\"\")\n",
    "\n",
    "contents_per_author = {}\n",
    "\n",
    "# Process submissions\n",
    "for submission in submissions.itertuples():\n",
    "    author = submission.author\n",
    "    full_text = clean(f\"{submission.title} {submission.selftext}\")\n",
    "    contents_per_author.setdefault(author, []).append(full_text)\n",
    "\n",
    "# Process comments\n",
    "for comment in comments.itertuples():\n",
    "    author = comment.author\n",
    "    body = clean(comment.body)\n",
    "    contents_per_author.setdefault(author, []).append(body)\n",
    "\n",
    "# Merge all texts for each author\n",
    "for author, contents in contents_per_author.items():\n",
    "    contents_per_author[author] = \" \".join(contents)\n",
    "\n",
    "authors = list(contents_per_author.keys())\n",
    "contents = list(contents_per_author.values())\n",
    "\n",
    "os.makedirs('cslasl-pre', exist_ok=True)\n",
    "with open('cslasl-pre/authors.pkl', 'wb') as f:\n",
    "    pickle.dump(authors, f)\n",
    "with open('cslasl-pre/contents.pkl', 'wb') as f:\n",
    "    pickle.dump(contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eca2b7-9fad-4f50-bae1-15658c7e17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CSL edges and with tau_c at 0.3, then compute estimated tau_c\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True, device=\"cuda:1\")\n",
    "embeddings = model.encode(contents, normalize=True)\n",
    "with open('cslasl-pre/csl_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25594e6-333c-47cd-8fed-03b7d8ff01ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:   0%|                                  | 1/16900 [02:17<645:54:57, 137.60s/batch]\n",
      "Computing Similarities:  51%|████████████████▎               | 8646/16900 [2:08:26<2:02:37,  1.12batch/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Compute cosine similarity in **batches** to prevent MemoryError\n",
    "num_users = len(authors)\n",
    "batch_size = 1000  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "total_batches = (num_users // batch_size) ** 2  # Total iterations (i, j) pairs\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Computing Similarities\", unit=\"batch\")\n",
    "\n",
    "for i in range(0, num_users, batch_size):\n",
    "    edges = {}\n",
    "    end_i = min(i + batch_size, num_users)\n",
    "    \n",
    "    for j in range(i, num_users, batch_size):\n",
    "        end_j = min(j + batch_size, num_users)\n",
    "        \n",
    "        # Compute batch-wise similarity\n",
    "        similarity_batch = np.dot(embeddings[i:end_i], embeddings[j:end_j].T)\n",
    "        \n",
    "        # Extract upper triangle indices to avoid duplicate calculations\n",
    "        if i == j:\n",
    "            upper_tri_indices = np.triu_indices(end_i - i, k=1)\n",
    "            similarity_values = similarity_batch[upper_tri_indices]\n",
    "        else:\n",
    "            similarity_values = similarity_batch.flatten()\n",
    "\n",
    "        # Store only meaningful similarities\n",
    "        for x, sim in enumerate(similarity_values):\n",
    "            if sim > 0.3:  # Adjust threshold as needed\n",
    "                author_1 = authors[i + x // (end_j - j)]\n",
    "                author_2 = authors[j + x % (end_j - j)]\n",
    "                edges[(author_1, author_2)] = sim\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "                \n",
    "    # Save edges to a file\n",
    "    os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "    with open(f\"cslasl-pre/edges/edges_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges, f)\n",
    "    del edges\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a250129-9b49-4e15-925f-0c9ba212a43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_c (90th percentile): 0.5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "\n",
    "def custom_round(num):\n",
    "    \"\"\"\n",
    "    Ceil if the third digit after the decimal is >= 5, otherwise floor.\n",
    "    Always returns a number with two decimal places, but prevents rounding to 1.00.\n",
    "    \"\"\"\n",
    "    # Convert number to string for precise control\n",
    "    num_str = f\"{num:.10f}\"  # Ensure precision to check the third decimal\n",
    "    integer_part, decimal_part = num_str.split('.')\n",
    "\n",
    "    # Extract digits\n",
    "    second_digit = int(decimal_part[1]) if len(decimal_part) > 1 else 0\n",
    "    third_digit = int(decimal_part[2]) if len(decimal_part) > 2 else 0\n",
    "\n",
    "    # Compute the base rounded value\n",
    "    base_value = math.floor(num * 100) / 100  # Default floor rounding\n",
    "\n",
    "    # Only increase within the 2 decimal boundary\n",
    "    if third_digit >= 5:\n",
    "        adjusted_value = base_value + 0.01  # Move up without crossing integer boundary\n",
    "        return min(adjusted_value, math.floor(num))  # Prevent rounding to 1.00\n",
    "\n",
    "    return base_value  # Default floored value\n",
    "\n",
    "\n",
    "# Compute empirical threshold\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"*.pkl\"))\n",
    "\n",
    "similarity_values = []\n",
    "\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)    \n",
    "    similarity_values.extend(current.values())\n",
    "    del current\n",
    "\n",
    "    # Optional: Manually trigger garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "TAU_C = custom_round(np.percentile(similarity_values, 90))  # 90th percentile\n",
    "print(\"Estimated tau_c (90th percentile):\", TAU_C)\n",
    "\n",
    "del similarity_values\n",
    "\n",
    "all_edges = {}\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    for key, value in current.items():\n",
    "        if float(value) > TAU_C:\n",
    "            all_edges[key] = float(value)\n",
    "    del current\n",
    "\n",
    "with open(f\"cslasl-pre/edges/all_edges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_edges, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c566071-e447-4c4c-ad87-98c3a839efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSL nodes: 130430\n",
      "CSL edges: 186477809\n",
      "CSL density: 0.02192327818567331\n",
      "Average CSL Edge Weight: 0.5557\n"
     ]
    }
   ],
   "source": [
    "### Build CSL\n",
    "with open(\"cslasl-pre/edges/all_edges.pkl\", \"rb\") as f:\n",
    "    edges = pickle.load(f)\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "CSL = nx.Graph()\n",
    "for author in authors:\n",
    "    CSL.add_node(author)\n",
    "\n",
    "del authors\n",
    "\n",
    "keys = list(edges.keys())\n",
    "for key in keys:\n",
    "    CSL.add_edge(key[0], key[1], weight=float(edges[key]))\n",
    "    del edges[key]\n",
    "\n",
    "del edges\n",
    "del keys\n",
    "\n",
    "print(f\"CSL nodes: {len(CSL.nodes())}\")\n",
    "print(f\"CSL edges: {len(CSL.edges())}\")\n",
    "print(f\"CSL density: {nx.density(CSL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(CSL)\n",
    "print(f\"Average CSL Edge Weight: {avg_weight:.4f}\")\n",
    "\n",
    "with open('csl.pkl', 'wb') as f:\n",
    "    pickle.dump(CSL, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c65f027-3d59-43d0-b494-aefc3db0a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ASL edges and with tau_a at 0.5, then compute estimated tau_a\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "with open(\"cslasl-pre/contents.pkl\", \"rb\") as f:\n",
    "    contents = pickle.load(f)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "\n",
    "def get_sentiment_vector(text):\n",
    "    \"\"\"Compute sentiment vector from RoBERTa model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).logits  # Get logits\n",
    "    return output.squeeze().cpu().numpy()\n",
    "\n",
    "authors_sentiments = {}\n",
    "for i in range(0, len(authors)):\n",
    "    authors_sentiments[authors[i]] = get_sentiment_vector(contents[i])\n",
    "\n",
    "with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'wb') as f:\n",
    "    pickle.dump(authors_sentiments, f)\n",
    "\n",
    "del authors\n",
    "del contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cfd8f0-de63-472a-96f5-f54923974adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment vectors to NumPy\n",
    "sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "\n",
    "with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'wb') as f:\n",
    "    pickle.dump(normalized_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49232de1-ff94-41fb-839c-06966a345212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:  50%|██████████████▌              | 213531/425104 [5:27:19<2:45:41, 21.28batch/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "# Compute cosine similarity in **batches** to prevent MemoryError\n",
    "num_users = len(authors)\n",
    "batch_size = 200  # Adjust based on memory constraints\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "total_batches = (num_users // batch_size) ** 2  # Total iterations (i, j) pairs\n",
    "progress_bar = tqdm(total=total_batches, desc=\"Computing Similarities\", unit=\"batch\")\n",
    "\n",
    "# Compute cosine similarity in **batches** for sentiment analysis\n",
    "for i in range(0, num_users, batch_size):\n",
    "    edges_sentiment = {}\n",
    "    end_i = min(i + batch_size, num_users)\n",
    "    \n",
    "    for j in range(i, num_users, batch_size):\n",
    "        end_j = min(j + batch_size, num_users)\n",
    "        \n",
    "        # Compute batch-wise similarity\n",
    "        similarity_batch = np.dot(normalized_matrix[i:end_i], normalized_matrix[j:end_j].T)\n",
    "        \n",
    "        # Extract upper triangle indices to avoid duplicate calculations\n",
    "        if i == j:\n",
    "            upper_tri_indices = np.triu_indices(end_i - i, k=1)\n",
    "            similarity_values = similarity_batch[upper_tri_indices]\n",
    "        else:\n",
    "            similarity_values = similarity_batch.flatten()\n",
    "\n",
    "        # Store only meaningful similarities\n",
    "        for x, sim in enumerate(similarity_values):\n",
    "            if sim > 0.5:  # Adjust threshold as needed\n",
    "                author_1 = authors[i + x // (end_j - j)]\n",
    "                author_2 = authors[j + x % (end_j - j)]\n",
    "                edges_sentiment[(author_1, author_2)] = sim\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "                \n",
    "    # Save edges to a file\n",
    "    os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "    with open(f\"cslasl-pre/edges/edges_sentiments_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges_sentiment, f)\n",
    "    del edges_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0a05e-a81c-449b-bb2b-780935ceb8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "from tdigest import TDigest\n",
    "from tqdm import tqdm\n",
    "\n",
    "digest = TDigest()\n",
    "\n",
    "def custom_round(num):\n",
    "    \"\"\"\n",
    "    Ceil if the fourth digit after the decimal is >= 5, otherwise floor.\n",
    "    Always returns a number with three decimal places.\n",
    "    \"\"\"\n",
    "    # Convert number to string for precise control\n",
    "    num_str = f\"{num:.10f}\"  # Ensure precision to check the fourth decimal\n",
    "    integer_part, decimal_part = num_str.split('.')\n",
    "\n",
    "    # Extract digits\n",
    "    third_digit = int(decimal_part[2]) if len(decimal_part) > 2 else 0\n",
    "    fourth_digit = int(decimal_part[3]) if len(decimal_part) > 3 else 0  # Fourth digit\n",
    "\n",
    "    # Compute the base rounded value (3 decimal places)\n",
    "    base_value = math.floor(num * 1000) / 1000  # Default floor rounding\n",
    "\n",
    "    # Only increase within the 3 decimal boundary\n",
    "    if fourth_digit >= 5:\n",
    "        adjusted_value = base_value + 0.001  # Move up without crossing integer boundary\n",
    "        return min(adjusted_value, math.floor(num * 100) / 100)  # Prevent rounding to unintended values\n",
    "\n",
    "    return base_value  # Default floored value\n",
    "\n",
    "\n",
    "# Compute empirical threshold\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"edges_sentiments_*.pkl\"))\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Computing TAU_A\", unit=\"batch\")\n",
    "\n",
    "\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "    # Update the digest with each similarity value\n",
    "    for value in current.values():\n",
    "        digest.update(value)\n",
    "    del current\n",
    "    progress_bar.update(1)\n",
    "\n",
    "estimated_90th = digest.percentile(90)\n",
    "TAU_A = custom_round(estimated_90th)\n",
    "print(\"Estimated tau_a (90th percentile):\", TAU_A)\n",
    "del digest\n",
    "\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Generating all edges\")\n",
    "current_edges = {}\n",
    "for i in range(0, len(edges_files)):\n",
    "    with open(edges_files[i], \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "        current_edges = {key: f_val for key, value in current.items() if (f_val := float(value)) > TAU_A}\n",
    "    del current\n",
    "    with open(f\"cslasl-pre/edges/all_edges_sentiment_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(current_edges, f)\n",
    "    del current_edges\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4bc57a-9268-4e16-b0f6-5773b790b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating all edges: 100%|█████████████████████████████████████| 653/653 [1:52:45<00:00,  4.37s/it]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import glob\n",
    "from tdigest import TDigest\n",
    "from tqdm import tqdm\n",
    "\n",
    "TAU_A = 0.998\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"edges_sentiments_*.pkl\"))\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Generating all edges\")\n",
    "current_edges = {}\n",
    "for i in range(0, len(edges_files)):\n",
    "    with open(edges_files[i], \"rb\") as f:\n",
    "        current = pickle.load(f)\n",
    "        current_edges = {key: f_val for key, value in current.items() if (f_val := float(value)) > TAU_A}\n",
    "    del current\n",
    "    with open(f\"cslasl-pre/edges/all_edges_sentiment_{i}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(current_edges, f)\n",
    "    del current_edges\n",
    "    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07dd18-eef0-491c-b57c-7e1575545017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding authors:  93%|███████████████████████████████▌  | 121117/130430 [00:00<00:00, 1211162.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASL nodes: 130430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding authors: 100%|██████████████████████████████████| 130430/130430 [00:00<00:00, 1061088.10it/s]\u001b[A\n",
      "\n",
      "Adding edges from files:   0%|                                      | 1/653 [00:04<51:10,  4.71s/it]\u001b[A\n",
      "Adding edges from files:   0%|                                      | 2/653 [00:08<44:43,  4.12s/it]\u001b[A\n",
      "Adding edges from files:   0%|▏                                     | 3/653 [00:09<27:07,  2.50s/it]\u001b[A\n",
      "Adding edges from files:   1%|▏                                     | 4/653 [00:09<17:22,  1.61s/it]\u001b[A\n",
      "Adding edges from files:   1%|▎                                     | 5/653 [00:12<23:15,  2.15s/it]\u001b[A\n",
      "Adding edges from files:   1%|▎                                     | 6/653 [00:12<16:20,  1.52s/it]\u001b[A\n",
      "Adding edges from files:   1%|▍                                     | 7/653 [00:12<11:49,  1.10s/it]\u001b[A\n",
      "Adding edges from files:   1%|▍                                     | 8/653 [00:13<08:57,  1.20it/s]\u001b[A\n",
      "Adding edges from files:   1%|▌                                     | 9/653 [00:13<06:57,  1.54it/s]\u001b[A\n",
      "Adding edges from files:   2%|▌                                    | 10/653 [00:13<05:35,  1.92it/s]\u001b[A\n",
      "Adding edges from files:   2%|▌                                    | 11/653 [00:13<04:41,  2.28it/s]\u001b[A\n",
      "Adding edges from files:   2%|▋                                    | 12/653 [00:14<04:02,  2.64it/s]\u001b[A\n",
      "Adding edges from files:   2%|▋                                    | 13/653 [00:14<03:31,  3.02it/s]\u001b[A\n",
      "Adding edges from files:   2%|▊                                    | 14/653 [00:14<04:30,  2.36it/s]\u001b[A\n",
      "Adding edges from files:   2%|▊                                    | 15/653 [00:15<03:53,  2.73it/s]\u001b[A\n",
      "Adding edges from files:   2%|▉                                    | 16/653 [00:15<03:27,  3.07it/s]\u001b[A\n",
      "Adding edges from files:   3%|▉                                    | 17/653 [00:19<15:17,  1.44s/it]\u001b[A\n",
      "Adding edges from files:   3%|█                                    | 18/653 [00:24<26:25,  2.50s/it]\u001b[A\n",
      "Adding edges from files:   3%|█                                    | 19/653 [00:24<19:26,  1.84s/it]\u001b[A\n",
      "Adding edges from files:   3%|█▏                                   | 20/653 [00:25<14:40,  1.39s/it]\u001b[A\n",
      "Adding edges from files:   3%|█▏                                   | 21/653 [00:25<11:16,  1.07s/it]\u001b[A\n",
      "Adding edges from files:   3%|█▏                                   | 22/653 [00:25<09:09,  1.15it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▎                                   | 23/653 [00:26<07:36,  1.38it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▎                                   | 24/653 [00:26<06:20,  1.65it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▍                                   | 25/653 [00:27<07:16,  1.44it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▍                                   | 26/653 [00:27<05:52,  1.78it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▌                                   | 27/653 [00:27<04:57,  2.11it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▌                                   | 28/653 [00:28<04:09,  2.51it/s]\u001b[A\n",
      "Adding edges from files:   4%|█▋                                   | 29/653 [00:28<03:36,  2.88it/s]\u001b[A\n",
      "Adding edges from files:   5%|█▋                                   | 30/653 [00:34<21:05,  2.03s/it]\u001b[A\n",
      "Adding edges from files:   5%|█▊                                   | 31/653 [00:34<15:16,  1.47s/it]\u001b[A\n",
      "Adding edges from files:   5%|█▊                                   | 32/653 [00:34<11:08,  1.08s/it]\u001b[A\n",
      "Adding edges from files:   5%|█▊                                   | 33/653 [00:34<08:15,  1.25it/s]\u001b[A\n",
      "Adding edges from files:   5%|█▉                                   | 34/653 [00:34<06:16,  1.64it/s]\u001b[A\n",
      "Adding edges from files:   5%|█▉                                   | 35/653 [00:35<04:51,  2.12it/s]\u001b[A\n",
      "Adding edges from files:   6%|██                                   | 36/653 [00:38<14:17,  1.39s/it]\u001b[A\n",
      "Adding edges from files:   6%|██                                   | 37/653 [00:38<10:27,  1.02s/it]\u001b[A\n",
      "Adding edges from files:   6%|██▏                                  | 38/653 [00:38<07:42,  1.33it/s]\u001b[A\n",
      "Adding edges from files:   6%|██▏                                  | 39/653 [00:39<05:50,  1.75it/s]\u001b[A\n",
      "Adding edges from files:   6%|██▎                                  | 40/653 [00:39<04:36,  2.22it/s]\u001b[A\n",
      "Adding edges from files:   6%|██▎                                  | 41/653 [00:39<03:46,  2.71it/s]\u001b[A\n",
      "Adding edges from files:   6%|██▍                                  | 42/653 [00:42<12:50,  1.26s/it]\u001b[A\n",
      "Adding edges from files:   7%|██▍                                  | 43/653 [00:42<09:28,  1.07it/s]\u001b[A\n",
      "Adding edges from files:   7%|██▍                                  | 44/653 [00:43<07:03,  1.44it/s]\u001b[A\n",
      "Adding edges from files:   7%|██▌                                  | 45/653 [00:43<05:22,  1.89it/s]\u001b[A\n",
      "Adding edges from files:   7%|██▌                                  | 46/653 [00:43<04:12,  2.41it/s]\u001b[A\n",
      "Adding edges from files:   7%|██▋                                  | 47/653 [00:44<04:46,  2.12it/s]\u001b[A\n",
      "Adding edges from files:   7%|██▋                                  | 48/653 [00:44<03:45,  2.68it/s]\u001b[A\n",
      "Adding edges from files:   8%|██▊                                  | 49/653 [00:44<03:00,  3.35it/s]\u001b[A\n",
      "Adding edges from files:   8%|██▊                                  | 50/653 [00:44<02:29,  4.04it/s]\u001b[A\n",
      "Adding edges from files:   8%|██▉                                  | 51/653 [00:44<02:09,  4.67it/s]\u001b[A\n",
      "Adding edges from files:   8%|██▉                                  | 52/653 [00:44<01:55,  5.18it/s]\u001b[A\n",
      "Adding edges from files:   8%|███                                  | 53/653 [00:44<01:45,  5.70it/s]\u001b[A\n",
      "Adding edges from files:   8%|███                                  | 54/653 [00:48<12:13,  1.22s/it]\u001b[A\n",
      "Adding edges from files:   8%|███                                  | 55/653 [00:48<08:52,  1.12it/s]\u001b[A\n",
      "Adding edges from files:   9%|███▏                                 | 56/653 [00:48<06:31,  1.52it/s]\u001b[A\n",
      "Adding edges from files:   9%|███▎                                 | 58/653 [00:49<04:57,  2.00it/s]\u001b[A\n",
      "Adding edges from files:   9%|███▍                                 | 60/653 [00:49<03:20,  2.96it/s]\u001b[A\n",
      "Adding edges from files:   9%|███▌                                 | 62/653 [00:49<02:26,  4.04it/s]\u001b[A\n",
      "Adding edges from files:  10%|███▋                                 | 64/653 [00:49<01:51,  5.30it/s]\u001b[A\n",
      "Adding edges from files:  10%|███▋                                 | 66/653 [00:53<07:20,  1.33it/s]\u001b[A\n",
      "Adding edges from files:  10%|███▊                                 | 68/653 [00:53<05:10,  1.89it/s]\u001b[A\n",
      "Adding edges from files:  11%|███▉                                 | 70/653 [00:54<04:37,  2.10it/s]\u001b[A\n",
      "Adding edges from files:  11%|████▏                                | 73/653 [00:54<02:55,  3.30it/s]\u001b[A\n",
      "Adding edges from files:  12%|████▎                                | 77/653 [00:54<01:45,  5.45it/s]\u001b[A\n",
      "Adding edges from files:  12%|████▍                                | 79/653 [00:58<05:32,  1.73it/s]\u001b[A\n",
      "Adding edges from files:  12%|████▌                                | 81/653 [00:59<04:53,  1.95it/s]\u001b[A\n",
      "Adding edges from files:  13%|████▋                                | 83/653 [01:02<08:00,  1.19it/s]\u001b[A\n",
      "Adding edges from files:  13%|████▊                                | 84/653 [01:06<12:20,  1.30s/it]\u001b[A\n",
      "Adding edges from files:  13%|████▊                                | 85/653 [01:09<15:48,  1.67s/it]\u001b[A\n",
      "Adding edges from files:  13%|████▊                                | 86/653 [01:13<19:14,  2.04s/it]\u001b[A\n",
      "Adding edges from files:  13%|████▉                                | 87/653 [01:17<24:18,  2.58s/it]\u001b[A\n",
      "Adding edges from files:  13%|████▉                                | 88/653 [01:21<26:51,  2.85s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████                                | 89/653 [01:24<28:30,  3.03s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████                                | 90/653 [01:28<30:21,  3.23s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████▏                               | 91/653 [01:29<23:27,  2.51s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████▏                               | 92/653 [01:32<25:48,  2.76s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████▎                               | 93/653 [01:35<27:51,  2.98s/it]\u001b[A\n",
      "Adding edges from files:  14%|█████▎                               | 94/653 [01:39<29:48,  3.20s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▍                               | 95/653 [01:43<31:32,  3.39s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▍                               | 96/653 [01:47<32:11,  3.47s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▍                               | 97/653 [01:51<33:03,  3.57s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▌                               | 98/653 [01:54<33:38,  3.64s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▌                               | 99/653 [01:59<36:20,  3.94s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▌                              | 100/653 [02:03<36:20,  3.94s/it]\u001b[A\n",
      "Adding edges from files:  15%|█████▌                              | 101/653 [02:07<35:34,  3.87s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▌                              | 102/653 [02:07<26:46,  2.92s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▋                              | 103/653 [02:11<27:59,  3.05s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▋                              | 104/653 [02:14<29:57,  3.27s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▊                              | 105/653 [02:18<30:44,  3.37s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▊                              | 106/653 [02:22<31:44,  3.48s/it]\u001b[A\n",
      "Adding edges from files:  16%|█████▉                              | 107/653 [02:26<32:29,  3.57s/it]\u001b[A\n",
      "Adding edges from files:  17%|█████▉                              | 108/653 [02:29<32:38,  3.59s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████                              | 109/653 [02:33<33:18,  3.67s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████                              | 110/653 [02:37<33:29,  3.70s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████                              | 111/653 [02:41<34:34,  3.83s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████▏                             | 112/653 [02:45<33:49,  3.75s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████▏                             | 113/653 [02:49<35:37,  3.96s/it]\u001b[A\n",
      "Adding edges from files:  17%|██████▎                             | 114/653 [02:50<26:59,  3.00s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▎                             | 115/653 [02:54<29:26,  3.28s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▍                             | 116/653 [02:57<29:22,  3.28s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▍                             | 117/653 [03:01<30:36,  3.43s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▌                             | 118/653 [03:04<31:03,  3.48s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▌                             | 119/653 [03:08<32:18,  3.63s/it]\u001b[A\n",
      "Adding edges from files:  18%|██████▌                             | 120/653 [03:12<32:18,  3.64s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▋                             | 121/653 [03:16<32:57,  3.72s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▋                             | 122/653 [03:20<33:01,  3.73s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▊                             | 123/653 [03:24<33:51,  3.83s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▊                             | 124/653 [03:29<36:39,  4.16s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▉                             | 125/653 [03:29<27:36,  3.14s/it]\u001b[A\n",
      "Adding edges from files:  19%|██████▉                             | 126/653 [03:34<30:11,  3.44s/it]\u001b[A\n",
      "Adding edges from files:  19%|███████                             | 127/653 [03:37<30:33,  3.49s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████                             | 128/653 [03:41<31:21,  3.58s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████                             | 129/653 [03:45<32:19,  3.70s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████▏                            | 130/653 [03:48<31:56,  3.66s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████▏                            | 131/653 [03:52<31:21,  3.61s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████▎                            | 132/653 [03:56<31:24,  3.62s/it]\u001b[A\n",
      "Adding edges from files:  20%|███████▎                            | 133/653 [03:59<31:34,  3.64s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▍                            | 134/653 [04:03<30:58,  3.58s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▍                            | 135/653 [04:06<30:40,  3.55s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▍                            | 136/653 [04:07<23:19,  2.71s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▌                            | 137/653 [04:11<25:57,  3.02s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▌                            | 138/653 [04:15<30:16,  3.53s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▋                            | 139/653 [04:19<29:54,  3.49s/it]\u001b[A\n",
      "Adding edges from files:  21%|███████▋                            | 140/653 [04:22<29:31,  3.45s/it]\u001b[A\n",
      "Adding edges from files:  22%|███████▊                            | 141/653 [04:26<29:12,  3.42s/it]\u001b[A\n",
      "Adding edges from files:  22%|███████▊                            | 142/653 [04:29<28:37,  3.36s/it]\u001b[A\n",
      "Adding edges from files:  22%|███████▉                            | 143/653 [04:32<28:55,  3.40s/it]\u001b[A\n",
      "Adding edges from files:  22%|███████▉                            | 144/653 [04:39<37:21,  4.40s/it]\u001b[A\n",
      "Adding edges from files:  22%|███████▉                            | 145/653 [04:45<41:16,  4.88s/it]\u001b[A\n",
      "Adding edges from files:  22%|████████                            | 146/653 [04:50<42:36,  5.04s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████                            | 147/653 [04:51<32:13,  3.82s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▏                           | 148/653 [04:58<38:43,  4.60s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▏                           | 149/653 [05:04<43:36,  5.19s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▎                           | 150/653 [05:13<52:15,  6.23s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▎                           | 151/653 [05:20<54:23,  6.50s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▍                           | 152/653 [05:27<54:51,  6.57s/it]\u001b[A\n",
      "Adding edges from files:  23%|████████▍                           | 153/653 [05:34<55:00,  6.60s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▍                           | 154/653 [05:41<56:20,  6.77s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▌                           | 155/653 [05:48<56:12,  6.77s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▌                           | 156/653 [05:54<55:54,  6.75s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▋                           | 157/653 [06:02<57:30,  6.96s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▏                         | 158/653 [06:11<1:02:05,  7.53s/it]\u001b[A\n",
      "Adding edges from files:  24%|████████▎                         | 159/653 [06:18<1:01:49,  7.51s/it]\u001b[A\n",
      "Adding edges from files:  25%|████████▊                           | 160/653 [06:24<58:01,  7.06s/it]\u001b[A\n",
      "Adding edges from files:  25%|████████▉                           | 161/653 [06:30<54:45,  6.68s/it]\u001b[A\n",
      "Adding edges from files:  25%|████████▉                           | 162/653 [06:39<59:50,  7.31s/it]\u001b[A\n",
      "Adding edges from files:  25%|████████▍                         | 163/653 [06:46<1:00:44,  7.44s/it]\u001b[A\n",
      "Adding edges from files:  25%|█████████                           | 164/653 [06:53<58:29,  7.18s/it]\u001b[A\n",
      "Adding edges from files:  25%|█████████                           | 165/653 [06:59<56:47,  6.98s/it]\u001b[A\n",
      "Adding edges from files:  25%|█████████▏                          | 166/653 [07:06<56:22,  6.95s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▏                          | 167/653 [07:13<55:14,  6.82s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▎                          | 168/653 [07:18<51:55,  6.42s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▎                          | 169/653 [07:19<38:06,  4.72s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▎                          | 170/653 [07:23<35:24,  4.40s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▍                          | 171/653 [07:29<39:22,  4.90s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▍                          | 172/653 [07:35<43:14,  5.39s/it]\u001b[A\n",
      "Adding edges from files:  26%|█████████▌                          | 173/653 [07:42<45:16,  5.66s/it]\u001b[A\n",
      "Adding edges from files:  27%|█████████▌                          | 174/653 [07:49<48:52,  6.12s/it]\u001b[A\n",
      "Adding edges from files:  27%|█████████▋                          | 175/653 [07:54<46:13,  5.80s/it]\u001b[A\n",
      "Adding edges from files:  27%|█████████▋                          | 176/653 [07:59<45:17,  5.70s/it]\u001b[A\n",
      "Adding edges from files:  27%|█████████▊                          | 177/653 [08:04<44:03,  5.55s/it]\u001b[A\n",
      "Adding edges from files:  27%|█████████▊                          | 178/653 [08:08<38:41,  4.89s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "### Build ASL\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "ASL = nx.Graph()\n",
    "progress_bar = tqdm(total=len(authors), desc=\"Adding authors\")\n",
    "for author in authors:\n",
    "    ASL.add_node(author)\n",
    "    progress_bar.update(1)\n",
    "print(f\"ASL nodes: {len(ASL.nodes())}\")\n",
    "del authors\n",
    "\n",
    "\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"all_edges_sentiment_*.pkl\"))\n",
    "progress_bar = tqdm(total=len(edges_files), desc=\"Adding edges from files\")\n",
    "for edges_file in edges_files:\n",
    "    with open(edges_file, \"rb\") as f:\n",
    "        edges = pickle.load(f)\n",
    "        for edge, weight in edges.items():\n",
    "            ASL.add_edge(edge[0], edge[1], weight=weight)\n",
    "        del edges\n",
    "    progress_bar.update(1)\n",
    "\n",
    "with open('asl.pkl', 'wb') as f:\n",
    "    pickle.dump(ASL, f)\n",
    "    \n",
    "print(f\"ASL edges: {len(ASL.edges())}\")\n",
    "print(f\"ASL density: {nx.density(ASL)}\")\n",
    "\n",
    "def average_edge_weight(G):\n",
    "    \"\"\"Compute the average edge weight in a NetworkX graph.\"\"\"\n",
    "    edge_weights = [data[\"weight\"] for _, _, data in G.edges(data=True)]  # Extract weights\n",
    "    return sum(edge_weights) / len(edge_weights) if edge_weights else 0 \n",
    "\n",
    "avg_weight = average_edge_weight(ASL)\n",
    "print(f\"Average ASL Edge Weight: {avg_weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853b7dc5-4019-4696-ad19-56e633fff075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes (authors): 130430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edge files: 100%|████████████████████████████████████| 653/653 [26:14<00:00,  2.41s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edges: 757759444\n",
      "Average edge weight: 0.9992\n",
      "Graph density: 0.089086\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------------\n",
    "# Write Authors to Disk\n",
    "# --------------------------\n",
    "with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "    authors = pickle.load(f)\n",
    "\n",
    "# Write authors to a text file (one per line)\n",
    "with open(\"cslasl-pre/edges/authors.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for author in authors:\n",
    "        fout.write(f\"{author}\\n\")\n",
    "\n",
    "num_nodes = len(authors)\n",
    "print(f\"Total number of nodes (authors): {num_nodes}\")\n",
    "del authors\n",
    "\n",
    "# --------------------------\n",
    "# Write Edges Directly to Disk\n",
    "# --------------------------\n",
    "edges_files = glob.glob(os.path.join(\"cslasl-pre/edges\", \"all_edges_sentiment_*.pkl\"))\n",
    "edges_output_file = \"cslasl-pre/edges/all_edges_asl.txt\"\n",
    "\n",
    "# Open the output file in write mode; each line will contain: source target weight\n",
    "with open(edges_output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    progress_bar = tqdm(total=len(edges_files), desc=\"Processing edge files\", unit=\"file\")\n",
    "    for edges_file in edges_files:\n",
    "        with open(edges_file, \"rb\") as f:\n",
    "            edges = pickle.load(f)\n",
    "        for edge, weight in edges.items():\n",
    "            fout.write(f\"{edge[0]} {edge[1]} {weight}\\n\")\n",
    "        del edges\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "# --------------------------\n",
    "# Compute Graph Metrics by Streaming the Edge File\n",
    "# --------------------------\n",
    "num_edges = 0\n",
    "weight_sum = 0.0\n",
    "\n",
    "with open(edges_output_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 3:\n",
    "            try:\n",
    "                w = float(parts[2])\n",
    "                weight_sum += w\n",
    "                num_edges += 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "print(f\"Total number of edges: {num_edges}\")\n",
    "\n",
    "# Compute average edge weight\n",
    "avg_edge_weight = weight_sum / num_edges if num_edges > 0 else 0\n",
    "print(f\"Average edge weight: {avg_edge_weight:.4f}\")\n",
    "\n",
    "# Compute graph density for an undirected graph:\n",
    "# Density = 2 * E / (N*(N-1))\n",
    "if num_nodes > 1:\n",
    "    density = 2 * num_edges / (num_nodes * (num_nodes - 1))\n",
    "else:\n",
    "    density = 0\n",
    "print(f\"Graph density: {density:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f119c40-e541-4758-8ead-3d38f2bdb77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 130430 nodes.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def edge_generator(edge_file):\n",
    "    \"\"\"Yield one edge at a time from the edge file.\"\"\"\n",
    "    with open(edge_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                source, target, weight = parts\n",
    "                if float(weight) > 0.999:\n",
    "                    yield source, target, {\"weight\": float(weight)}\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from the authors.txt file\n",
    "with open(\"cslasl-pre/edges/authors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        node = line.strip()\n",
    "        if node:\n",
    "            G.add_node(node)\n",
    "print(f\"Added {G.number_of_nodes()} nodes.\")\n",
    "\n",
    "# Add edges by streaming from the edge file\n",
    "G.add_edges_from(edge_generator(\"cslasl-pre/edges/all_edges_asl.txt\"))\n",
    "print(f\"Added {G.number_of_edges()} edges.\")\n",
    "\n",
    "# Save the graph to disk in gpickle format\n",
    "nx.write_gpickle(G, \"asl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cef323bb-c585-4710-8144-bc51678a23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uil.pkl', 'rb') as f:\n",
    "    UIL = pickle.load(f)\n",
    "with open('csl.pkl', 'rb') as f:\n",
    "    CSL = pickle.load(f)\n",
    "with open('tdl.pkl', 'rb') as f:\n",
    "    TDL = pickle.load(f)\n",
    "with open('asl.pkl', 'rb') as f:\n",
    "    ASL = pickle.load(f)\n",
    "\n",
    "layers = {\n",
    "    \"UIL\": UIL,\n",
    "    \"CSL\": CSL,\n",
    "    \"TDL\": TDL,\n",
    "    \"ASL\": ASL\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "22868134-26c6-4b09-a0dc-41767d32419c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multilayer = mnet.MultilayerGraph()\n",
    "\n",
    "# Add nodes from UIL (the same for all layers).\n",
    "for layer_name, G in layers.items():\n",
    "    for node in G.nodes():\n",
    "        multilayer.add_node((node, layer_name))\n",
    "\n",
    "# Add intralayer edges.\n",
    "for layer_name, G in layers.items():\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        multilayer.add_edge((u, layer_name), (v, layer_name), weight=data.get('weight', 1))\n",
    "\n",
    "# Add interlayer edges.\n",
    "all_users = set()\n",
    "for G in layers.values():\n",
    "    all_users.update(G.nodes())\n",
    "layer_names = list(layers.keys())\n",
    "for user in all_users:\n",
    "    for i in range(len(layer_names)):\n",
    "        for j in range(i+1, len(layer_names)):\n",
    "            multilayer.add_edge((user, layer_names[i]), (user, layer_names[j]), weight=OMEGA)\n",
    "\n",
    "del layers\n",
    "\n",
    "interlayer_edges = [e for e in multilayer.edges() if e[0][1] != e[1][1]]\n",
    "num_interlayer_edges = len(interlayer_edges)\n",
    "\n",
    "print(\"Multilayer network constructed with {} interlayer edges.\".format(len(interlayer_edges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "afc82b34-93ec-47af-a6f0-e49f8f6cacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilayer network saved to 'multilayer_network.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# SAVE THE MULTILAYER NETWORK\n",
    "# --------------------------\n",
    "with open('network.pkl', 'wb') as f:\n",
    "    pickle.dump(multilayer, f)\n",
    "print(\"Multilayer network saved to 'multilayer_network.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d019-7ef6-40ed-98ca-a6e944f52c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
