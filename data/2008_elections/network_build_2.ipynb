{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc84f72-9659-424b-b984-a533c585b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import multinetx as mnet\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import glob\n",
    "from tdigest import TDigest\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "os.makedirs('cslasl-pre', exist_ok=True)\n",
    "os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "os.makedirs('network', exist_ok=True)\n",
    "os.makedirs('network/uil', exist_ok=True)\n",
    "os.makedirs('network/tdl', exist_ok=True)\n",
    "os.makedirs('network/csl', exist_ok=True)\n",
    "os.makedirs('network/asl', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81c2bc9-61e7-4d15-8976-3ec8c67042a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_metrics(authors_path, edges_path):\n",
    "    # Count nodes from the authors file.\n",
    "    num_nodes = 0\n",
    "    with open(authors_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                num_nodes += 1\n",
    "\n",
    "    # Count edges and sum their weights from the edges file.\n",
    "    num_edges = 0\n",
    "    weight_sum = 0.0\n",
    "    with open(edges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    w = float(parts[2])\n",
    "                    num_edges += 1\n",
    "                    weight_sum += w\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    avg_weight = weight_sum / num_edges if num_edges > 0 else 0.0\n",
    "    # Compute density for a directed graph: density = m / (n*(n-1)) if n > 1\n",
    "    density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "    print(f\"Average edge weight: {avg_weight:.4f}\")\n",
    "    print(f\"Density: {density:.5f}\")\n",
    "\n",
    "\n",
    "def custom_round(num):\n",
    "    # Standard round to three decimals.\n",
    "    rounded = round(num, 3)\n",
    "    # Format to a string with exactly 3 decimal places.\n",
    "    s = f\"{rounded:.3f}\"\n",
    "    # Replace the last character (third decimal) with '9'\n",
    "    s = s[:-1] + '9'\n",
    "    return float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12294dbd-e5f2-4b2d-8324-717d86873930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(submissions_file):\n",
    "        return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "    \n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "        \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ce58e5-6c83-4821-a96f-42fbc129b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 13184\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"cslasl-pre/authors.pkl\"):\n",
    "    with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "        authors = pickle.load(f)    \n",
    "else:\n",
    "    authors = list(submissions['author'])\n",
    "    authors.extend(list(comments['author']))\n",
    "    authors = list(set(authors))\n",
    "\n",
    "with open(\"network/authors.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for author in authors:\n",
    "        fout.write(f\"{author}\\n\")    \n",
    "\n",
    "print(f\"Number of nodes: {len(authors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0f5b05-11e0-49da-ab8e-3e9f074463f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments for UIL and TDL: 100%|████████████████████████████████████████████████████████████████████████████| 65511/65511 [05:09<00:00, 211.85comment/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIL\n",
      "Number of nodes: 13184\n",
      "Number of edges: 56209\n",
      "Average edge weight: 1.1655\n",
      "Density: 0.00032\n",
      "\n",
      "\n",
      "TDL\n",
      "Number of nodes: 13184\n",
      "Number of edges: 56209\n",
      "Average edge weight: 1.0498\n",
      "Density: 0.00032\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths for UIL and TDL temporary and output files\n",
    "uil_temp_file = \"cslasl-pre/edges/uil_temp_edges.txt\"\n",
    "tdl_temp_file = \"cslasl-pre/edges/tdl_temp_edges.txt\"\n",
    "\n",
    "uil_sorted_file = \"cslasl-pre/edges/uil_temp_edges_sorted.txt\"\n",
    "tdl_sorted_file = \"cslasl-pre/edges/tdl_temp_edges_sorted.txt\"\n",
    "\n",
    "uil_output_file = \"network/uil/edges.txt\"\n",
    "tdl_output_file = \"network/tdl/edges.txt\"\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(os.path.dirname(uil_output_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(tdl_output_file), exist_ok=True)\n",
    "\n",
    "def edge_exists_in_file(file_path, edge):\n",
    "    \"\"\"Check if an edge exists in the file without loading the entire file into memory.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return False\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(edge):  # Faster string match check\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Process comments and generate only new edges\n",
    "with open(uil_temp_file, \"w\", encoding=\"utf-8\") as fout_uil, \\\n",
    "     open(tdl_temp_file, \"w\", encoding=\"utf-8\") as fout_tdl:\n",
    "\n",
    "    progress_bar = tqdm(total=len(comments), desc=\"Processing comments for UIL and TDL\", unit=\"comment\")\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        try:\n",
    "            parent = comment.parent_id.split(\"_\")[1]  # Extract parent ID\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Determine the reply-to author: check submissions first, then comments\n",
    "        reply_to = None\n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except Exception:\n",
    "            try:\n",
    "                reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "            except Exception:\n",
    "                reply_to = None\n",
    "\n",
    "        if reply_to:\n",
    "            edge_str = f\"{author};{reply_to}\"\n",
    "            edge_tdl_str = f\"{author};{reply_to};{comment.created_utc}\"\n",
    "\n",
    "            # Write only new UIL edges\n",
    "            if not edge_exists_in_file(uil_output_file, edge_str):\n",
    "                fout_uil.write(f\"{edge_str}\\n\")\n",
    "\n",
    "            # Write only new TDL edges with timestamp\n",
    "            if not edge_exists_in_file(tdl_output_file, edge_str):\n",
    "                fout_tdl.write(f\"{edge_tdl_str}\\n\")\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "# --------------------------\n",
    "# Process UIL Temporary File\n",
    "# --------------------------\n",
    "subprocess.run([\"sort\", uil_temp_file, \"-o\", uil_sorted_file])\n",
    "\n",
    "# Aggregate sorted UIL edges: count occurrences (i.e., weight).\n",
    "with open(uil_sorted_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(uil_output_file, \"a\", encoding=\"utf-8\") as fout:  # Append mode\n",
    "\n",
    "    current_edge = None\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        edge = line.strip()  # Format: \"author;reply_to\"\n",
    "        if current_edge is None:\n",
    "            current_edge = edge\n",
    "            count = 1\n",
    "        elif edge == current_edge:\n",
    "            count += 1\n",
    "        else:\n",
    "            fout.write(f\"{current_edge};{count}\\n\")\n",
    "            current_edge = edge\n",
    "            count = 1\n",
    "    if current_edge is not None:\n",
    "        fout.write(f\"{current_edge};{count}\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Process TDL Temporary File\n",
    "# --------------------------\n",
    "subprocess.run([\"sort\", \"-t\", \";\", \"-k1,1\", \"-k2,2\", \"-k3,3\", tdl_temp_file, \"-o\", tdl_sorted_file])\n",
    "\n",
    "DELTA_T = 3600  # 1 hour\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    if not timestamps:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "# Aggregate sorted TDL edges: compute burst weight\n",
    "with open(tdl_sorted_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(tdl_output_file, \"a\", encoding=\"utf-8\") as fout:  # Append mode\n",
    "\n",
    "    current_edge = None  # tuple: (author, reply_to)\n",
    "    timestamps = []\n",
    "\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\";\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        author, reply_to, ts_str = parts\n",
    "        try:\n",
    "            ts = float(ts_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        edge = (author, reply_to)\n",
    "        if current_edge is None:\n",
    "            current_edge = edge\n",
    "            timestamps = [ts]\n",
    "        elif edge == current_edge:\n",
    "            timestamps.append(ts)\n",
    "        else:\n",
    "            burst_weight = compute_sliding_window_max(sorted(timestamps), window=DELTA_T)\n",
    "            fout.write(f\"{current_edge[0]};{current_edge[1]};{burst_weight}\\n\")\n",
    "            current_edge = edge\n",
    "            timestamps = [ts]\n",
    "    if current_edge is not None:\n",
    "        burst_weight = compute_sliding_window_max(sorted(timestamps), window=DELTA_T)\n",
    "        fout.write(f\"{current_edge[0]};{current_edge[1]};{burst_weight}\\n\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(uil_temp_file)\n",
    "os.remove(uil_sorted_file)\n",
    "os.remove(tdl_temp_file)\n",
    "os.remove(tdl_sorted_file)\n",
    "\n",
    "print(\"UIL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/uil/edges.txt\")\n",
    "print(\"\\n\")\n",
    "print(\"TDL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/tdl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5966973-e81b-4d6b-81aa-d656a3feafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD CSL\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------\n",
    "# AGGREGATE TEXTS PER AUTHOR\n",
    "# --------------------------\n",
    "def aggregate_texts(df, text_fields):\n",
    "    \"\"\"Concatenate selected fields into a single string per row.\"\"\"\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions[\"full_text\"] = aggregate_texts(submissions, [\"title\", \"selftext\"])\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "submissions[\"selftext\"] = submissions[\"selftext\"].fillna(\"\")\n",
    "\n",
    "contents_dir = \"cslasl-pre/contents_individual\"\n",
    "os.makedirs(contents_dir, exist_ok=True)\n",
    "\n",
    "# Final merged output file (one line per author, in order)\n",
    "final_output_file = \"cslasl-pre/contents.txt\"\n",
    "\n",
    "with open(\"network/authors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # Keep order; also create a set for fast membership check.\n",
    "    authors_ordered = [line.strip() for line in f if line.strip()]\n",
    "authors_set = set(authors_ordered)\n",
    "\n",
    "def write_author_text(author, text):\n",
    "    \"\"\"Append the cleaned text for an author to the corresponding file.\"\"\"\n",
    "    # For safety, you might sanitize the author name if needed.\n",
    "    filepath = os.path.join(contents_dir, f\"{author}.txt\")\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as fout:\n",
    "        fout.write(text + \"\\n\")\n",
    "\n",
    "# --- PROCESS SUBMISSIONS ---\n",
    "for submission in submissions.itertuples():\n",
    "    author = submission.author\n",
    "    if author in authors_set:\n",
    "        full_text = clean(f\"{submission.title} {submission.selftext}\")\n",
    "        write_author_text(author, full_text)\n",
    "\n",
    "# --- PROCESS COMMENTS ---\n",
    "for comment in comments.itertuples():\n",
    "    author = comment.author\n",
    "    if author in authors_set:\n",
    "        body = clean(comment.body)\n",
    "        write_author_text(author, body)\n",
    "\n",
    "# --- MERGE PER-AUTHOR FILES INTO A SINGLE OUTPUT ---\n",
    "# This writes each merged text in the same order as in authors_ordered.\n",
    "with open(final_output_file, \"w\", encoding=\"utf-8\") as fout_final:\n",
    "    for author in authors_ordered:\n",
    "        filepath = os.path.join(contents_dir, f\"{author}.txt\")\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                # Merge all lines for this author into one string.\n",
    "                merged_text = \" \".join(line.strip() for line in f if line.strip())\n",
    "            fout_final.write(merged_text + \"\\n\")\n",
    "        else:\n",
    "            # If no texts were found for this author, write an empty line.\n",
    "            fout_final.write(\"\\n\")\n",
    "\n",
    "if os.path.isfile('cslasl-pre/csl_embeddings.pkl'):\n",
    "    with open('cslasl-pre/csl_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "else:\n",
    "    # Read the contents from the final output file\n",
    "    with open(final_output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents = f.readlines()  # Read all lines from the file\n",
    "    \n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True, device=\"cuda:1\")\n",
    "    embeddings = model.encode(contents, normalize=True)\n",
    "    \n",
    "    with open('cslasl-pre/csl_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "threshold = 0.4\n",
    "edges_file = \"cslasl-pre/edges/edges.txt\"\n",
    "os.makedirs(os.path.dirname(edges_file), exist_ok=True)\n",
    "\n",
    "num_users = len(authors)\n",
    "\n",
    "# --- STEP 1: Write edges to a text file (line by line) ---\n",
    "with open(edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    progress_bar = tqdm(total=num_users, desc=\"Processing authors\", unit=\"author\")\n",
    "    for i in range(num_users):\n",
    "        # Compare author i with all later authors (to avoid duplicates)\n",
    "        for j in range(i+1, num_users):\n",
    "            sim = np.dot(embeddings[i], embeddings[j])\n",
    "            if sim > threshold:\n",
    "                # Write as \"author1;author2;similarity\"\n",
    "                fout.write(f\"{authors[i]};{authors[j]};{sim}\\n\")\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "del embeddings\n",
    "\n",
    "# Initialize a TDigest instance.\n",
    "digest = TDigest()\n",
    "\n",
    "with open(edges_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\";\")\n",
    "        if len(parts) == 3:\n",
    "            try:\n",
    "                sim_val = float(parts[2])\n",
    "                digest.update(sim_val)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Compute the 90th percentile using the digest.\n",
    "tau_c_approx = digest.percentile(90)\n",
    "\n",
    "if tau_c_approx is not None:\n",
    "    tau_c = custom_round(tau_c_approx)  # Apply custom rounding.\n",
    "    print(\"Estimated tau_c (90th percentile):\", tau_c)\n",
    "else:\n",
    "    print(\"No similarity values above threshold were recorded.\")\n",
    "\n",
    "# --- STEP 3: Filter edges for those with weight greater than tau_c ---\n",
    "\n",
    "if tau_c is not None:\n",
    "    filtered_edges_file = \"network/csl/edges.txt\"\n",
    "    with open(edges_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(filtered_edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    weight = float(parts[2])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if weight > tau_c:\n",
    "                    fout.write(line)\n",
    "\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/csl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81ee37c0-4894-48f6-889d-66a6ee152027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 13184/13184 [04:08<00:00, 53.00author/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_a (90th percentile): 0.999\n",
      "Filtered edges written to network/asl/edges.txt\n",
      "Number of nodes: 13184\n",
      "Number of edges: 3170370\n",
      "Average edge weight: 0.9995\n",
      "Density: 0.01824\n"
     ]
    }
   ],
   "source": [
    "# BUILD ASL\n",
    "\n",
    "if os.path.isfile(\"cslasl-pre/edges/authors_sentiments.pkl\"):\n",
    "    with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'rb') as f:\n",
    "        authors_sentiments = pickle.load(f)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "    \n",
    "    def get_sentiment_vector(text):\n",
    "        \"\"\"Compute sentiment vector from RoBERTa model.\"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits  # Get logits\n",
    "        return output.squeeze().cpu().numpy()\n",
    "    \n",
    "    authors_sentiments = {}\n",
    "    for i in range(0, len(authors)):\n",
    "        authors_sentiments[authors[i]] = get_sentiment_vector(contents[i])\n",
    "    \n",
    "    with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'wb') as f:\n",
    "        pickle.dump(authors_sentiments, f)\n",
    "\n",
    "if os.path.isfile(\"cslasl-pre/edges/normalized_matrix.pkl\"):\n",
    "    with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'rb') as f:\n",
    "        normalized_matrix = pickle.load(f)\n",
    "else:\n",
    "    # Convert sentiment vectors to NumPy\n",
    "    sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "    norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "    valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "    normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "    normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "    \n",
    "    with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'wb') as f:\n",
    "        pickle.dump(normalized_matrix, f)\n",
    "\n",
    "del authors_sentiments\n",
    "\n",
    "threshold_sent = 0.7  # Only write edges with similarity > 0.5.\n",
    "edges_file = \"cslasl-pre/edges/edges_sentiments.txt\"\n",
    "os.makedirs(os.path.dirname(edges_file), exist_ok=True)\n",
    "\n",
    "num_users = len(authors)\n",
    "\n",
    "# --- STEP 1: Write edges to a text file (line by line) ---\n",
    "with open(edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    progress_bar = tqdm(total=num_users, desc=\"Processing authors\", unit=\"author\")\n",
    "    for i in range(num_users):\n",
    "        for j in range(i+1, num_users):\n",
    "            sim = np.dot(normalized_matrix[i], normalized_matrix[j])\n",
    "            if sim > threshold_sent:\n",
    "                fout.write(f\"{authors[i]};{authors[j]};{sim}\\n\")\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "# Free memory if needed.\n",
    "del normalized_matrix\n",
    "\n",
    "# --- STEP 2: Compute empirical tau_a using a TDigest ---\n",
    "digest = TDigest()\n",
    "with open(edges_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\";\")\n",
    "        if len(parts) == 3:\n",
    "            try:\n",
    "                sim_val = float(parts[2])\n",
    "                digest.update(sim_val)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "tau_a_approx = digest.percentile(90)  # 90th percentile.\n",
    "if tau_a_approx is not None:\n",
    "    TAU_A = custom_round(tau_a_approx)\n",
    "    print(\"Estimated tau_a (90th percentile):\", TAU_A)\n",
    "else:\n",
    "    print(\"No similarity values above threshold were recorded.\")\n",
    "    TAU_A = None\n",
    "\n",
    "# --- STEP 3: Filter edges for those with weight greater than TAU_A ---\n",
    "if TAU_A is not None:\n",
    "    filtered_edges_file = \"network/asl/edges.txt\"\n",
    "    os.makedirs(os.path.dirname(filtered_edges_file), exist_ok=True)\n",
    "    with open(edges_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(filtered_edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    weight = float(parts[2])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if weight > TAU_A:\n",
    "                    fout.write(line)\n",
    "    print(f\"Filtered edges written to {filtered_edges_file}\")\n",
    "\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/asl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c4d928-361f-438c-a8c6-07cd6e3cd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 13184/13184 [00:00<00:00, 123436.78author/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of interlayer edges: 79104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the interlayer coupling weight.\n",
    "omega = 1.0\n",
    "\n",
    "# Define the layers.\n",
    "layers = [\"UIL\", \"TDL\", \"CSL\", \"ASL\"]\n",
    "\n",
    "# Paths for the authors file and the output interlayer edges file.\n",
    "authors_file = \"network/authors.txt\"\n",
    "interlayer_edges_file = \"network/interlayer_edges.txt\"\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "os.makedirs(os.path.dirname(interlayer_edges_file), exist_ok=True)\n",
    "\n",
    "# Read authors (one author per line, preserving order).\n",
    "with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    authors = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Open the interlayer edges file for writing.\n",
    "edge_count = 0\n",
    "with open(interlayer_edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    # For each author, generate an interlayer edge for every distinct pair of layers.\n",
    "    for author in tqdm(authors, desc=\"Processing authors\", unit=\"author\"):\n",
    "        for layer1, layer2 in itertools.combinations(layers, 2):\n",
    "            node1 = f\"{author}@{layer1}\"\n",
    "            node2 = f\"{author}@{layer2}\"\n",
    "            fout.write(f\"{node1};{node2};{omega}\\n\")\n",
    "            edge_count += 1\n",
    "\n",
    "print(f\"Total number of interlayer edges: {edge_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715649f-c7e1-45a9-ab7e-c637a6e98cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
