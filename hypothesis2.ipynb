{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1957bce7-f533-4059-b9cf-5380057d8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def compute_TBI_from_tdl(event_folder):\n",
    "    \"\"\"\n",
    "    Compute the Temporal Burst Influence (TBI) for each user using the TDL edges.\n",
    "    The TDL layer contains an 'edges.txt' file with lines formatted as:\n",
    "    author1;author2;weight\n",
    "    Here, we assume that the weight on each edge reflects the burst value of interactions.\n",
    "    \n",
    "    For each user, we collect all weights from edges in which the user appears (either as source or target).\n",
    "    Then, for each user u_i, we compute:\n",
    "      mu = mean(weight list)\n",
    "      sigma = standard deviation(weight list)\n",
    "    The revised TBI is defined as:\n",
    "      TBI(u_i) = (sigma - mu) / (sigma + mu)\n",
    "    If sigma + mu equals zero, TBI is defined as 0.\n",
    "    \n",
    "    The resulting dictionary mapping user IDs to their TBI value is saved as \"TBI_values.pkl\"\n",
    "    in the given event folder.\n",
    "    \"\"\"\n",
    "    # Path to the TDL edges file\n",
    "    tdl_file = os.path.join(event_folder, \"network\", \"tdl\", \"edges.txt\")\n",
    "    if not os.path.exists(tdl_file):\n",
    "        raise FileNotFoundError(f\"TDL edges file not found: {tdl_file}\")\n",
    "    \n",
    "    # Read the TDL edges file; expect format: author1;author2;weight\n",
    "    df = pd.read_csv(tdl_file, sep=\";\", header=None, names=[\"u\", \"v\", \"weight\"], engine='python')\n",
    "    \n",
    "    # Create a dictionary mapping each user to a list of weights from incident edges.\n",
    "    user_weights = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        u, v, w = row[\"u\"], row[\"v\"], row[\"weight\"]\n",
    "        # Add weight for user u\n",
    "        if u not in user_weights:\n",
    "            user_weights[u] = []\n",
    "        user_weights[u].append(w)\n",
    "        # Add weight for user v\n",
    "        if v not in user_weights:\n",
    "            user_weights[v] = []\n",
    "        user_weights[v].append(w)\n",
    "    \n",
    "    # Compute TBI for each user using the revised formula.\n",
    "    tbi_dict = {}\n",
    "    for user, weights in user_weights.items():\n",
    "        weights = np.array(weights)\n",
    "        mu = np.mean(weights)\n",
    "        sigma = np.std(weights)\n",
    "        denom = sigma + mu\n",
    "        if denom > 0:\n",
    "            tbi = (sigma - mu) / denom\n",
    "        else:\n",
    "            tbi = 0\n",
    "        tbi_dict[user] = tbi\n",
    "    \n",
    "    # Save the TBI dictionary to a pickle file in the event folder.\n",
    "    output_file = os.path.join(event_folder, \"TBI_values.pkl\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(tbi_dict, f)\n",
    "    print(f\"TBI values saved to: {output_file}\")\n",
    "    \n",
    "    return tbi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d98a216-4944-4b45-9a34-3c0dcfc34315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def compute_SSC_from_asl(event_folder):\n",
    "    \"\"\"\n",
    "    Compute the revised Sentiment Synchronization Coefficient (SSC) for each user using the ASL edges,\n",
    "    optimized to avoid loading the entire file into memory.\n",
    "    \n",
    "    The ASL layer contains an 'edges.txt' file with lines formatted as:\n",
    "    author1;author2;weight\n",
    "    where 'weight' represents the cosine similarity between the affective vectors of two users.\n",
    "    \n",
    "    For each user, we accumulate the squared weight of each incident edge and compute:\n",
    "      SSC(u_i) = (1 / degree(u_i)) * sum_{u_j in N(u_i)} (weight)^2\n",
    "    \n",
    "    The resulting dictionary mapping each user to its SSC value is saved as 'SSC_values.pkl'.\n",
    "    \"\"\"\n",
    "    asl_file = os.path.join(event_folder, \"network\", \"asl\", \"edges.txt\")\n",
    "    if not os.path.exists(asl_file):\n",
    "        raise FileNotFoundError(f\"ASL edges file not found: {asl_file}\")\n",
    "    \n",
    "    # Initialize an empty dictionary to accumulate squared weights.\n",
    "    user_weights_sq = {}\n",
    "    \n",
    "    # Open and process the file line-by-line to minimize memory usage.\n",
    "    with open(asl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            u, v, w_str = parts\n",
    "            try:\n",
    "                w = float(w_str)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            w_sq = w ** 2  # square the cosine similarity to emphasize strong alignment\n",
    "            \n",
    "            # Update user u\n",
    "            if u in user_weights_sq:\n",
    "                user_weights_sq[u] += w_sq\n",
    "                # Also count the occurrence for averaging\n",
    "                user_weights_sq[u + \"_count\"] += 1\n",
    "            else:\n",
    "                user_weights_sq[u] = w_sq\n",
    "                user_weights_sq[u + \"_count\"] = 1\n",
    "                \n",
    "            # Update user v\n",
    "            if v in user_weights_sq:\n",
    "                user_weights_sq[v] += w_sq\n",
    "                user_weights_sq[v + \"_count\"] += 1\n",
    "            else:\n",
    "                user_weights_sq[v] = w_sq\n",
    "                user_weights_sq[v + \"_count\"] = 1\n",
    "    \n",
    "    # Compute SSC for each user as the average of the squared weights.\n",
    "    ssc_dict = {}\n",
    "    # Iterate over keys and skip the '_count' ones.\n",
    "    for key in list(user_weights_sq.keys()):\n",
    "        if key.endswith(\"_count\"):\n",
    "            continue\n",
    "        count_key = key + \"_count\"\n",
    "        if count_key in user_weights_sq and user_weights_sq[count_key] > 0:\n",
    "            ssc_dict[key] = user_weights_sq[key] / user_weights_sq[count_key]\n",
    "        else:\n",
    "            ssc_dict[key] = 0\n",
    "\n",
    "    # Save SSC dictionary to a pickle file.\n",
    "    output_file = os.path.join(event_folder, \"SSC_values.pkl\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(ssc_dict, f)\n",
    "    print(f\"SSC values saved to: {output_file}\")\n",
    "    \n",
    "    return ssc_dict\n",
    "\n",
    "# Example usage:\n",
    "# event_folder = \"data/2008_elections\"\n",
    "# ssc_values = compute_SSC_from_asl(event_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6055519a-e0b5-4df0-a5d2-d8b3a418d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Make sure you have already downloaded nltk data:\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def compute_negative_word_distribution(event_folder, authors_perc=0.1, top_k_words=20):\n",
    "    \"\"\"\n",
    "    For the top authors by content volume, aggregate their content and compute the frequency distribution\n",
    "    of negative words based on VADER's lexicon (words with sentiment scores < -0.8).\n",
    "    \n",
    "    Returns:\n",
    "        neg_word_freq (list): Top negative words with their frequencies.\n",
    "        radical_keywords (list): Top negative words (longer than 3 characters) to be used as radical keywords.\n",
    "    \"\"\"\n",
    "    authors_file = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "    contents_file = os.path.join(event_folder, \"cslasl-pre\", \"contents.txt\")\n",
    "    \n",
    "    with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        authors_list = [line.strip() for line in f if line.strip()]\n",
    "    with open(contents_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents_list = [line for line in f if line]\n",
    "        \n",
    "    if len(authors_list) != len(contents_list):\n",
    "        raise ValueError(\"Mismatch between number of authors and contents.\")\n",
    "    \n",
    "    data_df = pd.DataFrame({\"author\": authors_list, \"content\": contents_list})\n",
    "    aggregated_text = \" \".join(data_df[\"content\"].tolist())\n",
    "    aggregated_text = aggregated_text.lower()\n",
    "    aggregated_text = re.sub(r'[^a-z\\s]', ' ', aggregated_text)\n",
    "    tokens = word_tokenize(aggregated_text)\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    vader_lexicon = sia.lexicon\n",
    "    \n",
    "    negative_tokens = [token for token in tokens if token in vader_lexicon and vader_lexicon[token] < -0.8]\n",
    "    freq_counter = Counter(negative_tokens)\n",
    "    neg_word_freq = freq_counter.most_common(top_k_words)\n",
    "    radical_keywords = [word for word, freq in neg_word_freq if len(word) > 3]\n",
    "    \n",
    "    return neg_word_freq, radical_keywords\n",
    "\n",
    "def analyze_tbi_ssc(tbi_dict, ssc_dict, event_folder, radical_keywords):\n",
    "    \"\"\"\n",
    "    This function performs the following steps:\n",
    "      1. Loads the authors and contents from the event folder.\n",
    "      2. Computes a radicalization score for each author based on the frequency of radical keywords.\n",
    "      3. Creates a binary radicalization label using the median radicalization score.\n",
    "      4. Merges the TBI and SSC values (provided as dictionaries) with the radicalization data.\n",
    "      5. Adds an interaction term (TBI * SSC) to capture the synergistic effect.\n",
    "      6. Generates visualizations (histograms and boxplots) of TBI, SSC, and their interaction by radicalization status.\n",
    "      7. Performs logistic regression with TBI, SSC, and the interaction term as predictors of radicalization.\n",
    "      8. Computes Pearson correlation coefficients.\n",
    "      \n",
    "    The merged DataFrame and statistical results are saved for further analysis.\n",
    "    \"\"\"\n",
    "    # Load authors and contents\n",
    "    authors_file = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "    contents_file = os.path.join(event_folder, \"cslasl-pre\", \"contents.txt\")\n",
    "    \n",
    "    with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        authors = [line.strip() for line in f if line.strip()]\n",
    "    with open(contents_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents = [line for line in f if line]\n",
    "    \n",
    "    if len(authors) != len(contents):\n",
    "        raise ValueError(\"Mismatch between number of authors and contents.\")\n",
    "    \n",
    "    rad_df = pd.DataFrame({\"author\": authors, \"content\": contents})\n",
    "    \n",
    "    # Compute radicalization score based on frequency of radical keywords\n",
    "    def compute_radical_score(text):\n",
    "        text_lower = text.lower()\n",
    "        score = 0\n",
    "        for kw in radical_keywords:\n",
    "            score += len(re.findall(r'\\b' + re.escape(kw) + r'\\b', text_lower))\n",
    "        words = text_lower.split()\n",
    "        return score / len(words) if words else 0.0\n",
    "    \n",
    "    rad_df[\"rad_score\"] = rad_df[\"content\"].apply(compute_radical_score)\n",
    "    \n",
    "    # Create binary radicalization label using the median of the radical score\n",
    "    median_score = rad_df[\"rad_score\"].median()\n",
    "    rad_df[\"radical\"] = (rad_df[\"rad_score\"] > median_score).astype(int)\n",
    "    \n",
    "    # Merge TBI and SSC dictionaries with radicalization data\n",
    "    tbi_df = pd.DataFrame(list(tbi_dict.items()), columns=[\"author\", \"TBI\"])\n",
    "    ssc_df = pd.DataFrame(list(ssc_dict.items()), columns=[\"author\", \"SSC\"])\n",
    "    merged_df = pd.merge(rad_df, tbi_df, on=\"author\", how=\"inner\")\n",
    "    merged_df = pd.merge(merged_df, ssc_df, on=\"author\", how=\"inner\")\n",
    "    \n",
    "    # Ensure numeric types\n",
    "    merged_df[\"TBI\"] = pd.to_numeric(merged_df[\"TBI\"], errors=\"coerce\")\n",
    "    merged_df[\"SSC\"] = pd.to_numeric(merged_df[\"SSC\"], errors=\"coerce\")\n",
    "    merged_df[\"radical\"] = merged_df[\"radical\"].astype(int)\n",
    "    \n",
    "    # Create interaction term between TBI and SSC\n",
    "    merged_df[\"TBI_SSC\"] = merged_df[\"TBI\"] * merged_df[\"SSC\"]\n",
    "    \n",
    "    # Visualization: Histograms and Boxplots for TBI, SSC, and Interaction Term\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(data=merged_df, x=\"TBI\", hue=\"radical\", kde=True, bins=30)\n",
    "    plt.title(\"Distribution of Temporal Burst Influence (TBI) by Radicalization Status\")\n",
    "    plt.xlabel(\"TBI\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(os.path.join(event_folder, \"TBI_distribution_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x=\"radical\", y=\"TBI\", data=merged_df)\n",
    "    plt.title(\"TBI by Radicalization Status\")\n",
    "    plt.xlabel(\"Radicalization Status (0 = Non-Radicalized, 1 = Radicalized)\")\n",
    "    plt.ylabel(\"TBI\")\n",
    "    plt.savefig(os.path.join(event_folder, \"TBI_boxplot_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(data=merged_df, x=\"SSC\", hue=\"radical\", kde=True, bins=30)\n",
    "    plt.title(\"Distribution of Sentiment Synchronization Coefficient (SSC) by Radicalization Status\")\n",
    "    plt.xlabel(\"SSC\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(os.path.join(event_folder, \"SSC_distribution_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x=\"radical\", y=\"SSC\", data=merged_df)\n",
    "    plt.title(\"SSC by Radicalization Status\")\n",
    "    plt.xlabel(\"Radicalization Status (0 = Non-Radicalized, 1 = Radicalized)\")\n",
    "    plt.ylabel(\"SSC\")\n",
    "    plt.savefig(os.path.join(event_folder, \"SSC_boxplot_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(data=merged_df, x=\"TBI_SSC\", hue=\"radical\", kde=True, bins=30)\n",
    "    plt.title(\"Distribution of TBI × SSC (Interaction) by Radicalization Status\")\n",
    "    plt.xlabel(\"TBI × SSC\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(os.path.join(event_folder, \"Interaction_distribution_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x=\"radical\", y=\"TBI_SSC\", data=merged_df)\n",
    "    plt.title(\"Interaction (TBI × SSC) by Radicalization Status\")\n",
    "    plt.xlabel(\"Radicalization Status (0 = Non-Radicalized, 1 = Radicalized)\")\n",
    "    plt.ylabel(\"TBI × SSC\")\n",
    "    plt.savefig(os.path.join(event_folder, \"Interaction_boxplot_radical.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Logistic Regression Analysis with TBI, SSC, and their Interaction as predictors.\n",
    "    merged_df[\"intercept\"] = 1.0\n",
    "    predictors = [\"intercept\", \"TBI\", \"SSC\", \"TBI_SSC\"]\n",
    "    X = merged_df[predictors]\n",
    "    y = merged_df[\"radical\"]\n",
    "    \n",
    "    logit_model = sm.Logit(y, X)\n",
    "    try:\n",
    "        logit_result = logit_model.fit(disp=False)\n",
    "    except Exception as e:\n",
    "        print(\"Logistic regression failed:\", e)\n",
    "        logit_result = None\n",
    "    \n",
    "    # Pearson Correlations.\n",
    "    corr_tbi, pval_tbi = pearsonr(merged_df[\"TBI\"], merged_df[\"radical\"])\n",
    "    corr_ssc, pval_ssc = pearsonr(merged_df[\"SSC\"], merged_df[\"radical\"])\n",
    "    corr_inter, pval_inter = pearsonr(merged_df[\"TBI_SSC\"], merged_df[\"radical\"])\n",
    "    \n",
    "    # Save the merged DataFrame for further analysis.\n",
    "    merged_df.to_csv(os.path.join(event_folder, \"TBI_SSC_radicalization_analysis.csv\"), index=False)\n",
    "    \n",
    "    print(\"Logistic Regression Summary (with TBI, SSC, and interaction):\")\n",
    "    if logit_result is not None:\n",
    "        print(logit_result.summary())\n",
    "    print(f\"Pearson correlation between TBI and radicalization: {corr_tbi:.3f} (p={pval_tbi:.3e})\")\n",
    "    print(f\"Pearson correlation between SSC and radicalization: {corr_ssc:.3f} (p={pval_ssc:.3e})\")\n",
    "    print(f\"Pearson correlation between TBI×SSC and radicalization: {corr_inter:.3f} (p={pval_inter:.3e})\")\n",
    "    \n",
    "    # Optionally, produce a contour plot of predicted radicalization probability\n",
    "    # over a grid of TBI and SSC values.\n",
    "    tbi_range = np.linspace(merged_df[\"TBI\"].min(), merged_df[\"TBI\"].max(), 50)\n",
    "    ssc_range = np.linspace(merged_df[\"SSC\"].min(), merged_df[\"SSC\"].max(), 50)\n",
    "    TBI_grid, SSC_grid = np.meshgrid(tbi_range, ssc_range)\n",
    "    grid_df = pd.DataFrame({\n",
    "        \"TBI\": TBI_grid.ravel(),\n",
    "        \"SSC\": SSC_grid.ravel()\n",
    "    })\n",
    "    grid_df[\"TBI_SSC\"] = grid_df[\"TBI\"] * grid_df[\"SSC\"]\n",
    "    grid_df[\"intercept\"] = 1.0\n",
    "    X_grid = grid_df[[\"intercept\", \"TBI\", \"SSC\", \"TBI_SSC\"]]\n",
    "    \n",
    "    if logit_result is not None:\n",
    "        grid_df[\"predicted_prob\"] = logit_result.predict(X_grid)\n",
    "        grid_df = grid_df.reshape((50, 50, -1))  # Not strictly needed, we can reshape the predicted_prob.\n",
    "        Z = logit_result.predict(X_grid).values.reshape(50, 50)\n",
    "        plt.figure(figsize=(8,6))\n",
    "        cp = plt.contourf(TBI_grid, SSC_grid, Z, levels=20, cmap='viridis')\n",
    "        plt.colorbar(cp)\n",
    "        plt.xlabel(\"TBI\")\n",
    "        plt.ylabel(\"SSC\")\n",
    "        plt.title(\"Predicted Radicalization Probability\\n(TBI and SSC Interaction)\")\n",
    "        plt.savefig(os.path.join(event_folder, \"predicted_probability_contour.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    return merged_df, logit_result, (corr_tbi, corr_ssc, corr_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c79f2b-559e-4af4-a436-a35589f9711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBI values saved to: data/2008_elections/TBI_values.pkl\n",
      "SSC values saved to: data/2008_elections/SSC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "tbi_2008 = compute_TBI_from_tdl(\"data/2008_elections\")\n",
    "ssc_2008 = compute_SSC_from_asl(\"data/2008_elections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a8acb0-1e14-41ce-9de5-2224d500c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radical Keywords: ['wrong', 'fuck', 'shit', 'stop', 'problem', 'crisis', 'stupid', 'hate', 'lies', 'anti', 'fraud', 'hell', 'poor', 'attack', 'racist', 'terrorist', 'lost', 'argument', 'lose', 'fear', 'crap', 'bullshit', 'worse', 'problems', 'crazy', 'damn', 'weapons', 'racism', 'doubt', 'illegal', 'attacks', 'fight', 'kill']\n",
      "Logistic Regression Summary (TBI and SSC as predictors):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:                11817\n",
      "Model:                          Logit   Df Residuals:                    11814\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                 0.09459\n",
      "Time:                        10:53:28   Log-Likelihood:                -7367.2\n",
      "converged:                       True   LL-Null:                       -8136.9\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept  -3395.7892    172.232    -19.716      0.000   -3733.357   -3058.221\n",
      "TBI            4.6186      0.177     26.029      0.000       4.271       4.966\n",
      "SSC         3403.1892    172.394     19.741      0.000    3065.303    3741.075\n",
      "==============================================================================\n",
      "Pearson correlation between TBI and radicalization: 0.288 (p=6.706e-224)\n",
      "Pearson correlation between SSC and radicalization: 0.200 (p=2.404e-107)\n"
     ]
    }
   ],
   "source": [
    "# Compute negative word distribution to extract radical keywords.\n",
    "neg_word_freq, radical_keywords = compute_negative_word_distribution(\"data/2008_elections\", authors_perc=0.1, top_k_words=40)\n",
    "print(\"Radical Keywords:\", radical_keywords)\n",
    "\n",
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2008, ssc_2008, \"data/2008_elections\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19b77cfb-0de1-40f6-ad25-d105fad5d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBI values saved to: data/2011_wallstreet/TBI_values.pkl\n",
      "SSC values saved to: data/2011_wallstreet/SSC_values.pkl\n",
      "Radical Keywords: ['protest', 'protesters', 'protests', 'problem', 'shit', 'wrong', 'fuck', 'stop', 'protesting', 'violence', 'poor', 'arrested', 'problems', 'anti', 'violent', 'debt', 'argument', 'stupid', 'fight', 'hate', 'hell', 'bullshit', 'arrest', 'blame', 'illegal', 'disagree', 'greed', 'fail', 'worse', 'lack', 'lose', 'lost', 'lower', 'risk', 'fighting']\n",
      "Logistic Regression Summary (TBI and SSC as predictors):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:                 1396\n",
      "Model:                          Logit   Df Residuals:                     1393\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                0.004911\n",
      "Time:                        11:17:54   Log-Likelihood:                -937.89\n",
      "converged:                       True   LL-Null:                       -942.52\n",
      "Covariance Type:            nonrobust   LLR p-value:                  0.009769\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   -257.5211    483.707     -0.532      0.594   -1205.569     690.527\n",
      "TBI            0.8282      0.287      2.885      0.004       0.266       1.391\n",
      "SSC          258.8861    484.131      0.535      0.593    -689.994    1207.766\n",
      "==============================================================================\n",
      "Pearson correlation between TBI and radicalization: 0.079 (p=3.060e-03)\n",
      "Pearson correlation between SSC and radicalization: 0.021 (p=4.226e-01)\n"
     ]
    }
   ],
   "source": [
    "tbi_2011 = compute_TBI_from_tdl(\"data/2011_wallstreet\")\n",
    "ssc_2011 = compute_SSC_from_asl(\"data/2011_wallstreet\")\n",
    "\n",
    "# Compute negative word distribution to extract radical keywords.\n",
    "neg_word_freq, radical_keywords = compute_negative_word_distribution(\"data/2011_wallstreet\", authors_perc=0.1, top_k_words=40)\n",
    "print(\"Radical Keywords:\", radical_keywords)\n",
    "\n",
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2011, ssc_2011, \"data/2011_wallstreet\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95327d7b-148b-4920-b529-38d1b8ad355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBI values saved to: data/2016_elections/TBI_values.pkl\n",
      "SSC values saved to: data/2016_elections/SSC_values.pkl\n",
      "Radical Keywords: ['fake', 'shit', 'fuck', 'wrong', 'stop', 'lost', 'racist', 'hate', 'problem', 'anti', 'rapist', 'stupid', 'illegal', 'bullshit', 'worse', 'hell', 'lose', 'argument', 'blame', 'crazy', 'damn', 'poor', 'attack', 'fraud', 'conspiracy', 'doubt', 'fight', 'rape', 'worst', 'rigged', 'lies', 'fucked', 'lying']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analyze_tbi_ssc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRadical Keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m, radical_keywords)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Analyze the relationship between TBI, SSC, and radicalization.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m merged_df, logit_result, correlations \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_tbi_ssc\u001b[49m(tbi_2016, ssc_2016, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/2016_elections\u001b[39m\u001b[38;5;124m\"\u001b[39m, radical_keywords)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_tbi_ssc' is not defined"
     ]
    }
   ],
   "source": [
    "tbi_2016 = compute_TBI_from_tdl(\"data/2016_elections\")\n",
    "ssc_2016 = compute_SSC_from_asl(\"data/2016_elections\")\n",
    "\n",
    "# Compute negative word distribution to extract radical keywords.\n",
    "neg_word_freq, radical_keywords = compute_negative_word_distribution(\"data/2016_elections\", authors_perc=0.1, top_k_words=40)\n",
    "print(\"Radical Keywords:\", radical_keywords)\n",
    "\n",
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2016, ssc_2016, \"data/2016_elections\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa67958b-1514-4cdb-b466-b0a0a9129fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Summary (with TBI, SSC, and interaction):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               288877\n",
      "Model:                          Logit   Df Residuals:                   288873\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                 -0.5938\n",
      "Time:                        14:48:50   Log-Likelihood:            -3.1908e+05\n",
      "converged:                      False   LL-Null:                   -2.0020e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   1.936e+05   1979.889     97.805      0.000     1.9e+05    1.98e+05\n",
      "TBI         2.006e+05   1981.048    101.280      0.000    1.97e+05    2.05e+05\n",
      "SSC        -1.938e+05   1981.464    -97.804      0.000   -1.98e+05    -1.9e+05\n",
      "TBI_SSC    -2.008e+05   1982.623   -101.280      0.000   -2.05e+05   -1.97e+05\n",
      "==============================================================================\n",
      "Pearson correlation between TBI and radicalization: 0.401 (p=0.000e+00)\n",
      "Pearson correlation between SSC and radicalization: 0.295 (p=0.000e+00)\n",
      "Pearson correlation between TBI×SSC and radicalization: 0.401 (p=0.000e+00)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1142129/2675222479.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Analyze the relationship between TBI, SSC, and radicalization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_tbi_ssc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbi_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssc_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/2016_elections\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1142129/1518235134.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tbi_dict, ssc_dict, event_folder, radical_keywords)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mX_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intercept\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SSC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI_SSC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogit_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Not strictly needed, we can reshape the predicted_prob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTBI_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSC_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2016, ssc_2016, \"data/2016_elections\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e33ca4-2296-416a-acb4-aa0fed5a77f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBI values saved to: data/2017_rally/TBI_values.pkl\n",
      "SSC values saved to: data/2017_rally/SSC_values.pkl\n",
      "Radical Keywords: ['racist', 'shit', 'violence', 'hate', 'fuck', 'wrong', 'racism', 'anti', 'supremacists', 'stop', 'violent', 'problem', 'protest', 'stupid', 'attack', 'fake', 'protesters', 'racists', 'fight', 'bullshit', 'argument', 'fascist', 'killed', 'worse', 'hell', 'illegal', 'blame', 'evil', 'terrorist', 'lost', 'death', 'crime', 'kill', 'fire', 'terrorism', 'murder', 'fighting']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Summary (with TBI, SSC, and interaction):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               102925\n",
      "Model:                          Logit   Df Residuals:                   102921\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                  -5.432\n",
      "Time:                        15:11:44   Log-Likelihood:            -4.5886e+05\n",
      "converged:                      False   LL-Null:                       -71342.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   5.578e+05   1.92e+04     29.028      0.000     5.2e+05    5.96e+05\n",
      "TBI         5.717e+05   1.92e+04     29.745      0.000    5.34e+05    6.09e+05\n",
      "SSC        -5.584e+05   1.92e+04    -29.028      0.000   -5.96e+05   -5.21e+05\n",
      "TBI_SSC    -5.722e+05   1.92e+04    -29.745      0.000    -6.1e+05   -5.34e+05\n",
      "==============================================================================\n",
      "Pearson correlation between TBI and radicalization: -0.003 (p=3.964e-01)\n",
      "Pearson correlation between SSC and radicalization: 0.328 (p=0.000e+00)\n",
      "Pearson correlation between TBI×SSC and radicalization: -0.003 (p=3.676e-01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/discrete/discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1142129/479528036.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneg_word_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_negative_word_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/2017_rally\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors_perc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Radical Keywords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Analyze the relationship between TBI, SSC, and radicalization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_tbi_ssc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbi_2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssc_2017\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/2017_rally\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1142129/1518235134.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tbi_dict, ssc_dict, event_folder, radical_keywords)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mX_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intercept\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SSC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI_SSC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogit_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Not strictly needed, we can reshape the predicted_prob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTBI_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSC_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "tbi_2017 = compute_TBI_from_tdl(\"data/2017_rally\")\n",
    "ssc_2017 = compute_SSC_from_asl(\"data/2017_rally\")\n",
    "\n",
    "# Compute negative word distribution to extract radical keywords.\n",
    "neg_word_freq, radical_keywords = compute_negative_word_distribution(\"data/2017_rally\", authors_perc=0.1, top_k_words=40)\n",
    "print(\"Radical Keywords:\", radical_keywords)\n",
    "\n",
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2017, ssc_2017, \"data/2017_rally\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6eee74-122d-45a0-af3f-1692773a00bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBI values saved to: data/2021_riot/TBI_values.pkl\n",
      "SSC values saved to: data/2021_riot/SSC_values.pkl\n",
      "Radical Keywords: ['shit', 'fuck', 'stop', 'violence', 'riot', 'terrorists', 'wrong', 'attack', 'stupid', 'fight', 'lost', 'hate', 'conspiracy', 'fraud', 'terrorist', 'protests', 'death', 'violent', 'riots', 'hell', 'bullshit', 'problem', 'protest', 'arrested', 'killed', 'crazy', 'worse', 'fire', 'damn', 'prison', 'charges', 'died', 'lies', 'criminal', 'argument', 'crime']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Summary (with TBI, SSC, and interaction):\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               253207\n",
      "Model:                          Logit   Df Residuals:                   253203\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                  -2.529\n",
      "Time:                        16:46:24   Log-Likelihood:            -6.1191e+05\n",
      "converged:                      False   LL-Null:                   -1.7337e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     1.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept   3.746e+05   6605.347     56.712      0.000    3.62e+05    3.88e+05\n",
      "TBI         3.846e+05   6606.368     58.212      0.000    3.72e+05    3.98e+05\n",
      "SSC         -3.75e+05   6611.669    -56.712      0.000   -3.88e+05   -3.62e+05\n",
      "TBI_SSC    -3.849e+05   6612.691    -58.212      0.000   -3.98e+05   -3.72e+05\n",
      "==============================================================================\n",
      "Pearson correlation between TBI and radicalization: 0.002 (p=3.791e-01)\n",
      "Pearson correlation between SSC and radicalization: 0.286 (p=0.000e+00)\n",
      "Pearson correlation between TBI×SSC and radicalization: 0.002 (p=4.203e-01)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1142129/145601826.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneg_word_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_negative_word_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/2021_riot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors_perc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Radical Keywords:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Analyze the relationship between TBI, SSC, and radicalization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_tbi_ssc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbi_2021\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssc_2021\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/2021_riot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradical_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1142129/1518235134.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tbi_dict, ssc_dict, event_folder, radical_keywords)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mX_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"intercept\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SSC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TBI_SSC\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogit_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mgrid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Not strictly needed, we can reshape the predicted_prob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontourf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTBI_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSC_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "tbi_2021 = compute_TBI_from_tdl(\"data/2021_riot\")\n",
    "ssc_2021 = compute_SSC_from_asl(\"data/2021_riot\")\n",
    "\n",
    "# Compute negative word distribution to extract radical keywords.\n",
    "neg_word_freq, radical_keywords = compute_negative_word_distribution(\"data/2021_riot\", authors_perc=0.1, top_k_words=40)\n",
    "print(\"Radical Keywords:\", radical_keywords)\n",
    "\n",
    "# Analyze the relationship between TBI, SSC, and radicalization.\n",
    "merged_df, logit_result, correlations = analyze_tbi_ssc(tbi_2021, ssc_2021, \"data/2021_riot\", radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a4ca2-96b3-4513-9e1e-db861c9f3f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
