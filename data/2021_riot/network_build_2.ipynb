{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc84f72-9659-424b-b984-a533c585b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 10:10:16.656184: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 10:10:16.667073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742202616.679965 2056015 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742202616.683570 2056015 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-17 10:10:16.696517: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import glob\n",
    "from tdigest import TDigest\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "os.makedirs('cslasl-pre', exist_ok=True)\n",
    "os.makedirs('cslasl-pre/edges', exist_ok=True)\n",
    "os.makedirs('network', exist_ok=True)\n",
    "os.makedirs('network/uil', exist_ok=True)\n",
    "os.makedirs('network/tdl', exist_ok=True)\n",
    "os.makedirs('network/csl', exist_ok=True)\n",
    "os.makedirs('network/asl', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81c2bc9-61e7-4d15-8976-3ec8c67042a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_metrics(authors_path, edges_path):\n",
    "    # Count nodes from the authors file.\n",
    "    num_nodes = 0\n",
    "    with open(authors_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                num_nodes += 1\n",
    "\n",
    "    # Count edges and sum their weights from the edges file.\n",
    "    num_edges = 0\n",
    "    weight_sum = 0.0\n",
    "    with open(edges_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    w = float(parts[2])\n",
    "                    num_edges += 1\n",
    "                    weight_sum += w\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    avg_weight = weight_sum / num_edges if num_edges > 0 else 0.0\n",
    "    # Compute density for a directed graph: density = m / (n*(n-1)) if n > 1\n",
    "    density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "    print(f\"Average edge weight: {avg_weight:.4f}\")\n",
    "    print(f\"Density: {density:.5f}\")\n",
    "\n",
    "import io\n",
    "import zstandard as zstd\n",
    "\n",
    "def compute_network_metrics_zst(authors_path, edges_path):\n",
    "    # Count nodes from the authors file.\n",
    "    num_nodes = 0\n",
    "    with open(authors_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                num_nodes += 1\n",
    "\n",
    "    # Count edges and sum their weights from the compressed edges file.\n",
    "    num_edges = 0\n",
    "    weight_sum = 0.0\n",
    "    with open(edges_path, \"rb\") as f:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(f) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "            for line in text_stream:\n",
    "                parts = line.strip().split(\";\")\n",
    "                if len(parts) == 3:\n",
    "                    try:\n",
    "                        w = float(parts[2])\n",
    "                        num_edges += 1\n",
    "                        weight_sum += w\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    avg_weight = weight_sum / num_edges if num_edges > 0 else 0.0\n",
    "    # Compute density for a directed graph: density = m / (n*(n-1)) if n > 1\n",
    "    density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "\n",
    "    print(f\"Number of nodes: {num_nodes}\")\n",
    "    print(f\"Number of edges: {num_edges}\")\n",
    "    print(f\"Average edge weight: {avg_weight:.4f}\")\n",
    "    print(f\"Density: {density:.5f}\")\n",
    "    \n",
    "def custom_round(num):\n",
    "    # Standard round to three decimals.\n",
    "    rounded = round(num, 3)\n",
    "    # Format to a string with exactly 3 decimal places.\n",
    "    s = f\"{rounded:.3f}\"\n",
    "    # Replace the last character (third decimal) with '9'\n",
    "    s = s[:-1] + '9'\n",
    "    return float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12294dbd-e5f2-4b2d-8324-717d86873930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_submissions(submissions_file):\n",
    "        return pd.read_csv(submissions_file, compression=\"zstd\")\n",
    "    \n",
    "def load_comments(comments_file):\n",
    "    return pd.read_csv(comments_file, compression=\"zstd\")\n",
    "        \n",
    "submissions = pd.read_csv('submissions.csv.zst')\n",
    "submissions['selftext'] = submissions['selftext'].fillna(\"\")\n",
    "comments = pd.read_csv('comments.csv.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ce58e5-6c83-4821-a96f-42fbc129b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 254480\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"cslasl-pre/authors.pkl\"):\n",
    "    with open(\"cslasl-pre/authors.pkl\", \"rb\") as f:\n",
    "        authors = pickle.load(f)    \n",
    "else:\n",
    "    authors = list(submissions['author'])\n",
    "    authors.extend(list(comments['author']))\n",
    "    authors = list(set(authors))\n",
    "\n",
    "with open(\"network/authors.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for author in authors:\n",
    "        fout.write(f\"{author}\\n\")    \n",
    "\n",
    "print(f\"Number of nodes: {len(authors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f5b05-11e0-49da-ab8e-3e9f074463f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments for UIL and TDL:   8%| | 117597/1429786 [24:54:37<291:07:32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define file paths for UIL and TDL temporary and output files\n",
    "uil_temp_file = \"cslasl-pre/edges/uil_temp_edges.txt\"\n",
    "tdl_temp_file = \"cslasl-pre/edges/tdl_temp_edges.txt\"\n",
    "\n",
    "uil_sorted_file = \"cslasl-pre/edges/uil_temp_edges_sorted.txt\"\n",
    "tdl_sorted_file = \"cslasl-pre/edges/tdl_temp_edges_sorted.txt\"\n",
    "\n",
    "uil_output_file = \"network/uil/edges.txt\"\n",
    "tdl_output_file = \"network/tdl/edges.txt\"\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(os.path.dirname(uil_output_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(tdl_output_file), exist_ok=True)\n",
    "\n",
    "def edge_exists_in_file(file_path, edge):\n",
    "    \"\"\"Check if an edge exists in the file without loading the entire file into memory.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return False\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(edge):  # Faster string match check\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Process comments and generate only new edges\n",
    "with open(uil_temp_file, \"w\", encoding=\"utf-8\") as fout_uil, \\\n",
    "     open(tdl_temp_file, \"w\", encoding=\"utf-8\") as fout_tdl:\n",
    "\n",
    "    progress_bar = tqdm(total=len(comments), desc=\"Processing comments for UIL and TDL\", unit=\"comment\")\n",
    "    for comment in comments.itertuples():\n",
    "        author = comment.author\n",
    "        try:\n",
    "            parent = comment.parent_id.split(\"_\")[1]  # Extract parent ID\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Determine the reply-to author: check submissions first, then comments\n",
    "        reply_to = None\n",
    "        try:\n",
    "            reply_to = submissions.loc[submissions['id'] == parent]['author'].values[0]\n",
    "        except Exception:\n",
    "            try:\n",
    "                reply_to = comments.loc[comments['id'] == parent]['author'].values[0]\n",
    "            except Exception:\n",
    "                reply_to = None\n",
    "\n",
    "        if reply_to:\n",
    "            edge_str = f\"{author};{reply_to}\"\n",
    "            edge_tdl_str = f\"{author};{reply_to};{comment.created_utc}\"\n",
    "\n",
    "            # Write only new UIL edges\n",
    "            if not edge_exists_in_file(uil_output_file, edge_str):\n",
    "                fout_uil.write(f\"{edge_str}\\n\")\n",
    "\n",
    "            # Write only new TDL edges with timestamp\n",
    "            if not edge_exists_in_file(tdl_output_file, edge_str):\n",
    "                fout_tdl.write(f\"{edge_tdl_str}\\n\")\n",
    "\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "# --------------------------\n",
    "# Process UIL Temporary File\n",
    "# --------------------------\n",
    "subprocess.run([\"sort\", uil_temp_file, \"-o\", uil_sorted_file])\n",
    "\n",
    "# Aggregate sorted UIL edges: count occurrences (i.e., weight).\n",
    "with open(uil_sorted_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(uil_output_file, \"a\", encoding=\"utf-8\") as fout:  # Append mode\n",
    "\n",
    "    current_edge = None\n",
    "    count = 0\n",
    "    for line in fin:\n",
    "        edge = line.strip()  # Format: \"author;reply_to\"\n",
    "        if current_edge is None:\n",
    "            current_edge = edge\n",
    "            count = 1\n",
    "        elif edge == current_edge:\n",
    "            count += 1\n",
    "        else:\n",
    "            fout.write(f\"{current_edge};{count}\\n\")\n",
    "            current_edge = edge\n",
    "            count = 1\n",
    "    if current_edge is not None:\n",
    "        fout.write(f\"{current_edge};{count}\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Process TDL Temporary File\n",
    "# --------------------------\n",
    "subprocess.run([\"sort\", \"-t\", \";\", \"-k1,1\", \"-k2,2\", \"-k3,3\", tdl_temp_file, \"-o\", tdl_sorted_file])\n",
    "\n",
    "DELTA_T = 3600  # 1 hour\n",
    "\n",
    "def compute_sliding_window_max(timestamps, window=DELTA_T):\n",
    "    if not timestamps:\n",
    "        return 0\n",
    "    max_count = 0\n",
    "    start = 0\n",
    "    for end in range(len(timestamps)):\n",
    "        while timestamps[end] - timestamps[start] > window:\n",
    "            start += 1\n",
    "        count = end - start + 1\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "    return max_count\n",
    "\n",
    "# Aggregate sorted TDL edges: compute burst weight\n",
    "with open(tdl_sorted_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(tdl_output_file, \"a\", encoding=\"utf-8\") as fout:  # Append mode\n",
    "\n",
    "    current_edge = None  # tuple: (author, reply_to)\n",
    "    timestamps = []\n",
    "\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\";\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        author, reply_to, ts_str = parts\n",
    "        try:\n",
    "            ts = float(ts_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        edge = (author, reply_to)\n",
    "        if current_edge is None:\n",
    "            current_edge = edge\n",
    "            timestamps = [ts]\n",
    "        elif edge == current_edge:\n",
    "            timestamps.append(ts)\n",
    "        else:\n",
    "            burst_weight = compute_sliding_window_max(sorted(timestamps), window=DELTA_T)\n",
    "            fout.write(f\"{current_edge[0]};{current_edge[1]};{burst_weight}\\n\")\n",
    "            current_edge = edge\n",
    "            timestamps = [ts]\n",
    "    if current_edge is not None:\n",
    "        burst_weight = compute_sliding_window_max(sorted(timestamps), window=DELTA_T)\n",
    "        fout.write(f\"{current_edge[0]};{current_edge[1]};{burst_weight}\\n\")\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(uil_temp_file)\n",
    "os.remove(uil_sorted_file)\n",
    "os.remove(tdl_temp_file)\n",
    "os.remove(tdl_sorted_file)\n",
    "\n",
    "print(\"UIL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/uil/edges.txt\")\n",
    "print(\"\\n\")\n",
    "print(\"TDL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/tdl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0518e42a-503d-43fe-b6ed-f779274e132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UIL\n",
      "Number of nodes: 254480\n",
      "Number of edges: 1144819\n",
      "Average edge weight: 1.2489\n",
      "Density: 0.00002\n",
      "\n",
      "\n",
      "TDL\n",
      "Number of nodes: 254480\n",
      "Number of edges: 1144819\n",
      "Average edge weight: 1.0915\n",
      "Density: 0.00002\n"
     ]
    }
   ],
   "source": [
    "print(\"UIL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/uil/edges.txt\")\n",
    "print(\"\\n\")\n",
    "print(\"TDL\")\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/tdl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5966973-e81b-4d6b-81aa-d656a3feafb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors: 100%|██████████| 254480/254480 [18:12:56<00:00,  3.88author/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_c (90th percentile): 0.559\n",
      "Number of nodes: 254480\n",
      "Number of edges: 142582076\n",
      "Average edge weight: 0.6018\n",
      "Density: 0.00220\n"
     ]
    }
   ],
   "source": [
    "# BUILD CSL\n",
    "def clean(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# --------------------------\n",
    "# AGGREGATE TEXTS PER AUTHOR\n",
    "# --------------------------\n",
    "def aggregate_texts(df, text_fields):\n",
    "    \"\"\"Concatenate selected fields into a single string per row.\"\"\"\n",
    "    return df[text_fields].fillna(\"\").agg(\" \".join, axis=1)\n",
    "\n",
    "submissions[\"full_text\"] = aggregate_texts(submissions, [\"title\", \"selftext\"])\n",
    "comments[\"body\"] = comments[\"body\"].fillna(\"\")\n",
    "submissions[\"selftext\"] = submissions[\"selftext\"].fillna(\"\")\n",
    "\n",
    "contents_dir = \"cslasl-pre/contents_individual\"\n",
    "os.makedirs(contents_dir, exist_ok=True)\n",
    "\n",
    "# Final merged output file (one line per author, in order)\n",
    "final_output_file = \"cslasl-pre/contents.txt\"\n",
    "\n",
    "with open(\"network/authors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # Keep order; also create a set for fast membership check.\n",
    "    authors_ordered = [line.strip() for line in f if line.strip()]\n",
    "authors_set = set(authors_ordered)\n",
    "\n",
    "def write_author_text(author, text):\n",
    "    \"\"\"Append the cleaned text for an author to the corresponding file.\"\"\"\n",
    "    # For safety, you might sanitize the author name if needed.\n",
    "    filepath = os.path.join(contents_dir, f\"{author}.txt\")\n",
    "    with open(filepath, \"a\", encoding=\"utf-8\") as fout:\n",
    "        fout.write(text + \"\\n\")\n",
    "\n",
    "# --- PROCESS SUBMISSIONS ---\n",
    "for submission in submissions.itertuples():\n",
    "    author = submission.author\n",
    "    if author in authors_set:\n",
    "        full_text = clean(f\"{submission.title} {submission.selftext}\")\n",
    "        write_author_text(author, full_text)\n",
    "\n",
    "# --- PROCESS COMMENTS ---\n",
    "for comment in comments.itertuples():\n",
    "    author = comment.author\n",
    "    if author in authors_set:\n",
    "        body = clean(comment.body)\n",
    "        write_author_text(author, body)\n",
    "\n",
    "# --- MERGE PER-AUTHOR FILES INTO A SINGLE OUTPUT ---\n",
    "# This writes each merged text in the same order as in authors_ordered.\n",
    "with open(final_output_file, \"w\", encoding=\"utf-8\") as fout_final:\n",
    "    for author in authors_ordered:\n",
    "        filepath = os.path.join(contents_dir, f\"{author}.txt\")\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                # Merge all lines for this author into one string.\n",
    "                merged_text = \" \".join(line.strip() for line in f if line.strip())\n",
    "            fout_final.write(merged_text + \"\\n\")\n",
    "        else:\n",
    "            # If no texts were found for this author, write an empty line.\n",
    "            fout_final.write(\"\\n\")\n",
    "\n",
    "if os.path.isfile('cslasl-pre/csl_embeddings.pkl'):\n",
    "    with open('cslasl-pre/csl_embeddings.pkl', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "else:\n",
    "    # Read the contents from the final output file\n",
    "    with open(final_output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents = f.readlines()  # Read all lines from the file\n",
    "    \n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True, device=\"cuda:1\")\n",
    "    embeddings = model.encode(contents, normalize=True)\n",
    "    \n",
    "    with open('cslasl-pre/csl_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "threshold = 0.4\n",
    "edges_file = \"cslasl-pre/edges/edges.txt\"\n",
    "os.makedirs(os.path.dirname(edges_file), exist_ok=True)\n",
    "\n",
    "num_users = len(authors)\n",
    "\n",
    "# --- STEP 1: Write edges to a text file (line by line) ---\n",
    "with open(edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    progress_bar = tqdm(total=num_users, desc=\"Processing authors\", unit=\"author\")\n",
    "    for i in range(num_users):\n",
    "        # Compare author i with all later authors (to avoid duplicates)\n",
    "        for j in range(i+1, num_users):\n",
    "            sim = np.dot(embeddings[i], embeddings[j])\n",
    "            if sim > threshold:\n",
    "                # Write as \"author1;author2;similarity\"\n",
    "                fout.write(f\"{authors[i]};{authors[j]};{sim}\\n\")\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "del embeddings\n",
    "\n",
    "# Initialize a TDigest instance.\n",
    "digest = TDigest()\n",
    "\n",
    "with open(edges_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\";\")\n",
    "        if len(parts) == 3:\n",
    "            try:\n",
    "                sim_val = float(parts[2])\n",
    "                digest.update(sim_val)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "# Compute the 90th percentile using the digest.\n",
    "tau_c_approx = digest.percentile(90)\n",
    "\n",
    "if tau_c_approx is not None:\n",
    "    tau_c = custom_round(tau_c_approx)  # Apply custom rounding.\n",
    "    print(\"Estimated tau_c (90th percentile):\", tau_c)\n",
    "else:\n",
    "    print(\"No similarity values above threshold were recorded.\")\n",
    "\n",
    "# --- STEP 3: Filter edges for those with weight greater than tau_c ---\n",
    "\n",
    "if tau_c is not None:\n",
    "    filtered_edges_file = \"network/csl/edges.txt\"\n",
    "    with open(edges_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(filtered_edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    weight = float(parts[2])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if weight > tau_c:\n",
    "                    fout.write(line)\n",
    "\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/csl/edges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ee37c0-4894-48f6-889d-66a6ee152027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors: 100%|██████████| 254480/254480 [14:04:07<00:00,  5.02author/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tau_a (90th percentile): 1.009\n",
      "Filtered edges written to network/asl/edges.txt.zst\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb5 in position 1: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m                                 compressor\u001b[38;5;241m.\u001b[39mwrite(line\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltered edges written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiltered_edges_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mcompute_network_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnetwork/authors.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnetwork/asl/edges.txt.zst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36mcompute_network_metrics\u001b[0;34m(authors_path, edges_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m weight_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(edges_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb5 in position 1: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tdigest import TDigest\n",
    "import zstandard as zstd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# BUILD ASL\n",
    "\n",
    "# Load authors and contents\n",
    "with open(\"network/authors.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    authors = [line.strip() for line in f if line.strip()]\n",
    "with open(\"cslasl-pre/contents.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = [line for line in f if line]\n",
    "\n",
    "if os.path.isfile(\"cslasl-pre/edges/authors_sentiments.pkl\"):\n",
    "    with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'rb') as f:\n",
    "        authors_sentiments = pickle.load(f)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(\"cuda:1\")\n",
    "    \n",
    "    def get_sentiment_vector(text):\n",
    "        \"\"\"Compute sentiment vector from RoBERTa model.\"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cuda:1\")\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs).logits  # Get logits\n",
    "        return output.squeeze().cpu().numpy()\n",
    "    \n",
    "    authors_sentiments = {}\n",
    "    for i in range(len(authors)):\n",
    "        authors_sentiments[authors[i]] = get_sentiment_vector(contents[i])\n",
    "    \n",
    "    with open(\"cslasl-pre/edges/authors_sentiments.pkl\", 'wb') as f:\n",
    "        pickle.dump(authors_sentiments, f)\n",
    "\n",
    "if os.path.isfile(\"cslasl-pre/edges/normalized_matrix.pkl\"):\n",
    "    with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'rb') as f:\n",
    "        normalized_matrix = pickle.load(f)\n",
    "else:\n",
    "    # Convert sentiment vectors to NumPy\n",
    "    sentiment_matrix = np.array(list(authors_sentiments.values()))\n",
    "    norms = np.linalg.norm(sentiment_matrix, axis=1, keepdims=True)\n",
    "    valid_norms = norms.flatten() > 0  # Mask for non-zero norms\n",
    "    normalized_matrix = np.zeros_like(sentiment_matrix)\n",
    "    normalized_matrix[valid_norms] = sentiment_matrix[valid_norms] / norms[valid_norms]\n",
    "    \n",
    "    with open(\"cslasl-pre/edges/normalized_matrix.pkl\", 'wb') as f:\n",
    "        pickle.dump(normalized_matrix, f)\n",
    "\n",
    "del authors_sentiments\n",
    "\n",
    "threshold_sent = 0.8  # Only write edges with similarity > threshold.\n",
    "# Write the edges_sentiments file as a Zstandard-compressed text file.\n",
    "edges_file = \"cslasl-pre/edges/edges_sentiments.txt.zst\"\n",
    "os.makedirs(os.path.dirname(edges_file), exist_ok=True)\n",
    "\n",
    "num_users = len(authors)\n",
    "\n",
    "# --- STEP 1: Write edges to a compressed file (line by line) ---\n",
    "with open(edges_file, \"wb\") as fout:\n",
    "    cctx = zstd.ZstdCompressor()\n",
    "    with cctx.stream_writer(fout) as compressor:\n",
    "        progress_bar = tqdm(total=num_users, desc=\"Processing authors\", unit=\"author\")\n",
    "        for i in range(num_users):\n",
    "            for j in range(i+1, num_users):\n",
    "                sim = np.dot(normalized_matrix[i], normalized_matrix[j])\n",
    "                if sim > threshold_sent:\n",
    "                    line = f\"{authors[i]};{authors[j]};{sim}\\n\"\n",
    "                    compressor.write(line.encode(\"utf-8\"))\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "# Free memory if needed.\n",
    "del normalized_matrix\n",
    "\n",
    "# --- STEP 2: Compute empirical tau_a using a TDigest ---\n",
    "digest = TDigest()\n",
    "# Open the compressed file for reading.\n",
    "with open(edges_file, \"rb\") as fin:\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with dctx.stream_reader(fin) as reader:\n",
    "        text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "        for line in text_stream:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) == 3:\n",
    "                try:\n",
    "                    sim_val = float(parts[2])\n",
    "                    digest.update(sim_val)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "tau_a_approx = digest.percentile(90)  # 90th percentile.\n",
    "if tau_a_approx is not None:\n",
    "    TAU_A = custom_round(tau_a_approx)\n",
    "    print(\"Estimated tau_a (90th percentile):\", TAU_A)\n",
    "else:\n",
    "    print(\"No similarity values above threshold were recorded.\")\n",
    "    TAU_A = None\n",
    "\n",
    "# --- STEP 3: Filter edges for those with weight greater than TAU_A ---\n",
    "if TAU_A is not None:\n",
    "    filtered_edges_file = \"network/asl/edges.txt.zst\"\n",
    "    os.makedirs(os.path.dirname(filtered_edges_file), exist_ok=True)\n",
    "    # Read from the compressed edges file and write the filtered edges to a new compressed file.\n",
    "    with open(edges_file, \"rb\") as fin:\n",
    "        dctx = zstd.ZstdDecompressor()\n",
    "        with dctx.stream_reader(fin) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "            with open(filtered_edges_file, \"wb\") as fout:\n",
    "                cctx = zstd.ZstdCompressor()\n",
    "                with cctx.stream_writer(fout) as compressor:\n",
    "                    for line in text_stream:\n",
    "                        parts = line.strip().split(\";\")\n",
    "                        if len(parts) == 3:\n",
    "                            try:\n",
    "                                weight = float(parts[2])\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                            if weight > TAU_A:\n",
    "                                compressor.write(line.encode(\"utf-8\"))\n",
    "    print(f\"Filtered edges written to {filtered_edges_file}\")\n",
    "\n",
    "compute_network_metrics(\"network/authors.txt\", \"network/asl/edges.txt.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c4d928-361f-438c-a8c6-07cd6e3cd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors: 100%|█████████| 254480/254480 [00:02<00:00, 109903.61author/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of interlayer edges: 1526880\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the interlayer coupling weight.\n",
    "omega = 1.0\n",
    "\n",
    "# Define the layers.\n",
    "layers = [\"UIL\", \"TDL\", \"CSL\", \"ASL\"]\n",
    "\n",
    "# Paths for the authors file and the output interlayer edges file.\n",
    "authors_file = \"network/authors.txt\"\n",
    "interlayer_edges_file = \"network/interlayer_edges.txt\"\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "os.makedirs(os.path.dirname(interlayer_edges_file), exist_ok=True)\n",
    "\n",
    "# Read authors (one author per line, preserving order).\n",
    "with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    authors = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Open the interlayer edges file for writing.\n",
    "edge_count = 0\n",
    "with open(interlayer_edges_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    # For each author, generate an interlayer edge for every distinct pair of layers.\n",
    "    for author in tqdm(authors, desc=\"Processing authors\", unit=\"author\"):\n",
    "        for layer1, layer2 in itertools.combinations(layers, 2):\n",
    "            node1 = f\"{author}@{layer1}\"\n",
    "            node2 = f\"{author}@{layer2}\"\n",
    "            fout.write(f\"{node1};{node2};{omega}\\n\")\n",
    "            edge_count += 1\n",
    "\n",
    "print(f\"Total number of interlayer edges: {edge_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715649f-c7e1-45a9-ab7e-c637a6e98cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
