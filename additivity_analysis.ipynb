{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216a183-ea17-4639-b450-578208f02284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# -------- CONFIG --------\n",
    "EVENTS = {\n",
    "    \"2008 Elections\": \"data/2008_elections\",\n",
    "    \"2011 Occupy Wall Street\": \"data/2011_wallstreet\",\n",
    "    \"2016 Elections\": \"data/2016_elections\",\n",
    "    \"2017 Charlottesville Rally\": \"data/2017_rally\",\n",
    "    \"2021 Capitol Riot\": \"data/2021_riot\",\n",
    "}\n",
    "\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"add_vs_mul_baseline_all_events.csv\")\n",
    "OUT_FIG = os.path.join(OUT_DIR, \"fig_add_vs_mul_delta_auc_cv.png\")\n",
    "\n",
    "omega = 1.0\n",
    "beta  = {\"uil\": 1.0, \"csl\": 1.0, \"tdl\": 1.0, \"asl\": 1.0}\n",
    "LAYS  = [\"uil\",\"csl\",\"tdl\",\"asl\"]\n",
    "\n",
    "# -------- HELPERS (streaming, low-RAM) --------\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def strength_stream_edges(path, authors_set):\n",
    "    \"\"\"Somma log1p(w) su u e v per ogni riga 'u;v;w'. Sparse dict.\"\"\"\n",
    "    s = defaultdict(float)\n",
    "    if not os.path.isfile(path): return s\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            parts = line.rstrip(\"\\n\").split(\";\")\n",
    "            if len(parts) != 3: continue\n",
    "            u, v, ws = parts\n",
    "            if (u not in authors_set) or (v not in authors_set): continue\n",
    "            try:\n",
    "                w = float(ws)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            inc = math.log1p(w)\n",
    "            s[u] += inc; s[v] += inc\n",
    "    return s\n",
    "\n",
    "def effective_phi_per_layer(str_by_layer, authors, omega=1.0):\n",
    "    \"\"\"Ritorna matrice N x 4 delle effective influence per layer.\"\"\"\n",
    "    n = len(authors)\n",
    "    Ef = np.zeros((n, len(LAYS)), dtype=np.float64)\n",
    "    totals = np.zeros(n, dtype=np.float64)\n",
    "    # pre-somma totals\n",
    "    for j,l in enumerate(LAYS):\n",
    "        for i,a in enumerate(authors):\n",
    "            totals[i] += str_by_layer[l].get(a, 0.0)\n",
    "    denom = max(n-1, 1)\n",
    "    for j,l in enumerate(LAYS):\n",
    "        for i,a in enumerate(authors):\n",
    "            phi = str_by_layer[l].get(a, 0.0)\n",
    "            eff = (phi + omega * (totals[i] - phi)) / denom\n",
    "            Ef[i,j] = eff\n",
    "    return Ef\n",
    "\n",
    "def fit_alpha_ci_auc_on_score(s, y):\n",
    "    \"\"\"Logit(y ~ s). Ritorna alpha, lo, hi, auc_in.\"\"\"\n",
    "    s = (s - np.mean(s)) / (np.std(s) if np.std(s)>0 else 1.0)\n",
    "    alpha = np.nan; lo = np.nan; hi = np.nan; auc = np.nan\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X = sm.add_constant(s)\n",
    "        res = sm.Logit(y, X).fit(disp=0)\n",
    "        alpha = float(res.params[1])\n",
    "        ci = res.conf_int(alpha=0.05)\n",
    "        lo, hi = float(ci.iloc[1,0]), float(ci.iloc[1,1])\n",
    "        p = res.predict(X)\n",
    "        auc = roc_auc_score(y, p) if len(np.unique(y))==2 else np.nan\n",
    "        return alpha, lo, hi, auc\n",
    "    except Exception:\n",
    "        lr = LogisticRegression(solver=\"liblinear\")\n",
    "        s2 = s.reshape(-1,1)\n",
    "        lr.fit(s2, y)\n",
    "        p = lr.predict_proba(s2)[:,1]\n",
    "        auc = roc_auc_score(y, p)\n",
    "        alpha = float(lr.coef_.ravel()[0]); lo = hi = np.nan\n",
    "        return alpha, lo, hi, auc\n",
    "\n",
    "def cv_auc_for_score(s, y, n_splits=5, seed=42):\n",
    "    \"\"\"5-fold CV AUC: in ogni fold fit logit(y~s) sul train e predici sul test.\"\"\"\n",
    "    s = (s - np.mean(s)) / (np.std(s) if np.std(s)>0 else 1.0)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    aucs = []\n",
    "    for tr, te in skf.split(np.zeros_like(y), y):\n",
    "        lr = LogisticRegression(solver=\"liblinear\")\n",
    "        lr.fit(s[tr].reshape(-1,1), y[tr])\n",
    "        p = lr.predict_proba(s[te].reshape(-1,1))[:,1]\n",
    "        aucs.append(roc_auc_score(y[te], p))\n",
    "    return float(np.mean(aucs)), float(np.std(aucs))\n",
    "\n",
    "def topk_set(score, authors, frac=0.01):\n",
    "    k = max(1, int(math.ceil(frac*len(authors))))\n",
    "    idx = np.argsort(-score, kind=\"stable\")[:k]\n",
    "    return set([authors[i] for i in idx])\n",
    "\n",
    "def jaccard(a,b):\n",
    "    if not a and not b: return 1.0\n",
    "    u = len(a|b); return len(a&b)/u if u>0 else 0.0\n",
    "\n",
    "# -------- MAIN LOOP --------\n",
    "rows = []\n",
    "for ev_name, ev_folder in EVENTS.items():\n",
    "    AUTHORS_FILE = os.path.join(ev_folder, \"network\", \"authors.txt\")\n",
    "    LABELS_FILE  = os.path.join(ev_folder, \"CRC_radicalization_analysis.csv\")\n",
    "    UIL_FILE     = os.path.join(ev_folder, \"network\", \"uil\", \"edges.txt\")\n",
    "    CSL_FILE     = os.path.join(ev_folder, \"network\", \"csl\", \"edges.txt\")   # baseline τ_c already applied\n",
    "    TDL_FILE     = os.path.join(ev_folder, \"network\", \"tdl\", \"edges.txt\")\n",
    "    ASL_FILE     = os.path.join(ev_folder, \"network\", \"asl\", \"edges.txt\")   # baseline τ_a already applied\n",
    "\n",
    "    if not (os.path.isfile(AUTHORS_FILE) and os.path.isfile(LABELS_FILE)):\n",
    "        print(f\"[skip] Missing authors/labels for {ev_name}\")\n",
    "        continue\n",
    "\n",
    "    # Autori + etichette (manteniamo l'ordine del file authors.txt)\n",
    "    authors = load_authors(AUTHORS_FILE)\n",
    "    labs = pd.read_csv(LABELS_FILE, usecols=[\"author\",\"radical\"]).astype({\"author\":str})\n",
    "    lab_set = set(labs[\"author\"])\n",
    "    authors = [a for a in authors if a in lab_set]\n",
    "    y = labs.set_index(\"author\").loc[authors, \"radical\"].astype(\"int8\").values\n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(f\"[skip] Only one class in labels for {ev_name}\")\n",
    "        continue\n",
    "    authors_set = set(authors)\n",
    "\n",
    "    # Strength per layer (streaming)\n",
    "    str_uil = strength_stream_edges(UIL_FILE, authors_set)\n",
    "    str_csl = strength_stream_edges(CSL_FILE, authors_set)  # file già filtrato a baseline\n",
    "    str_tdl = strength_stream_edges(TDL_FILE, authors_set)\n",
    "    str_asl = strength_stream_edges(ASL_FILE, authors_set)\n",
    "\n",
    "    # Effective influence matrix N x 4\n",
    "    Ef = effective_phi_per_layer(\n",
    "        {\"uil\":str_uil,\"csl\":str_csl,\"tdl\":str_tdl,\"asl\":str_asl},\n",
    "        authors, omega=omega\n",
    "    )\n",
    "    # Z-score per colonna per stabilità e per Z-sum\n",
    "    Ef_mean = Ef.mean(axis=0, keepdims=True)\n",
    "    Ef_std  = Ef.std(axis=0, keepdims=True); Ef_std[Ef_std==0] = 1.0\n",
    "    Ef_z = (Ef - Ef_mean) / Ef_std\n",
    "\n",
    "    # ---- Indici ----\n",
    "    betavec = np.array([beta[\"uil\"], beta[\"csl\"], beta[\"tdl\"], beta[\"asl\"]], dtype=np.float64)\n",
    "    # CRC× (multiplicative)\n",
    "    crc_mul = np.prod(1.0 + Ef * betavec, axis=1)\n",
    "    # CRC+ (additive)\n",
    "    crc_add = np.sum(Ef * betavec, axis=1)\n",
    "    # Z-sum (media delle colonne z-scored)\n",
    "    z_sum   = Ef_z.mean(axis=1)\n",
    "\n",
    "    # Linear (learned): predittore lineare (senza intercetta) da logit su Ef_z\n",
    "    s_lin = None\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X4 = sm.add_constant(Ef_z)\n",
    "        res4 = sm.Logit(y, X4).fit(disp=0)\n",
    "        theta = np.asarray(res4.params[1:], dtype=float)  # 4 pesi (skip intercept)\n",
    "        s_lin = (Ef_z @ theta).astype(float)\n",
    "    except Exception:\n",
    "        lr4 = LogisticRegression(solver=\"liblinear\")\n",
    "        lr4.fit(Ef_z, y)\n",
    "        theta = lr4.coef_.ravel().astype(float)\n",
    "        s_lin = (Ef_z @ theta).astype(float)\n",
    "\n",
    "    # ---- Metriche per ogni indice ----\n",
    "    def eval_index(idx_name, s):\n",
    "        alpha, lo, hi, auc_in = fit_alpha_ci_auc_on_score(s, y)\n",
    "        auc_cv_mean, auc_cv_sd = cv_auc_for_score(s, y, n_splits=5, seed=42)\n",
    "        return {\n",
    "            \"Event\": ev_name, \"Index\": idx_name,\n",
    "            \"alpha\": alpha, \"alpha_lo\": lo, \"alpha_hi\": hi,\n",
    "            \"AUC_in\": auc_in, \"AUC_cv_mean\": auc_cv_mean, \"AUC_cv_sd\": auc_cv_sd\n",
    "        }\n",
    "\n",
    "    rows.append(eval_index(\"CRC× (multiplicative)\", crc_mul))\n",
    "    rows.append(eval_index(\"CRC+ (additive)\",      crc_add))\n",
    "    rows.append(eval_index(\"Z-sum\",                z_sum))\n",
    "    rows.append(eval_index(\"Linear (learned)\",     s_lin))\n",
    "\n",
    "    # ---- Ranking overlap (top-1%) vs CRC× ----\n",
    "    base_top1 = topk_set(crc_mul, authors, frac=0.01)\n",
    "    for name, s in [(\"CRC+ (additive)\", crc_add), (\"Z-sum\", z_sum), (\"Linear (learned)\", s_lin)]:\n",
    "        s_top1 = topk_set(s, authors, frac=0.01)\n",
    "        rows.append({\n",
    "            \"Event\": ev_name, \"Index\": f\"Jaccard vs CRC× — {name}\",\n",
    "            \"alpha\": np.nan, \"alpha_lo\": np.nan, \"alpha_hi\": np.nan,\n",
    "            \"AUC_in\": jaccard(base_top1, s_top1),\n",
    "            \"AUC_cv_mean\": np.nan, \"AUC_cv_sd\": np.nan\n",
    "        })\n",
    "\n",
    "    # free per-event\n",
    "    del str_uil, str_csl, str_tdl, str_asl, Ef, Ef_z, crc_mul, crc_add, z_sum, s_lin\n",
    "    gc.collect()\n",
    "\n",
    "# -------- Save & quick viz --------\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved → {OUT_CSV}\")\n",
    "display(df)\n",
    "\n",
    "# Optional: barplot ΔAUC_cv (index - CRC×) per evento\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    pivot = df[df[\"Index\"].isin([\"CRC× (multiplicative)\",\"CRC+ (additive)\",\"Z-sum\",\"Linear (learned)\"])].pivot_table(\n",
    "        index=[\"Event\",\"Index\"], values=\"AUC_cv_mean\", aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    base = pivot[pivot[\"Index\"]==\"CRC× (multiplicative)\"][[\"Event\",\"AUC_cv_mean\"]].rename(columns={\"AUC_cv_mean\":\"base\"})\n",
    "    merged = pivot.merge(base, on=\"Event\")\n",
    "    merged[\"DeltaAUC\"] = merged[\"AUC_cv_mean\"] - merged[\"base\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 4.2))\n",
    "    for ev in merged[\"Event\"].unique():\n",
    "        sub = merged[merged[\"Event\"]==ev]\n",
    "        ax.plot(sub[\"Index\"], sub[\"DeltaAUC\"], marker=\"o\", label=ev)\n",
    "    ax.axhline(0.0, linestyle=\"--\", linewidth=1, color=\"0.3\")\n",
    "    ax.set_ylabel(\"ΔAUC (5-fold CV) vs CRC×\")\n",
    "    ax.set_xlabel(\"Index\")\n",
    "    ax.set_xticklabels([\"CRC×\",\"CRC+\",\"Z-sum\",\"Linear\"], rotation=0)\n",
    "    ax.legend(frameon=False, ncol=2)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_FIG, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved figure → {OUT_FIG}\")\n",
    "except Exception as e:\n",
    "    print(\"Plot skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7ac10-34a3-42a3-8113-075c182ddc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# -------- CONFIG --------\n",
    "EVENTS = {\n",
    "    \"2008 Elections\": \"data/2008_elections\",\n",
    "    \"2011 Occupy Wall Street\": \"data/2011_wallstreet\",\n",
    "    \"2016 Elections\": \"data/2016_elections\",\n",
    "    \"2017 Charlottesville Rally\": \"data/2017_rally\",\n",
    "    \"2021 Capitol Riot\": \"data/2021_riot\",\n",
    "}\n",
    "\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"add_vs_mul_topk_summary.csv\")\n",
    "OUT_TEX = os.path.join(OUT_DIR, \"add_vs_mul_topk_summary.tex\")\n",
    "\n",
    "# Top-k fractions to assess tail behavior\n",
    "TOPK_FRACS = [0.005, 0.01, 0.05]  # 0.5%, 1%, 5%\n",
    "INDEX_NAMES = [\"CRC×\", \"CRC+\", \"Z-sum\", \"Linear\"]\n",
    "\n",
    "# -------- HELPERS (streaming, low-RAM) --------\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def strength_stream_edges(path, authors_set):\n",
    "    \"\"\"Somma log1p(w) su u e v per ogni riga 'u;v;w' in streaming. Ritorna dict sparse.\"\"\"\n",
    "    s = defaultdict(float)\n",
    "    if not os.path.isfile(path): \n",
    "        return s\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            parts = line.rstrip(\"\\n\").split(\";\")\n",
    "            if len(parts) != 3: \n",
    "                continue\n",
    "            u, v, ws = parts\n",
    "            if (u not in authors_set) or (v not in authors_set): \n",
    "                continue\n",
    "            try:\n",
    "                w = float(ws)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            inc = math.log1p(w)\n",
    "            s[u] += inc; s[v] += inc\n",
    "    return s\n",
    "\n",
    "def effective_phi_matrix(str_by_layer, authors, omega=1.0, layer_order=(\"uil\",\"csl\",\"tdl\",\"asl\")):\n",
    "    \"\"\"N x L matrix of normalized effective influence per layer.\"\"\"\n",
    "    n = len(authors)\n",
    "    L = len(layer_order)\n",
    "    Ef = np.zeros((n, L), dtype=np.float64)\n",
    "    totals = np.zeros(n, dtype=np.float64)\n",
    "    for j,l in enumerate(layer_order):\n",
    "        for i,a in enumerate(authors):\n",
    "            totals[i] += str_by_layer[l].get(a, 0.0)\n",
    "    denom = max(n-1, 1)\n",
    "    for j,l in enumerate(layer_order):\n",
    "        for i,a in enumerate(authors):\n",
    "            phi = str_by_layer[l].get(a, 0.0)\n",
    "            Ef[i,j] = (phi + omega * (totals[i] - phi)) / denom\n",
    "    return Ef\n",
    "\n",
    "def build_indices(Ef, y):\n",
    "    \"\"\"Return dict of indices on Ef: CRC×, CRC+, Z-sum, Linear (learned).\"\"\"\n",
    "    # z-score per colonna (per Z-sum e linear)\n",
    "    Ef_mean = Ef.mean(axis=0, keepdims=True)\n",
    "    Ef_std  = Ef.std(axis=0, keepdims=True); Ef_std[Ef_std==0] = 1.0\n",
    "    Ef_z = (Ef - Ef_mean) / Ef_std\n",
    "\n",
    "    # CRC× (multiplicative) and CRC+ (additive)\n",
    "    crc_mul = np.prod(1.0 + Ef, axis=1)           # beta=1 su tutti i layer\n",
    "    crc_add = np.sum(Ef, axis=1)\n",
    "\n",
    "    # Z-sum (media dei z-scores)\n",
    "    z_sum   = Ef_z.mean(axis=1)\n",
    "\n",
    "    # Linear (learned) via logit su Ef_z\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X = sm.add_constant(Ef_z)\n",
    "        res = sm.Logit(y, X).fit(disp=0)\n",
    "        theta = np.asarray(res.params[1:], dtype=float)   # skip intercept\n",
    "        s_lin = (Ef_z @ theta).astype(float)\n",
    "    except Exception:\n",
    "        lr = LogisticRegression(solver=\"liblinear\")\n",
    "        lr.fit(Ef_z, y)\n",
    "        theta = lr.coef_.ravel().astype(float)\n",
    "        s_lin = (Ef_z @ theta).astype(float)\n",
    "\n",
    "    return {\n",
    "        \"CRC×\": crc_mul,\n",
    "        \"CRC+\": crc_add,\n",
    "        \"Z-sum\": z_sum,\n",
    "        \"Linear\": s_lin,\n",
    "        \"Ef_z\": Ef_z  # usato solo per la metrica di non-compensazione\n",
    "    }\n",
    "\n",
    "def precision_at_k(score, y, frac):\n",
    "    n = len(y)\n",
    "    k = max(1, int(math.ceil(frac * n)))\n",
    "    idx = np.argsort(-score, kind=\"stable\")[:k]\n",
    "    return float(y[idx].mean()), k\n",
    "\n",
    "def noncomp_fraction_at_k(Ef_z, score, frac, thresh=0.0):\n",
    "    \"\"\"Quota di utenti nel top-k con z>thresh in *tutti* i layer.\"\"\"\n",
    "    n = Ef_z.shape[0]\n",
    "    k = max(1, int(math.ceil(frac * n)))\n",
    "    idx = np.argsort(-score, kind=\"stable\")[:k]\n",
    "    Ztop = Ef_z[idx, :]\n",
    "    ok_all = np.all(Ztop > thresh, axis=1)\n",
    "    return float(ok_all.mean())\n",
    "\n",
    "def prevalence(y):\n",
    "    return float(np.mean(y))\n",
    "\n",
    "# -------- MAIN --------\n",
    "rows = []\n",
    "for ev_name, ev_folder in EVENTS.items():\n",
    "    AUTHORS_FILE = os.path.join(ev_folder, \"network\", \"authors.txt\")\n",
    "    LABELS_FILE  = os.path.join(ev_folder, \"CRC_radicalization_analysis.csv\")\n",
    "    UIL_FILE     = os.path.join(ev_folder, \"network\", \"uil\", \"edges.txt\")\n",
    "    CSL_FILE     = os.path.join(ev_folder, \"network\", \"csl\", \"edges.txt\")   # baseline τ_c già applicato\n",
    "    TDL_FILE     = os.path.join(ev_folder, \"network\", \"tdl\", \"edges.txt\")\n",
    "    ASL_FILE     = os.path.join(ev_folder, \"network\", \"asl\", \"edges.txt\")   # baseline τ_a già applicato\n",
    "\n",
    "    if not (os.path.isfile(AUTHORS_FILE) and os.path.isfile(LABELS_FILE)):\n",
    "        print(f\"[skip] Missing authors/labels for {ev_name}\")\n",
    "        continue\n",
    "\n",
    "    # Autori + etichette nell'ordine di authors.txt\n",
    "    authors = load_authors(AUTHORS_FILE)\n",
    "    labs = pd.read_csv(LABELS_FILE, usecols=[\"author\",\"radical\"]).astype({\"author\":str})\n",
    "    lab_set = set(labs[\"author\"])\n",
    "    authors = [a for a in authors if a in lab_set]\n",
    "    y = labs.set_index(\"author\").loc[authors, \"radical\"].astype(\"int8\").values\n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(f\"[skip] Only one class in labels for {ev_name}\")\n",
    "        continue\n",
    "    authors_set = set(authors)\n",
    "\n",
    "    # Strength per layer (streaming)\n",
    "    str_uil = strength_stream_edges(UIL_FILE, authors_set)\n",
    "    str_csl = strength_stream_edges(CSL_FILE, authors_set)\n",
    "    str_tdl = strength_stream_edges(TDL_FILE, authors_set)\n",
    "    str_asl = strength_stream_edges(ASL_FILE, authors_set)\n",
    "\n",
    "    # Matrice N x 4 delle effective influences\n",
    "    Ef = effective_phi_matrix(\n",
    "        {\"uil\":str_uil,\"csl\":str_csl,\"tdl\":str_tdl,\"asl\":str_asl},\n",
    "        authors, omega=1.0\n",
    "    )\n",
    "\n",
    "    # Indici\n",
    "    idx = build_indices(Ef, y)\n",
    "    p_base = prevalence(y)\n",
    "\n",
    "    # PR-AUC (Average Precision) per indice\n",
    "    pr_auc = {}\n",
    "    for name in INDEX_NAMES:\n",
    "        s = idx[name]\n",
    "        # Normalizza per stabilità\n",
    "        s_std = (s - s.mean()) / (s.std() if s.std()>0 else 1.0)\n",
    "        pr_auc[name] = average_precision_score(y, s_std)\n",
    "\n",
    "    # Metriche top-k e non-compensazione\n",
    "    for frac in TOPK_FRACS:\n",
    "        for name in INDEX_NAMES:\n",
    "            s = idx[name]\n",
    "            prec_k, k = precision_at_k(s, y, frac)\n",
    "            lift_k = prec_k / p_base if p_base > 0 else np.nan\n",
    "            noncomp_k = noncomp_fraction_at_k(idx[\"Ef_z\"], s, frac, thresh=0.0)\n",
    "            rows.append({\n",
    "                \"Event\": ev_name,\n",
    "                \"Index\": name,\n",
    "                \"k_frac\": frac,\n",
    "                \"k_abs\": k,\n",
    "                \"Prevalence\": p_base,\n",
    "                \"Precision@k\": prec_k,\n",
    "                \"Lift@k\": lift_k,\n",
    "                \"NonComp@AllLayers@k\": noncomp_k,\n",
    "                \"PR-AUC\": pr_auc[name],\n",
    "            })\n",
    "\n",
    "    # cleanup\n",
    "    del str_uil, str_csl, str_tdl, str_asl, Ef, idx\n",
    "    gc.collect()\n",
    "\n",
    "# Save\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved → {OUT_CSV}\")\n",
    "\n",
    "# Make a compact LaTeX table at k=1% (space-friendly)\n",
    "sub = df[df[\"k_frac\"]==0.01].copy()\n",
    "# Wide pivot: one row per event, columns = (Index × metric)\n",
    "wide = sub.pivot_table(index=\"Event\", columns=\"Index\", values=[\"PR-AUC\",\"Precision@k\",\"Lift@k\",\"NonComp@AllLayers@k\"])\n",
    "# Order columns nicely\n",
    "wide = wide.reindex(columns=pd.MultiIndex.from_product(\n",
    "    [[\"PR-AUC\",\"Precision@k\",\"Lift@k\",\"NonComp@AllLayers@k\"], INDEX_NAMES]\n",
    "))\n",
    "# Format\n",
    "tex = wide.round(3).to_latex(na_rep=\"\", escape=False, multirow=True, multicolumn=True, column_format=\"lcccccccccccc\")\n",
    "with open(OUT_TEX, \"w\") as f:\n",
    "    f.write(tex)\n",
    "print(f\"Saved → {OUT_TEX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2117bd0-6161-472e-af3d-a08c5936e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multiplicative vs Additive: exclusive enrichment + CV interactions + top-k co-activation ===\n",
    "# Outputs:\n",
    "#   data/thresholds_analysis/synergy_exclusive_enrichment.csv / .tex\n",
    "#   data/thresholds_analysis/add_vs_int_cv_deltas.csv / .tex\n",
    "#   data/thresholds_analysis/topk_minz_compare.csv / .tex\n",
    "\n",
    "import os, math, itertools, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.stats import fisher_exact, wilcoxon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "# -------- CONFIG --------\n",
    "EVENTS = {\n",
    "    \"2008 Elections\": \"data/2008_elections\",\n",
    "    \"2011 Occupy Wall Street\": \"data/2011_wallstreet\",\n",
    "    \"2016 Elections\": \"data/2016_elections\",\n",
    "    \"2017 Charlottesville Rally\": \"data/2017_rally\",\n",
    "    \"2021 Capitol Riot\": \"data/2021_riot\",\n",
    "}\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\"); os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ENR_CSV = os.path.join(OUT_DIR, \"synergy_exclusive_enrichment.csv\")\n",
    "ENR_TEX = os.path.join(OUT_DIR, \"synergy_exclusive_enrichment.tex\")\n",
    "CV_CSV  = os.path.join(OUT_DIR, \"add_vs_int_cv_deltas.csv\")\n",
    "CV_TEX  = os.path.join(OUT_DIR, \"add_vs_int_cv_deltas.tex\")\n",
    "TOPK_CSV= os.path.join(OUT_DIR, \"topk_minz_compare.csv\")\n",
    "TOPK_TEX= os.path.join(OUT_DIR, \"topk_minz_compare.tex\")\n",
    "\n",
    "LAYS = [\"uil\",\"csl\",\"tdl\",\"asl\"]\n",
    "QS   = [0.7, 0.8, 0.9]   # quantili candidati\n",
    "LOWQ = 0.5               # \"basso\" = ≤ mediana per gli altri layer\n",
    "N_MIN = 30               # minimo elementi per gruppo\n",
    "TOPK_FRAC = 0.01         # top-1%\n",
    "\n",
    "# -------- IO helpers --------\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def strength_stream_edges(path, authors_set):\n",
    "    s = defaultdict(float)\n",
    "    if not os.path.isfile(path): return s\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            parts = line.rstrip(\"\\n\").split(\";\")\n",
    "            if len(parts)!=3: continue\n",
    "            u,v,ws = parts\n",
    "            if (u not in authors_set) or (v not in authors_set): continue\n",
    "            try: w = float(ws)\n",
    "            except: continue\n",
    "            inc = math.log1p(w)\n",
    "            s[u] += inc; s[v] += inc\n",
    "    return s\n",
    "\n",
    "def effective_phi_matrix(str_by_layer, authors, omega=1.0, layer_order=(\"uil\",\"csl\",\"tdl\",\"asl\")):\n",
    "    n = len(authors); L = len(layer_order)\n",
    "    Ef = np.zeros((n,L), dtype=np.float64)\n",
    "    totals = np.zeros(n, dtype=np.float64)\n",
    "    for j,l in enumerate(layer_order):\n",
    "        for i,a in enumerate(authors):\n",
    "            totals[i] += str_by_layer[l].get(a,0.0)\n",
    "    denom = max(n-1,1)\n",
    "    for j,l in enumerate(layer_order):\n",
    "        for i,a in enumerate(authors):\n",
    "            phi = str_by_layer[l].get(a,0.0)\n",
    "            Ef[i,j] = (phi + omega*(totals[i]-phi))/denom\n",
    "    return Ef\n",
    "\n",
    "def zscore_cols(X):\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    sd = X.std(axis=0, keepdims=True); sd[sd==0]=1.0\n",
    "    return (X-mu)/sd\n",
    "\n",
    "# -------- exclusive enrichment --------\n",
    "def exclusive_masks(Ef_z, q, lowq=0.5):\n",
    "    \"\"\"Restituisce:\n",
    "       - all_high_excl: z >= q-quantile su TUTTI i layer\n",
    "       - single_high_excl: z >= q-quantile su UNO e z <= lowq-quantile su TUTTI gli altri\n",
    "    \"\"\"\n",
    "    n,L = Ef_z.shape\n",
    "    thr_hi = np.quantile(Ef_z, q, axis=0)\n",
    "    thr_lo = np.quantile(Ef_z, lowq, axis=0)\n",
    "    H = (Ef_z >= thr_hi)   # N x L\n",
    "    Lw = (Ef_z <= thr_lo)  # N x L\n",
    "    all_high = H.all(axis=1)\n",
    "    single_high = np.zeros(n, dtype=bool)\n",
    "    for j in range(L):\n",
    "        cond = H[:,j] & np.all(Lw[:,np.arange(L)!=j], axis=1)\n",
    "        single_high |= cond\n",
    "    return all_high, single_high, H.sum(axis=1)\n",
    "\n",
    "def risk_ratio(all_high, single_high, y, eps=1e-6):\n",
    "    A_pos = int(((y==1)&all_high).sum()); A_neg = int(((y==0)&all_high).sum())\n",
    "    B_pos = int(((y==1)&single_high).sum()); B_neg = int(((y==0)&single_high).sum())\n",
    "    if (A_pos+A_neg)<1 or (B_pos+B_neg)<1:\n",
    "        return np.nan, np.nan, A_pos, A_neg, B_pos, B_neg\n",
    "    risk_A = (A_pos+eps)/(A_pos+A_neg+eps)\n",
    "    risk_B = (B_pos+eps)/(B_pos+B_neg+eps)\n",
    "    RR = risk_A/risk_B\n",
    "    _, p = fisher_exact([[A_pos,A_neg],[B_pos,B_neg]], alternative=\"greater\")\n",
    "    return RR, float(p), A_pos, A_neg, B_pos, B_neg\n",
    "\n",
    "# -------- CV additive vs interactions (L2) --------\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def inter_feat(Z):\n",
    "    cols=[Z]; L=Z.shape[1]\n",
    "    for i,j in itertools.combinations(range(L),2):\n",
    "        cols.append((Z[:,i]*Z[:,j])[:,None])\n",
    "    return np.hstack(cols)\n",
    "\n",
    "def cv_add_vs_int(Ef_z, y, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    ll_add, ll_int, auc_add, auc_int = [], [], [], []\n",
    "    for tr,te in skf.split(Ef_z, y):\n",
    "        ss_add = StandardScaler().fit(Ef_z[tr])\n",
    "        Xtr_add = ss_add.transform(Ef_z[tr]); Xte_add = ss_add.transform(Ef_z[te])\n",
    "        ss_int = StandardScaler().fit(inter_feat(Ef_z[tr]))\n",
    "        Xtr_int = ss_int.transform(inter_feat(Ef_z[tr])); Xte_int = ss_int.transform(inter_feat(Ef_z[te]))\n",
    "        clf_add = LogisticRegressionCV(Cs=10, cv=3, scoring='neg_log_loss', solver='lbfgs', penalty='l2', max_iter=2000)\n",
    "        clf_int = LogisticRegressionCV(Cs=10, cv=3, scoring='neg_log_loss', solver='lbfgs', penalty='l2', max_iter=2000)\n",
    "        clf_add.fit(Xtr_add, y[tr]); clf_int.fit(Xtr_int, y[tr])\n",
    "        p_add = clf_add.predict_proba(Xte_add)[:,1]; p_int = clf_int.predict_proba(Xte_int)[:,1]\n",
    "        ll_add.append(log_loss(y[te], p_add)); ll_int.append(log_loss(y[te], p_int))\n",
    "        auc_add.append(roc_auc_score(y[te], p_add)); auc_int.append(roc_auc_score(y[te], p_int))\n",
    "    return np.array(ll_add), np.array(ll_int), np.array(auc_add), np.array(auc_int)\n",
    "\n",
    "# -------- top-k co-activation (min-z) --------\n",
    "def topk_minz(Ef_z, scores, frac=0.01):\n",
    "    n = Ef_z.shape[0]; k = max(1, int(math.ceil(frac*n)))\n",
    "    out={}\n",
    "    for name, s in scores.items():\n",
    "        idx = np.argsort(-s, kind=\"stable\")[:k]\n",
    "        out[name] = float(np.mean(Ef_z[idx,:].min(axis=1)))\n",
    "    return out\n",
    "\n",
    "# -------- MAIN --------\n",
    "rows_enr = []\n",
    "rows_cv  = []\n",
    "rows_topk= []\n",
    "\n",
    "for ev_name, ev_folder in EVENTS.items():\n",
    "    AUTHORS = os.path.join(ev_folder, \"network\", \"authors.txt\")\n",
    "    LABELS  = os.path.join(ev_folder, \"CRC_radicalization_analysis.csv\")\n",
    "    UIL     = os.path.join(ev_folder, \"network\", \"uil\", \"edges.txt\")\n",
    "    CSL     = os.path.join(ev_folder, \"network\", \"csl\", \"edges.txt\")\n",
    "    TDL     = os.path.join(ev_folder, \"network\", \"tdl\", \"edges.txt\")\n",
    "    ASL     = os.path.join(ev_folder, \"network\", \"asl\", \"edges.txt\")\n",
    "    if not (os.path.isfile(AUTHORS) and os.path.isfile(LABELS)): \n",
    "        print(f\"[skip] {ev_name}\"); continue\n",
    "\n",
    "    authors = load_authors(AUTHORS)\n",
    "    labs = pd.read_csv(LABELS, usecols=[\"author\",\"radical\"]).astype({\"author\":str})\n",
    "    lab_set = set(labs[\"author\"])\n",
    "    authors = [a for a in authors if a in lab_set]\n",
    "    y = labs.set_index(\"author\").loc[authors,\"radical\"].astype(\"int8\").values\n",
    "    if len(np.unique(y))<2: \n",
    "        print(f\"[skip one-class] {ev_name}\"); continue\n",
    "    Aset = set(authors)\n",
    "\n",
    "    str_uil = strength_stream_edges(UIL, Aset)\n",
    "    str_csl = strength_stream_edges(CSL, Aset)\n",
    "    str_tdl = strength_stream_edges(TDL, Aset)\n",
    "    str_asl = strength_stream_edges(ASL, Aset)\n",
    "\n",
    "    Ef = effective_phi_matrix({\"uil\":str_uil,\"csl\":str_csl,\"tdl\":str_tdl,\"asl\":str_asl}, authors, omega=1.0)\n",
    "    Ef_z = zscore_cols(Ef)\n",
    "\n",
    "    # --- (1) exclusive enrichment ---\n",
    "    best = None\n",
    "    for q in QS:\n",
    "        ah, sh, _ = exclusive_masks(Ef_z, q=q, lowq=LOWQ)\n",
    "        if ah.sum()>=N_MIN and sh.sum()>=N_MIN:\n",
    "            RR, p, Apos, Aneg, Bpos, Bneg = risk_ratio(ah, sh, y)\n",
    "            best = (q, ah.sum(), sh.sum(), RR, p, Apos, Aneg, Bpos, Bneg)\n",
    "            break\n",
    "    rows_enr.append({\n",
    "        \"Event\": ev_name,\n",
    "        \"q_used\": best[0] if best else np.nan,\n",
    "        \"all_high_n\": int(best[1]) if best else 0,\n",
    "        \"single_high_n\": int(best[2]) if best else 0,\n",
    "        \"RiskRatio_all/single\": float(best[3]) if best else np.nan,\n",
    "        \"Fisher_p\": float(best[4]) if best else np.nan,\n",
    "        \"A_pos\": int(best[5]) if best else 0, \"A_neg\": int(best[6]) if best else 0,\n",
    "        \"B_pos\": int(best[7]) if best else 0, \"B_neg\": int(best[8]) if best else 0\n",
    "    })\n",
    "\n",
    "    # --- (2) CV additive vs interactions ---\n",
    "    ll_add, ll_int, auc_add, auc_int = cv_add_vs_int(Ef_z, y, seed=42)\n",
    "    dLL  = ll_int - ll_add   # <0 = interactions better\n",
    "    dAUC = auc_int - auc_add # >0 = interactions better\n",
    "    # Wilcoxon accoppiato sulle 5 fold (se tutte finite)\n",
    "    try:\n",
    "        w_p_ll = wilcoxon(ll_add, ll_int, alternative=\"greater\").pvalue  # H1: add > int (int migliore)\n",
    "    except Exception:\n",
    "        w_p_ll = np.nan\n",
    "    try:\n",
    "        w_p_auc = wilcoxon(auc_int, auc_add, alternative=\"greater\").pvalue  # H1: int > add\n",
    "    except Exception:\n",
    "        w_p_auc = np.nan\n",
    "    rows_cv.append({\n",
    "        \"Event\": ev_name,\n",
    "        \"mean_dLogLoss(int-add)\": float(np.mean(dLL)),\n",
    "        \"mean_dAUC(int-add)\": float(np.mean(dAUC)),\n",
    "        \"wilcoxon_p_logloss(add>int)\": float(w_p_ll),\n",
    "        \"wilcoxon_p_auc(int>add)\": float(w_p_auc),\n",
    "        \"folds_dLogLoss\": \";\".join([f\"{x:.4f}\" for x in dLL]),\n",
    "        \"folds_dAUC\": \";\".join([f\"{x:.4f}\" for x in dAUC])\n",
    "    })\n",
    "\n",
    "    # --- (3) top-1% co-activation (min z across layers) ---\n",
    "    crc_mul = np.prod(1.0 + Ef, axis=1)\n",
    "    crc_add = np.sum(Ef, axis=1)\n",
    "    out = topk_minz(Ef_z, {\"CRC×\": crc_mul, \"CRC+\": crc_add}, frac=TOPK_FRAC)\n",
    "    rows_topk.append({\n",
    "        \"Event\": ev_name,\n",
    "        \"k_frac\": TOPK_FRAC,\n",
    "        \"mean_minZ@topk (CRC×)\": out[\"CRC×\"],\n",
    "        \"mean_minZ@topk (CRC+)\": out[\"CRC+\"],\n",
    "        \"Δ(minZ) CRC×-CRC+\": out[\"CRC×\"] - out[\"CRC+\"]\n",
    "    })\n",
    "\n",
    "# ----- SAVE & LaTeX -----\n",
    "enr = pd.DataFrame(rows_enr); enr.to_csv(ENR_CSV, index=False)\n",
    "cv  = pd.DataFrame(rows_cv);  cv.to_csv(CV_CSV, index=False)\n",
    "topk= pd.DataFrame(rows_topk);topk.to_csv(TOPK_CSV, index=False)\n",
    "print(\"Saved ->\", ENR_CSV, CV_CSV, TOPK_CSV)\n",
    "\n",
    "def to_tex(df, fname, caption, label, cols, rnd):\n",
    "    d = df.copy()\n",
    "    for c, r in rnd.items():\n",
    "        if c in d.columns:\n",
    "            d[c] = d[c].map(lambda x: \"\" if pd.isna(x) else f\"{x:.{r}f}\")\n",
    "    d = d[cols]\n",
    "    tex = d.to_latex(index=False, escape=False, caption=caption, label=label)\n",
    "    print(f\"{fname} {text}\")\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(tex)\n",
    "    print(\"Saved ->\", fname)\n",
    "\n",
    "to_tex(\n",
    "    enr, ENR_TEX,\n",
    "    \"Exclusive co-activation enrichment: all-high (top-$q$ on all layers) vs single-high (top-$q$ on exactly one layer and ≤ median on the others).\",\n",
    "    \"tab:enrichment_exclusive\",\n",
    "    cols=[\"Event\",\"q_used\",\"all_high_n\",\"single_high_n\",\"RiskRatio_all/single\",\"Fisher_p\"],\n",
    "    rnd={\"RiskRatio_all/single\":2,\"Fisher_p\":3}\n",
    ")\n",
    "\n",
    "to_tex(\n",
    "    cv, CV_TEX,\n",
    "    \"Cross-validated comparison of additive vs additive+interactions (L2 logistic): negative mean $\\\\Delta$log-loss (int$-$add) favors interactions; positive mean $\\\\Delta$AUC favors interactions.\",\n",
    "    \"tab:add_vs_int_cv\",\n",
    "    cols=[\"Event\",\"mean_dLogLoss(int-add)\",\"wilcoxon_p_logloss(add>int)\",\"mean_dAUC(int-add)\",\"wilcoxon_p_auc(int>add)\"],\n",
    "    rnd={\"mean_dLogLoss(int-add)\":4,\"mean_dAUC(int-add)\":4,\"wilcoxon_p_logloss(add>int)\":3,\"wilcoxon_p_auc(int>add)\":3}\n",
    ")\n",
    "\n",
    "to_tex(\n",
    "    topk, TOPK_TEX,\n",
    "    \"Top-1\\\\% co-activation: mean of the minimum $z$ across layers for users selected by CRC$\\\\times$ vs CRC$+$ (higher is more jointly high).\",\n",
    "    \"tab:topk_minz\",\n",
    "    cols=[\"Event\",\"k_frac\",\"mean_minZ@topk (CRC×)\",\"mean_minZ@topk (CRC+)\",\"Δ(minZ) CRC×-CRC+\"],\n",
    "    rnd={\"k_frac\":3,\"mean_minZ@topk (CRC×)\":3,\"mean_minZ@topk (CRC+)\":3,\"Δ(minZ) CRC×-CRC+\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a1be6-c68e-4c62-b5a7-2a3121ceeb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
