{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc9c350-2c0b-4308-ad92-ee7777e96b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_folder = \"data/2008_elections/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4232f5ba-b37c-47a3-a97a-5819736e42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: sentence-transformers/all-mpnet-base-v2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0f0754ddcd44f2ab0edabf8b6b948d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d9d6a1c8e443b1a19da4a43eb4537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: n=13184, d=768\n",
      "Device: cuda (GPU=True)\n",
      "Block size: 2048, min_sim: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Blocks: 100%|██████████████████████████████████████████████████████████████████████████| 28/28 [00:41<00:00,  1.47s/blk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity p50: 0.3023\n",
      "Similarity p60: 0.3288\n",
      "Similarity p70: 0.3600\n",
      "Similarity p75: 0.3784\n",
      "Similarity p80: 0.3998\n",
      "Similarity p85: 0.4256\n",
      "Similarity p90: 0.4592\n",
      "Similarity p95: 0.5104\n",
      "Edges total: 39,773,721\n",
      "Done. Edges file → data/2008_elections/cslasl-pre/edges/edges.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MIN_SIM = 0.20      # soglia minima di similarità per scrivere un edge\n",
    "BLOCK   = 2048      # block size per moltiplicazioni a blocchi\n",
    "BATCH_EMB = 64      # batch per encode() degli embeddings\n",
    "DEVICE_STR = None   # es: \"cuda:0\" oppure None per auto\n",
    "\n",
    "# --- paths (usano event_folder) ---\n",
    "AUTHORS_FILE = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "CONTENTS_FILE = os.path.join(event_folder, \"cslasl-pre\", \"contents.txt\")\n",
    "EMB_CACHE = os.path.join(event_folder, \"cslasl-pre\", \"csl_embeddings.pkl\")\n",
    "EDGES_DIR = os.path.join(event_folder, \"cslasl-pre\", \"edges\")\n",
    "EDGES_FILE = os.path.join(EDGES_DIR, \"edges.txt\")\n",
    "\n",
    "# --- funzioni utili ---\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        authors = [line.strip() for line in f if line.strip()]\n",
    "    return authors\n",
    "\n",
    "def load_contents(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents = [line.rstrip(\"\\n\") for line in f]\n",
    "    return contents\n",
    "\n",
    "def ensure_dirs():\n",
    "    os.makedirs(os.path.dirname(EMB_CACHE), exist_ok=True)\n",
    "    os.makedirs(EDGES_DIR, exist_ok=True)\n",
    "\n",
    "def get_device(device_arg):\n",
    "    if device_arg:\n",
    "        return torch.device(device_arg)\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_or_build_embeddings(contents, device, batch_size=64):\n",
    "    if os.path.isfile(EMB_CACHE):\n",
    "        with open(EMB_CACHE, \"rb\") as f:\n",
    "            emb = pickle.load(f)\n",
    "        emb = emb.astype(np.float32, copy=False)\n",
    "        norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        emb = emb / norms\n",
    "        return emb\n",
    "\n",
    "    print(\"Loading model: sentence-transformers/all-mpnet-base-v2 ...\")\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=str(device))\n",
    "    emb = model.encode(\n",
    "        contents,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    with open(EMB_CACHE, \"wb\") as f:\n",
    "        pickle.dump(emb, f)\n",
    "    return emb\n",
    "\n",
    "def write_edges_blockwise(authors, emb, min_sim, block, device, flush_every=1_000_000):\n",
    "    \"\"\"\n",
    "    Calcola similarità a blocchi (upper-triangular) e scrive \"u;v;sim\" (undirected, i<j).\n",
    "    \"\"\"\n",
    "    n, d = emb.shape\n",
    "    print(f\"Embeddings: n={n}, d={d}\")\n",
    "    if n != len(authors):\n",
    "        raise ValueError(f\"Authors ({len(authors)}) e embeddings ({n}) non coincidono.\")\n",
    "\n",
    "    use_gpu = (device.type == \"cuda\")\n",
    "    print(f\"Device: {device} (GPU={use_gpu})\")\n",
    "    print(f\"Block size: {block}, min_sim: {min_sim}\")\n",
    "\n",
    "    with open(EDGES_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
    "        total_written = 0\n",
    "        emb_torch = torch.from_numpy(emb)\n",
    "        if use_gpu:\n",
    "            emb_torch = emb_torch.to(device, non_blocking=True)\n",
    "\n",
    "        n_blocks = math.ceil(n / block)\n",
    "        pbar = tqdm(total=n_blocks * (n_blocks + 1) // 2, desc=\"Blocks\", unit=\"blk\")\n",
    "        for bi in range(n_blocks):\n",
    "            i0 = bi * block\n",
    "            i1 = min((bi + 1) * block, n)\n",
    "            Ei = emb_torch[i0:i1]\n",
    "\n",
    "            for bj in range(bi, n_blocks):\n",
    "                j0 = bj * block\n",
    "                j1 = min((bj + 1) * block, n)\n",
    "                Ej = emb_torch[j0:j1]\n",
    "\n",
    "                S = Ei @ Ej.T  # (i_len, j_len)\n",
    "\n",
    "                if bi == bj:\n",
    "                    tri_mask = torch.triu(torch.ones((i1 - i0, j1 - j0), dtype=torch.bool, device=S.device), diagonal=1)\n",
    "                    S = torch.where(tri_mask, S, torch.full_like(S, float(\"-inf\")))\n",
    "\n",
    "                keep = S > float(min_sim)\n",
    "                if not torch.any(keep):\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                idx_i, idx_j = torch.nonzero(keep, as_tuple=True)\n",
    "                sims = S[idx_i, idx_j].detach().float().cpu().numpy()\n",
    "                gi = (i0 + idx_i.cpu().numpy()).tolist()\n",
    "                gj = (j0 + idx_j.cpu().numpy()).tolist()\n",
    "\n",
    "                buf = []\n",
    "                for k in range(len(sims)):\n",
    "                    u = authors[gi[k]]\n",
    "                    v = authors[gj[k]]\n",
    "                    w = float(sims[k])\n",
    "                    buf.append(f\"{u};{v};{w:.6f}\\n\")\n",
    "\n",
    "                fout.writelines(buf)\n",
    "                total_written += len(buf)\n",
    "\n",
    "                if total_written >= flush_every:\n",
    "                    fout.flush()\n",
    "                    os.fsync(fout.fileno())\n",
    "                    total_written = 0\n",
    "\n",
    "                del keep, idx_i, idx_j, sims, buf, S\n",
    "                if use_gpu:\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            if use_gpu:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "def print_edge_percentiles():\n",
    "    import array\n",
    "    vals = array.array(\"f\")\n",
    "    if not os.path.isfile(EDGES_FILE):\n",
    "        print(f\"Missing {EDGES_FILE}\")\n",
    "        return\n",
    "    with open(EDGES_FILE, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            try:\n",
    "                w = float(line.rsplit(\";\", 1)[1])\n",
    "                vals.append(w)\n",
    "            except Exception:\n",
    "                continue\n",
    "    if len(vals) == 0:\n",
    "        print(\"No edges written; try lowering MIN_SIM.\")\n",
    "        return\n",
    "    arr = np.frombuffer(vals, dtype=np.float32)\n",
    "    for p in [50, 60, 70, 75, 80, 85, 90, 95]:\n",
    "        print(f\"Similarity p{p:02d}: {np.percentile(arr, p):.4f}\")\n",
    "    print(f\"Edges total: {len(arr):,}\")\n",
    "\n",
    "# --- run ---\n",
    "ensure_dirs()\n",
    "\n",
    "if not os.path.isfile(AUTHORS_FILE):\n",
    "    raise SystemExit(f\"Missing {AUTHORS_FILE}\")\n",
    "if not os.path.isfile(CONTENTS_FILE):\n",
    "    raise SystemExit(f\"Missing {CONTENTS_FILE} (one line per author, same order).\")\n",
    "\n",
    "authors = load_authors(AUTHORS_FILE)\n",
    "contents = load_contents(CONTENTS_FILE)\n",
    "\n",
    "if len(authors) != len(contents):\n",
    "    raise SystemExit(f\"authors ({len(authors)}) != contents lines ({len(contents)}) → allinea i file.\")\n",
    "\n",
    "device = get_device(DEVICE_STR)\n",
    "emb = load_or_build_embeddings(contents, device, batch_size=BATCH_EMB)\n",
    "del contents  # libera RAM\n",
    "\n",
    "write_edges_blockwise(authors, emb, min_sim=MIN_SIM, block=BLOCK, device=device)\n",
    "\n",
    "del emb\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print_edge_percentiles()\n",
    "print(f\"Done. Edges file -> {EDGES_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac3f80d-0e5d-4658-b05d-b50d284c03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau_c by percentile: {50: '0.309', 60: '0.329', 70: '0.369', 75: '0.379', 80: '0.409', 85: '0.429', 90: '0.459', 95: '0.519'}\n",
      "Saved metrics → data/thresholds_analysis/results_threshold_csl_density_gcc.csv\n",
      "Saved figure → data/thresholds_analysis/fig_threshold_csl_density_gcc.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "AUTHORS_FILE = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "EDGES_FILE   = os.path.join(event_folder, \"cslasl-pre\", \"edges\", \"edges.txt\")  # \"u;v;sim\"\n",
    "OUT_CSV      = os.path.join(\"data\", \"thresholds_analysis\", \"results_threshold_csl_density_gcc.csv\")\n",
    "OUT_FIG      = os.path.join(\"data\", \"thresholds_analysis\", \"fig_threshold_csl_density_gcc.png\")\n",
    "\n",
    "PERCENTILES  = [50, 60, 70, 75, 80, 85, 90, 95]\n",
    "BOOTSTRAP    = False   # True per ribbon 95% CI\n",
    "B_REPS       = 200     # numero bootstrap\n",
    "ALPHA        = 0.05    # 95% CI\n",
    "\n",
    "# --- helpers ---\n",
    "def custom_round(x: float) -> float:\n",
    "    r = round(float(x), 3)\n",
    "    s = f\"{r:.3f}\"\n",
    "    s = s[:-1] + \"9\"\n",
    "    return float(s)\n",
    "\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def load_edges_df(path):\n",
    "    # u;v;w  (trattiamo come undirected e deduplichiamo coppie simmetriche)\n",
    "    df = pd.read_csv(path, sep=\";\", header=None, names=[\"u\",\"v\",\"w\"],\n",
    "                     dtype={\"u\":\"string\",\"v\":\"string\",\"w\":\"float32\"}, engine=\"c\")\n",
    "    # canonicalizza (min, max) per togliere eventuali duplicati\n",
    "    uu = np.where(df[\"u\"].values < df[\"v\"].values, df[\"u\"].values, df[\"v\"].values)\n",
    "    vv = np.where(df[\"u\"].values < df[\"v\"].values, df[\"v\"].values, df[\"u\"].values)\n",
    "    df = pd.DataFrame({\"u\": uu, \"v\": vv, \"w\": df[\"w\"].values})\n",
    "    df = df.drop_duplicates(subset=[\"u\",\"v\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_graph_undirected(authors, df_edges):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(authors)\n",
    "    if not df_edges.empty:\n",
    "        G.add_edges_from(zip(df_edges[\"u\"].tolist(), df_edges[\"v\"].tolist()))\n",
    "    return G\n",
    "\n",
    "def graph_metrics(G, n_nodes):\n",
    "    dens = nx.density(G)\n",
    "    if G.number_of_edges() == 0 or G.number_of_nodes() == 0:\n",
    "        gcc_nodes = 1 if n_nodes > 0 else 0\n",
    "    else:\n",
    "        gcc_nodes = max((len(c) for c in nx.connected_components(G)), default=1)\n",
    "    gcc_pct = 100.0 * (gcc_nodes / n_nodes) if n_nodes > 0 else 0.0\n",
    "    return dens, gcc_pct, G.number_of_edges()\n",
    "\n",
    "def edge_bootstrap_ci(authors, df_edges, n_nodes, B=200, alpha=0.05, rng=None):\n",
    "    if df_edges.empty:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "    M = len(df_edges)\n",
    "    dens, gccs = [], []\n",
    "    idx = np.arange(M)\n",
    "    for _ in range(B):\n",
    "        sample_idx = rng.choice(idx, size=M, replace=True)\n",
    "        G_b = build_graph_undirected(authors, df_edges.iloc[sample_idx])\n",
    "        d_b, g_b, _ = graph_metrics(G_b, n_nodes)\n",
    "        dens.append(d_b); gccs.append(g_b)\n",
    "    lo = 100*alpha/2; hi = 100*(1-alpha/2)\n",
    "    den_lo, den_hi = np.percentile(dens, [lo, hi])\n",
    "    gcc_lo, gcc_hi = np.percentile(gccs, [lo, hi])\n",
    "    return (den_lo, den_hi, gcc_lo, gcc_hi)\n",
    "\n",
    "# --- load data ---\n",
    "authors = load_authors(AUTHORS_FILE)\n",
    "n_all   = len(authors)\n",
    "df_all  = load_edges_df(EDGES_FILE)\n",
    "sims    = df_all[\"w\"].astype(\"float64\").values\n",
    "\n",
    "# --- compute tau_c per percentile (con custom_round) ---\n",
    "taus = {p: custom_round(np.percentile(sims, p)) for p in PERCENTILES}\n",
    "print(\"tau_c by percentile:\", {k: f\"{v:.3f}\" for k,v in taus.items()})\n",
    "\n",
    "# --- sweep & metrics ---\n",
    "rows = []\n",
    "rng = np.random.default_rng(123)\n",
    "for p in PERCENTILES:\n",
    "    tau = taus[p]\n",
    "    df_tau = df_all[df_all[\"w\"] > tau]  # STRICT > come nel tuo pipeline\n",
    "\n",
    "    G = build_graph_undirected(authors, df_tau)\n",
    "    dens, gcc_pct, m_edges = graph_metrics(G, n_all)\n",
    "\n",
    "    if BOOTSTRAP:\n",
    "        den_lo, den_hi, gcc_lo, gcc_hi = edge_bootstrap_ci(authors, df_tau, n_all,\n",
    "                                                            B=B_REPS, alpha=ALPHA, rng=rng)\n",
    "    else:\n",
    "        den_lo = den_hi = gcc_lo = gcc_hi = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"percentile\": p,\n",
    "        \"tau_c\": tau,\n",
    "        \"edges\": m_edges,\n",
    "        \"density\": dens,\n",
    "        \"density_lo\": den_lo,\n",
    "        \"density_hi\": den_hi,\n",
    "        \"gcc_pct\": gcc_pct,\n",
    "        \"gcc_pct_lo\": gcc_lo,\n",
    "        \"gcc_pct_hi\": gcc_hi\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(rows).sort_values(\"percentile\")\n",
    "res.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved metrics → {OUT_CSV}\")\n",
    "\n",
    "# --- plot ---\n",
    "x = res[\"percentile\"].values\n",
    "y_den = res[\"density\"].values\n",
    "y_gcc = res[\"gcc_pct\"].values\n",
    "\n",
    "# --- cosmetic + baseline line + (optional) bootstrap ribbons already supported ---\n",
    "BASELINE = 90  # percentile baseline\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7.0, 3.6))\n",
    "\n",
    "# left axis: density\n",
    "(line1,) = ax1.plot(x, y_den, marker=\"o\", linestyle=\"-\", label=\"CSL density\", linewidth=2, markersize=6)\n",
    "ax1.set_xlabel(r\"Content similarity threshold percentile ($\\tau_c$)\")\n",
    "ax1.set_ylabel(\"CSL density\")\n",
    "ax1.grid(True, which=\"both\", axis=\"both\", alpha=0.3)\n",
    "\n",
    "# ribbons se disponibili\n",
    "if BOOTSTRAP and not res[\"density_lo\"].isna().all():\n",
    "    ax1.fill_between(x, res[\"density_lo\"].values, res[\"density_hi\"].values, alpha=0.2)\n",
    "\n",
    "# baseline marker\n",
    "ax1.axvline(BASELINE, linestyle=\"--\", linewidth=1)\n",
    "ax1.text(BASELINE+0.5, ax1.get_ylim()[1]*0.95, \"baseline\", rotation=90, va=\"top\")\n",
    "\n",
    "# right axis: GCC%\n",
    "ax2 = ax1.twinx()\n",
    "(line2,) = ax2.plot(x, y_gcc, marker=\"s\", linestyle=\"--\", label=\"GCC size (%)\", linewidth=2, markersize=6)\n",
    "ax2.set_ylabel(\"GCC size (% of nodes)\")\n",
    "\n",
    "if BOOTSTRAP and not res[\"gcc_pct_lo\"].isna().all():\n",
    "    ax2.fill_between(x, res[\"gcc_pct_lo\"].values, res[\"gcc_pct_hi\"].values, alpha=0.15)\n",
    "\n",
    "# legenda combinata\n",
    "lines = [line1, line2]\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc=\"best\", frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"Saved figure → {OUT_FIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2160082f-f657-47d9-806c-4efbca5704c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau_c by percentile: {50: '0.309', 60: '0.329', 70: '0.369', 75: '0.379', 80: '0.409', 85: '0.429', 90: '0.459', 95: '0.519'}\n",
      "Saved metrics → data/thresholds_analysis/results_threshold_crc_coef_auc.csv\n",
      "Saved figure -> data/thresholds_analysis/fig_threshold_crc_coef_auc.png\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PATHS / OUTPUT\n",
    "# -------------------------------------------------\n",
    "AUTHORS_FILE   = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "LABELS_FILE    = os.path.join(event_folder, \"CRC_radicalization_analysis.csv\")  # columns: author, radical\n",
    "CSL_ALL_EDGES  = os.path.join(event_folder, \"cslasl-pre\", \"edges\", \"edges.txt\") # u;v;sim\n",
    "UIL_FILE       = os.path.join(event_folder, \"network\", \"uil\", \"edges.txt\")      # u;v;w\n",
    "TDL_FILE       = os.path.join(event_folder, \"network\", \"tdl\", \"edges.txt\")      # u;v;w\n",
    "ASL_FILE       = os.path.join(event_folder, \"network\", \"asl\", \"edges.txt\")      # u;v;w\n",
    "\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"results_threshold_crc_coef_auc.csv\")\n",
    "OUT_FIG = os.path.join(OUT_DIR, \"fig_threshold_crc_coef_auc.png\")\n",
    "\n",
    "PERCENTILES = [50, 60, 70, 75, 80, 85, 90, 95]\n",
    "BASELINE = 90\n",
    "\n",
    "# CRC hyperparams (coerenti con la tua funzione)\n",
    "omega = 1.0\n",
    "beta  = {\"uil\": 1.0, \"csl\": 1.0, \"tdl\": 1.0, \"asl\": 1.0}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def custom_round(x: float) -> float:\n",
    "    r = round(float(x), 3)\n",
    "    s = f\"{r:.3f}\"\n",
    "    return float(s[:-1] + \"9\")\n",
    "\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def load_edges_semicolon(path):\n",
    "    if not os.path.isfile(path):\n",
    "        return pd.DataFrame(columns=[\"u\",\"v\",\"w\"])\n",
    "    return pd.read_csv(path, sep=\";\", header=None, names=[\"u\",\"v\",\"w\"],\n",
    "                       dtype={\"u\":\"string\",\"v\":\"string\",\"w\":\"float64\"}, engine=\"c\")\n",
    "\n",
    "def strength_from_stream_file(path, authors_set):\n",
    "    strengths = {a: 0.0 for a in authors_set}\n",
    "    if not os.path.isfile(path):\n",
    "        return strengths\n",
    "    chunksize = 1_000_00  # 100k righe per chunk (snappy e memory-friendly)\n",
    "    for chunk in pd.read_csv(path, sep=\";\", header=None, names=[\"u\",\"v\",\"w\"],\n",
    "                             dtype={\"u\":\"string\",\"v\":\"string\",\"w\":\"float64\"},\n",
    "                             engine=\"c\", chunksize=chunksize):\n",
    "        chunk = chunk.dropna(subset=[\"u\",\"v\",\"w\"])\n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        tw = np.log1p(chunk[\"w\"].values)\n",
    "        s_u = pd.Series(tw, index=chunk[\"u\"].values).groupby(level=0).sum()\n",
    "        s_v = pd.Series(tw, index=chunk[\"v\"].values).groupby(level=0).sum()\n",
    "        s = s_u.add(s_v, fill_value=0.0)\n",
    "        for node, val in s.items():\n",
    "            if node in strengths:\n",
    "                strengths[node] += float(val)\n",
    "            else:\n",
    "                strengths[node] = float(val)\n",
    "    return strengths\n",
    "\n",
    "def strength_from_csl_df(df_csl, authors_set):\n",
    "    strengths = {a: 0.0 for a in authors_set}\n",
    "    if df_csl.empty:\n",
    "        return strengths\n",
    "    tw = np.log1p(df_csl[\"w\"].values)\n",
    "    s_u = pd.Series(tw, index=df_csl[\"u\"].values).groupby(level=0).sum()\n",
    "    s_v = pd.Series(tw, index=df_csl[\"v\"].values).groupby(level=0).sum()\n",
    "    s = s_u.add(s_v, fill_value=0.0)\n",
    "    for node, val in s.items():\n",
    "        if node in strengths:\n",
    "            strengths[node] += float(val)\n",
    "        else:\n",
    "            strengths[node] = float(val)\n",
    "    return strengths\n",
    "\n",
    "def compute_crc_from_strengths(str_by_layer, authors, omega=1.0, beta=None):\n",
    "    if beta is None:\n",
    "        beta = {l: 1.0 for l in str_by_layer.keys()}\n",
    "    n = len(authors)\n",
    "    CRC = {}\n",
    "    total_phi = {a: sum(str_by_layer[l].get(a, 0.0) for l in str_by_layer.keys()) for a in authors}\n",
    "    for a in authors:\n",
    "        prod = 1.0\n",
    "        for l in str_by_layer.keys():\n",
    "            phi_val = str_by_layer[l].get(a, 0.0)\n",
    "            eff = (phi_val + omega * (total_phi[a] - phi_val)) / max(n - 1, 1)\n",
    "            prod *= (1.0 + beta.get(l, 1.0) * eff)\n",
    "        CRC[a] = prod\n",
    "    return CRC\n",
    "\n",
    "def fit_logit_and_ci(x, y):\n",
    "    alpha = np.nan; lo = np.nan; hi = np.nan; auc = np.nan\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X = sm.add_constant(x)\n",
    "        res = sm.Logit(y, X).fit(disp=0)\n",
    "        alpha = float(res.params[1])\n",
    "        ci = res.conf_int(alpha=0.05)\n",
    "        lo, hi = float(ci.iloc[1,0]), float(ci.iloc[1,1])\n",
    "        p = res.predict(X)\n",
    "        auc = roc_auc_score(y, p) if len(np.unique(y)) == 2 else np.nan\n",
    "        return alpha, lo, hi, auc\n",
    "    except Exception:\n",
    "        rng = np.random.default_rng(42)\n",
    "        n = len(y)\n",
    "        coefs = []\n",
    "        for _ in range(500):\n",
    "            idx = rng.integers(0, n, size=n)\n",
    "            xb = x[idx].reshape(-1,1)\n",
    "            yb = y[idx]\n",
    "            if len(np.unique(yb)) < 2:\n",
    "                continue\n",
    "            lr = LogisticRegression(solver=\"liblinear\")\n",
    "            lr.fit(xb, yb)\n",
    "            coefs.append(lr.coef_.ravel()[0])\n",
    "        if len(coefs) > 0:\n",
    "            alpha = float(np.median(coefs))\n",
    "            lo, hi = np.percentile(coefs, [2.5, 97.5])\n",
    "        lr = LogisticRegression(solver=\"liblinear\")\n",
    "        lr.fit(x.reshape(-1,1), y)\n",
    "        p = lr.predict_proba(x.reshape(-1,1))[:,1]\n",
    "        auc = roc_auc_score(y, p)\n",
    "        return alpha, lo, hi, auc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LOAD\n",
    "# -------------------------------------------------\n",
    "authors = load_authors(AUTHORS_FILE)\n",
    "labels_df = pd.read_csv(LABELS_FILE, usecols=[\"author\",\"radical\"])\n",
    "labels_df[\"author\"] = labels_df[\"author\"].astype(str)\n",
    "labels_map = dict(zip(labels_df[\"author\"], labels_df[\"radical\"].astype(int)))\n",
    "\n",
    "# allinea autori con etichette disponibili\n",
    "y = np.array([labels_map.get(a, np.nan) for a in authors])\n",
    "mask = ~np.isnan(y)\n",
    "authors_lab = [a for a, m in zip(authors, mask) if m]\n",
    "y = y[mask].astype(int)\n",
    "if len(np.unique(y)) < 2:\n",
    "    raise SystemExit(\"Labels must contain both classes 0/1.\")\n",
    "\n",
    "authors_set = set(authors_lab)\n",
    "\n",
    "# layer fissi (UIL/TDL/ASL): strength streaming\n",
    "str_uil = strength_from_stream_file(UIL_FILE, authors_set)\n",
    "str_tdl = strength_from_stream_file(TDL_FILE, authors_set)\n",
    "str_asl = strength_from_stream_file(ASL_FILE, authors_set)\n",
    "\n",
    "# CSL candidati (filtra su autori con label)\n",
    "df_csl_all = load_edges_semicolon(CSL_ALL_EDGES)\n",
    "if not df_csl_all.empty:\n",
    "    df_csl_all = df_csl_all[df_csl_all[\"u\"].isin(authors_set) & df_csl_all[\"v\"].isin(authors_set)].reset_index(drop=True)\n",
    "\n",
    "sims = df_csl_all[\"w\"].astype(\"float64\").values if not df_csl_all.empty else np.array([0.0])\n",
    "taus = {p: custom_round(np.percentile(sims, p)) for p in PERCENTILES}\n",
    "print(\"tau_c by percentile:\", {k: f\"{v:.3f}\" for k, v in taus.items()})\n",
    "\n",
    "# -------------------------------------------------\n",
    "# SWEEP\n",
    "# -------------------------------------------------\n",
    "rows = []\n",
    "for p in PERCENTILES:\n",
    "    tau = taus[p]\n",
    "    # CSL strict >\n",
    "    if df_csl_all.empty:\n",
    "        df_csl_tau = pd.DataFrame(columns=[\"u\",\"v\",\"w\"])\n",
    "    else:\n",
    "        df_csl_tau = df_csl_all[df_csl_all[\"w\"] > tau].reset_index(drop=True)\n",
    "\n",
    "    # strengths per layer\n",
    "    str_csl = strength_from_csl_df(df_csl_tau, authors_set)\n",
    "    str_by_layer = {\"uil\": str_uil, \"csl\": str_csl, \"tdl\": str_tdl, \"asl\": str_asl}\n",
    "\n",
    "    # CRC per autore\n",
    "    CRC = compute_crc_from_strengths(str_by_layer, authors_lab, omega=omega, beta=beta)\n",
    "    x = np.array([CRC[a] for a in authors_lab], dtype=float)\n",
    "\n",
    "    # z-score\n",
    "    mu, sd = x.mean(), x.std()\n",
    "    if sd == 0: sd = 1.0\n",
    "    xz = (x - mu) / sd\n",
    "\n",
    "    alpha, lo, hi, auc = fit_logit_and_ci(xz, y)\n",
    "    rows.append({\n",
    "        \"percentile\": p,\n",
    "        \"tau_c\": tau,\n",
    "        \"alpha\": alpha,\n",
    "        \"alpha_lo\": lo,\n",
    "        \"alpha_hi\": hi,\n",
    "        \"auc\": auc,\n",
    "        \"edges_csl\": len(df_csl_tau)\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(rows).sort_values(\"percentile\")\n",
    "res.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved metrics → {OUT_CSV}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PLOT (legend clean)\n",
    "# -------------------------------------------------\n",
    "xp  = res[\"percentile\"].values\n",
    "yal = res[\"alpha\"].values\n",
    "ylo = res[\"alpha_lo\"].values\n",
    "yhi = res[\"alpha_hi\"].values\n",
    "yauc= res[\"auc\"].values\n",
    "\n",
    "# safe yerr (evita NaN)\n",
    "yerr_low  = np.nan_to_num(yal - ylo, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "yerr_high = np.nan_to_num(yhi - yal, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "yerr = np.vstack([yerr_low, yerr_high])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(7.0, 3.6))\n",
    "\n",
    "# α + CI (senza label per evitare _nolegend_)\n",
    "err_container = ax1.errorbar(xp, yal, yerr=yerr, fmt=\"o\", capsize=3, elinewidth=1.2)\n",
    "\n",
    "ax1.set_xlabel(r\"Content similarity threshold percentile ($\\tau_c$)\")\n",
    "ax1.set_ylabel(r\"Logistic coefficient $\\alpha$ (CRC)\")\n",
    "ax1.grid(True, which=\"both\", axis=\"both\", alpha=0.3)\n",
    "\n",
    "# baseline (no label)\n",
    "baseline_line = ax1.axvline(BASELINE, linestyle=\"--\", linewidth=1, zorder=0)\n",
    "\n",
    "# asse destro per AUC\n",
    "ax2 = ax1.twinx()\n",
    "(line_auc,) = ax2.plot(xp, yauc, marker=\"s\", linestyle=\"-\")\n",
    "ax2.set_ylabel(\"AUC\")\n",
    "\n",
    "# stringi asse destro e formatta\n",
    "if np.isfinite(yauc).any():\n",
    "    m, M = float(np.nanmin(yauc)), float(np.nanmax(yauc))\n",
    "    pad = max(0.0025, 0.15 * (M - m if M > m else 0.005))\n",
    "    ax2.set_ylim(m - pad, M + pad)\n",
    "ax2.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "\n",
    "# LEGEND pulita con proxy (colori presi dagli handle reali)\n",
    "alpha_color = err_container.lines[0].get_color()\n",
    "auc_color   = line_auc.get_color()\n",
    "try:\n",
    "    base_color = baseline_line.get_edgecolor()\n",
    "except Exception:\n",
    "    base_color = \"0.35\"\n",
    "\n",
    "alpha_proxy    = Line2D([], [], marker=\"o\", linestyle=\"none\", color=alpha_color, label=r\"$\\alpha$ (95% CI)\")\n",
    "auc_proxy      = Line2D([], [], marker=\"s\", linestyle=\"-\",  color=auc_color,   label=\"AUC\")\n",
    "baseline_proxy = Line2D([], [], linestyle=\"--\", color=base_color,              label=\"baseline\")\n",
    "\n",
    "ax1.legend([alpha_proxy, auc_proxy, baseline_proxy],\n",
    "           [alpha_proxy.get_label(), \"AUC\", \"baseline\"],\n",
    "           loc=\"best\", frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG, bbox_inches=\"tight\", dpi=150)\n",
    "plt.close(fig)\n",
    "print(f\"Saved figure -> {OUT_FIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "041761a8-79c5-4970-8c37-1c7796d01544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau_c by percentile: {50: '0.309', 60: '0.329', 70: '0.369', 75: '0.379', 80: '0.409', 85: '0.429', 90: '0.459', 95: '0.519'}\n",
      "Saved Jaccard table → data/thresholds_analysis/results_threshold_crc_jaccard.csv\n",
      "Saved figure -> data/thresholds_analysis/fig_threshold_jaccard_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "# ----------------- PATHS -----------------\n",
    "AUTHORS_FILE   = os.path.join(event_folder, \"network\", \"authors.txt\")\n",
    "LABELS_FILE    = os.path.join(event_folder, \"CRC_radicalization_analysis.csv\")  # columns: author, radical\n",
    "CSL_ALL_EDGES  = os.path.join(event_folder, \"cslasl-pre\", \"edges\", \"edges.txt\") # u;v;sim\n",
    "UIL_FILE       = os.path.join(event_folder, \"network\", \"uil\", \"edges.txt\")      # u;v;w\n",
    "TDL_FILE       = os.path.join(event_folder, \"network\", \"tdl\", \"edges.txt\")      # u;v;w\n",
    "ASL_FILE       = os.path.join(event_folder, \"network\", \"asl\", \"edges.txt\")      # u;v;w\n",
    "\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"results_threshold_crc_jaccard.csv\")\n",
    "OUT_FIG = os.path.join(OUT_DIR, \"fig_threshold_jaccard_heatmap.png\")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "PERCENTILES = [50, 60, 70, 75, 80, 85, 90, 95]\n",
    "BASELINE = 90\n",
    "K_FRACS = [0.005, 0.01, 0.05]     # 0.5%, 1%, 5%\n",
    "K_LABELS = [\"0.5%\", \"1%\", \"5%\"]\n",
    "\n",
    "omega = 1.0\n",
    "beta  = {\"uil\": 1.0, \"csl\": 1.0, \"tdl\": 1.0, \"asl\": 1.0}\n",
    "\n",
    "# ----------------- HELPERS -----------------\n",
    "def custom_round(x: float) -> float:\n",
    "    r = round(float(x), 3)\n",
    "    s = f\"{r:.3f}\"\n",
    "    return float(s[:-1] + \"9\")\n",
    "\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def load_edges_semicolon(path):\n",
    "    if not os.path.isfile(path):\n",
    "        return pd.DataFrame(columns=[\"u\",\"v\",\"w\"])\n",
    "    return pd.read_csv(path, sep=\";\", header=None, names=[\"u\",\"v\",\"w\"],\n",
    "                       dtype={\"u\":\"string\",\"v\":\"string\",\"w\":\"float64\"}, engine=\"c\")\n",
    "\n",
    "def strength_from_stream_file(path, authors_set):\n",
    "    strengths = {a: 0.0 for a in authors_set}\n",
    "    if not os.path.isfile(path):\n",
    "        return strengths\n",
    "    chunksize = 100_000\n",
    "    for chunk in pd.read_csv(path, sep=\";\", header=None, names=[\"u\",\"v\",\"w\"],\n",
    "                             dtype={\"u\":\"string\",\"v\":\"string\",\"w\":\"float64\"},\n",
    "                             engine=\"c\", chunksize=chunksize):\n",
    "        chunk = chunk.dropna(subset=[\"u\",\"v\",\"w\"])\n",
    "        if chunk.empty: \n",
    "            continue\n",
    "        tw = np.log1p(chunk[\"w\"].values)\n",
    "        s_u = pd.Series(tw, index=chunk[\"u\"].values).groupby(level=0).sum()\n",
    "        s_v = pd.Series(tw, index=chunk[\"v\"].values).groupby(level=0).sum()\n",
    "        s = s_u.add(s_v, fill_value=0.0)\n",
    "        for node, val in s.items():\n",
    "            if node in strengths:\n",
    "                strengths[node] += float(val)\n",
    "            else:\n",
    "                strengths[node] = float(val)\n",
    "    return strengths\n",
    "\n",
    "def strength_from_csl_df(df_csl, authors_set):\n",
    "    strengths = {a: 0.0 for a in authors_set}\n",
    "    if df_csl.empty:\n",
    "        return strengths\n",
    "    tw = np.log1p(df_csl[\"w\"].values)\n",
    "    s_u = pd.Series(tw, index=df_csl[\"u\"].values).groupby(level=0).sum()\n",
    "    s_v = pd.Series(tw, index=df_csl[\"v\"].values).groupby(level=0).sum()\n",
    "    s = s_u.add(s_v, fill_value=0.0)\n",
    "    for node, val in s.items():\n",
    "        if node in strengths:\n",
    "            strengths[node] += float(val)\n",
    "        else:\n",
    "            strengths[node] = float(val)\n",
    "    return strengths\n",
    "\n",
    "def compute_crc_from_strengths(str_by_layer, authors, omega=1.0, beta=None):\n",
    "    if beta is None:\n",
    "        beta = {l: 1.0 for l in str_by_layer.keys()}\n",
    "    n = len(authors)\n",
    "    CRC = {}\n",
    "    total_phi = {a: sum(str_by_layer[l].get(a, 0.0) for l in str_by_layer.keys()) for a in authors}\n",
    "    for a in authors:\n",
    "        prod = 1.0\n",
    "        for l in str_by_layer.keys():\n",
    "            phi_val = str_by_layer[l].get(a, 0.0)\n",
    "            eff = (phi_val + omega * (total_phi[a] - phi_val)) / max(n - 1, 1)\n",
    "            prod *= (1.0 + beta.get(l, 1.0) * eff)\n",
    "        CRC[a] = prod\n",
    "    return CRC\n",
    "\n",
    "def topk_set(crc_dict, k):\n",
    "    \"\"\"Ritorna l'insieme dei top-k (k intero) ordinando per score decrescente, tie-break per autore.\"\"\"\n",
    "    items = sorted(crc_dict.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "    k = max(1, min(k, len(items)))\n",
    "    return set([a for a, _ in items[:k]])\n",
    "\n",
    "def jaccard(a, b):\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    inter = len(a & b)\n",
    "    union = len(a | b)\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "# ----------------- LOAD -----------------\n",
    "authors = load_authors(AUTHORS_FILE)\n",
    "\n",
    "# se vuoi limitarti agli autori con etichetta (coerenza con le altre figure)\n",
    "labels_df = pd.read_csv(LABELS_FILE, usecols=[\"author\",\"radical\"])\n",
    "labels_df[\"author\"] = labels_df[\"author\"].astype(str)\n",
    "lab_set = set(labels_df[\"author\"].tolist())\n",
    "authors = [a for a in authors if a in lab_set]   # mantieni ordine\n",
    "authors_set = set(authors)\n",
    "nA = len(authors)\n",
    "\n",
    "df_csl_all = load_edges_semicolon(CSL_ALL_EDGES)\n",
    "if not df_csl_all.empty:\n",
    "    df_csl_all = df_csl_all[df_csl_all[\"u\"].isin(authors_set) & df_csl_all[\"v\"].isin(authors_set)].reset_index(drop=True)\n",
    "\n",
    "# layer fissi strengths una volta sola\n",
    "str_uil = strength_from_stream_file(UIL_FILE, authors_set)\n",
    "str_tdl = strength_from_stream_file(TDL_FILE, authors_set)\n",
    "str_asl = strength_from_stream_file(ASL_FILE, authors_set)\n",
    "\n",
    "# percentili su CSL candidati\n",
    "sims = df_csl_all[\"w\"].astype(\"float64\").values if not df_csl_all.empty else np.array([0.0])\n",
    "taus = {p: custom_round(np.percentile(sims, p)) for p in PERCENTILES}\n",
    "print(\"tau_c by percentile:\", {k: f\"{v:.3f}\" for k, v in taus.items()})\n",
    "\n",
    "# ----------------- CRC per ciascun tau -----------------\n",
    "crc_by_tau = {}\n",
    "for p in PERCENTILES:\n",
    "    tau = taus[p]\n",
    "    if df_csl_all.empty:\n",
    "        df_csl_tau = pd.DataFrame(columns=[\"u\",\"v\",\"w\"])\n",
    "    else:\n",
    "        df_csl_tau = df_csl_all[df_csl_all[\"w\"] > tau].reset_index(drop=True)\n",
    "    str_csl = strength_from_csl_df(df_csl_tau, authors_set)\n",
    "    str_by_layer = {\"uil\": str_uil, \"csl\": str_csl, \"tdl\": str_tdl, \"asl\": str_asl}\n",
    "    crc_by_tau[p] = compute_crc_from_strengths(str_by_layer, authors, omega=omega, beta=beta)\n",
    "\n",
    "# baseline sets (90th)\n",
    "if BASELINE not in crc_by_tau:\n",
    "    raise SystemExit(\"Baseline percentile not computed.\")\n",
    "baseline_crc = crc_by_tau[BASELINE]\n",
    "\n",
    "k_sizes = [max(1, int(np.ceil(fr * nA))) for fr in K_FRACS]\n",
    "\n",
    "baseline_sets = {K_LABELS[i]: topk_set(baseline_crc, k_sizes[i]) for i in range(len(K_FRACS))}\n",
    "\n",
    "# ----------------- Jaccard matrix -----------------\n",
    "M = np.zeros((len(PERCENTILES), len(K_FRACS)), dtype=float)\n",
    "for i, p in enumerate(PERCENTILES):\n",
    "    crc_p = crc_by_tau[p]\n",
    "    for j, ksz in enumerate(k_sizes):\n",
    "        s_tau = topk_set(crc_p, ksz)\n",
    "        base_set = baseline_sets[K_LABELS[j]]\n",
    "        M[i, j] = jaccard(s_tau, base_set)\n",
    "\n",
    "# salva CSV (formato wide: una riga per percentile)\n",
    "df_out = pd.DataFrame({\n",
    "    \"percentile\": PERCENTILES,\n",
    "    K_LABELS[0]: M[:, 0],\n",
    "    K_LABELS[1]: M[:, 1],\n",
    "    K_LABELS[2]: M[:, 2],\n",
    "})\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved Jaccard table → {OUT_CSV}\")\n",
    "\n",
    "# ----------------- HEATMAP -----------------\n",
    "fig, ax = plt.subplots(figsize=(6.8, 3.8))\n",
    "im = ax.imshow(M, aspect=\"auto\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "# axis ticks/labels\n",
    "ax.set_yticks(range(len(PERCENTILES)))\n",
    "ax.set_yticklabels([f\"{p}%\" for p in PERCENTILES])\n",
    "ax.set_xticks(range(len(K_LABELS)))\n",
    "ax.set_xticklabels([f\"top-{lbl}\" for lbl in K_LABELS])\n",
    "\n",
    "ax.set_xlabel(\"Top-k set\")\n",
    "ax.set_ylabel(r\"Threshold percentile $\\tau_c$\")\n",
    "\n",
    "# colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label(\"Jaccard overlap\")\n",
    "\n",
    "# annotate cells\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        ax.text(j, i, f\"{M[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "# baseline row marker\n",
    "try:\n",
    "    base_idx = PERCENTILES.index(BASELINE)\n",
    "    ax.hlines(base_idx, -0.5, len(K_LABELS)-0.5, colors=\"k\", linestyles=\"--\", linewidth=1)\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_FIG, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(f\"Saved figure -> {OUT_FIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "511b897d-93b9-46c0-99b3-565d095378bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> data/thresholds_analysis/robustness_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>alpha@90</th>\n",
       "      <th>alpha_ci</th>\n",
       "      <th>AUC@90</th>\n",
       "      <th>Max AUC drop (%)</th>\n",
       "      <th>Min Jaccard top-1%</th>\n",
       "      <th>Min GCC%</th>\n",
       "      <th>✓_alpha</th>\n",
       "      <th>✓_AUC</th>\n",
       "      <th>✓_Net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008 Elections</td>\n",
       "      <td>1.301132</td>\n",
       "      <td></td>\n",
       "      <td>0.781060</td>\n",
       "      <td>1.196286</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>82.782160</td>\n",
       "      <td>✗</td>\n",
       "      <td>✓</td>\n",
       "      <td>✓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011 Occupy Wall Street</td>\n",
       "      <td>1.076047</td>\n",
       "      <td></td>\n",
       "      <td>0.767263</td>\n",
       "      <td>2.567598</td>\n",
       "      <td>0.853801</td>\n",
       "      <td>59.265817</td>\n",
       "      <td>✗</td>\n",
       "      <td>✓</td>\n",
       "      <td>✗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016 Elections</td>\n",
       "      <td>1.124495</td>\n",
       "      <td></td>\n",
       "      <td>0.779893</td>\n",
       "      <td>1.878128</td>\n",
       "      <td>0.849375</td>\n",
       "      <td>86.639579</td>\n",
       "      <td>✗</td>\n",
       "      <td>✓</td>\n",
       "      <td>✓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017 Charlottesville Rally</td>\n",
       "      <td>0.989415</td>\n",
       "      <td></td>\n",
       "      <td>0.755183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>✗</td>\n",
       "      <td>✓</td>\n",
       "      <td>✗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021 Capitol Riot</td>\n",
       "      <td>0.897817</td>\n",
       "      <td></td>\n",
       "      <td>0.737315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>✗</td>\n",
       "      <td>✓</td>\n",
       "      <td>✗</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Event  alpha@90 alpha_ci    AUC@90  Max AUC drop (%)  \\\n",
       "0              2008 Elections  1.301132           0.781060          1.196286   \n",
       "1     2011 Occupy Wall Street  1.076047           0.767263          2.567598   \n",
       "2              2016 Elections  1.124495           0.779893          1.878128   \n",
       "3  2017 Charlottesville Rally  0.989415           0.755183          0.000000   \n",
       "4           2021 Capitol Riot  0.897817           0.737315          0.000000   \n",
       "\n",
       "   Min Jaccard top-1%   Min GCC% ✓_alpha ✓_AUC ✓_Net  \n",
       "0            0.808219  82.782160       ✗     ✓     ✓  \n",
       "1            0.853801  59.265817       ✗     ✓     ✗  \n",
       "2            0.849375  86.639579       ✗     ✓     ✓  \n",
       "3            1.000000   0.000960       ✗     ✓     ✗  \n",
       "4            1.000000   0.000393       ✗     ✓     ✗  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "EVENTS = {\n",
    "    \"2008 Elections\": \"data/2008_elections\",\n",
    "    \"2011 Occupy Wall Street\": \"data/2011_wallstreet\",\n",
    "    \"2016 Elections\": \"data/2016_elections\",\n",
    "    \"2017 Charlottesville Rally\": \"data/2017_rally\",\n",
    "    \"2021 Capitol Riot\": \"data/2021_riot\",\n",
    "}\n",
    "\n",
    "OUT_DIR = os.path.join(\"data\", \"thresholds_analysis\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"robustness_summary.csv\")\n",
    "\n",
    "P_SWEEP_ALPHA = [70, 75, 80, 85, 90, 95]   # for α and GCC\n",
    "P_SWEEP_JACC  = [80, 85, 90, 95]           # for top-1% Jaccard\n",
    "BASELINE = 90\n",
    "AUC_DROP_OK = 0.05   # <= 5%\n",
    "JACC_MIN_OK = 0.60\n",
    "GCC_MIN_OK  = 80.0\n",
    "\n",
    "omega = 1.0\n",
    "beta  = {\"uil\": 1.0, \"csl\": 1.0, \"tdl\": 1.0, \"asl\": 1.0}\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def custom_round(x: float) -> float:\n",
    "    r = round(float(x), 3); s = f\"{r:.3f}\"\n",
    "    return float(s[:-1] + \"9\")\n",
    "\n",
    "def load_authors(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "def strength_from_stream_file(path, authors_set):\n",
    "    \"\"\"UIL/TDL/ASL: stream 'u;v;w' and accumulate log1p(w). Sparse dict.\"\"\"\n",
    "    strengths = defaultdict(float)\n",
    "    if not os.path.isfile(path):\n",
    "        return strengths\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            parts = line.rstrip(\"\\n\").split(\";\")\n",
    "            if len(parts) != 3: continue\n",
    "            u, v, ws = parts\n",
    "            if (u not in authors_set) or (v not in authors_set): continue\n",
    "            try:\n",
    "                w = float(ws)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            inc = math.log1p(w)\n",
    "            strengths[u] += inc\n",
    "            strengths[v] += inc\n",
    "    return strengths\n",
    "\n",
    "def get_tau_percentiles_stream(path, percentiles):\n",
    "    \"\"\"Compute CSL similarity percentiles by streaming. Works across tdigest versions.\"\"\"\n",
    "    from tdigest import TDigest\n",
    "    d = TDigest()\n",
    "    count = 0\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                parts = line.rstrip(\"\\n\").split(\";\")\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                try:\n",
    "                    d.update(float(parts[2]))\n",
    "                    count += 1\n",
    "                except Exception:\n",
    "                    continue\n",
    "    taus = {}\n",
    "    for p in sorted(set(percentiles)):\n",
    "        if count == 0:\n",
    "            val = 0.0\n",
    "        else:\n",
    "            # Prefer percentile([0..100]); fall back to quantile([0..1])\n",
    "            try:\n",
    "                val = d.percentile(p)             # tdigest ≥0.5.2\n",
    "            except Exception:\n",
    "                val = d.quantile(p / 100.0)       # older API\n",
    "        taus[p] = custom_round(float(val) if val is not None else 0.0)\n",
    "    return taus\n",
    "\n",
    "class DSU:\n",
    "    \"\"\"Union–find for GCC% without building a graph.\"\"\"\n",
    "    __slots__ = (\"parent\",\"size\")\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "        self.size = [1]*n\n",
    "    def find(self, x):\n",
    "        while self.parent[x] != x:\n",
    "            self.parent[x] = self.parent[self.parent[x]]\n",
    "            x = self.parent[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra, rb = self.find(a), self.find(b)\n",
    "        if ra == rb: return\n",
    "        if self.size[ra] < self.size[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        self.size[ra] += self.size[rb]\n",
    "    def max_size(self):\n",
    "        # Only sizes of roots matter; but scanning size is fine.\n",
    "        return max(self.size) if self.size else 0\n",
    "\n",
    "def csl_strength_and_gcc_stream(path, authors, authors_set, tau):\n",
    "    \"\"\"\n",
    "    Stream CSL once for a given tau:\n",
    "      - accumulate strengths (log1p(w)) for u,v when w>tau\n",
    "      - union endpoints in DSU to track GCC size\n",
    "    Returns (strengths_dict, gcc_pct, edges_kept)\n",
    "    \"\"\"\n",
    "    strengths = defaultdict(float)\n",
    "    n = len(authors)\n",
    "    idx = {a:i for i,a in enumerate(authors)}\n",
    "    dsu = DSU(n)\n",
    "    edges_kept = 0\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                parts = line.rstrip(\"\\n\").split(\";\")\n",
    "                if len(parts) != 3: continue\n",
    "                u, v, ws = parts\n",
    "                if (u not in authors_set) or (v not in authors_set): continue\n",
    "                try:\n",
    "                    w = float(ws)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if w > tau:\n",
    "                    edges_kept += 1\n",
    "                    inc = math.log1p(w)\n",
    "                    strengths[u] += inc\n",
    "                    strengths[v] += inc\n",
    "                    dsu.union(idx[u], idx[v])\n",
    "    gcc_pct = 100.0 * dsu.max_size() / max(n,1)\n",
    "    return strengths, gcc_pct, edges_kept\n",
    "\n",
    "def compute_crc_from_strengths(str_by_layer, authors, omega=1.0, beta=None):\n",
    "    if beta is None: beta = {l: 1.0 for l in str_by_layer.keys()}\n",
    "    # Precompute totals per author lazily\n",
    "    layers = list(str_by_layer.keys())\n",
    "    CRC = {}\n",
    "    n = len(authors)\n",
    "    for a in authors:\n",
    "        total_phi = 0.0\n",
    "        for l in layers:\n",
    "            total_phi += str_by_layer[l].get(a, 0.0)\n",
    "        prod = 1.0\n",
    "        for l in layers:\n",
    "            phi_val = str_by_layer[l].get(a, 0.0)\n",
    "            eff = (phi_val + omega * (total_phi - phi_val)) / max(n - 1, 1)\n",
    "            prod *= (1.0 + beta.get(l, 1.0) * eff)\n",
    "        CRC[a] = prod\n",
    "    return CRC\n",
    "\n",
    "def fit_logit_alpha_auc(x, y):\n",
    "    alpha = np.nan; lo = np.nan; hi = np.nan; auc = np.nan\n",
    "    try:\n",
    "        import statsmodels.api as sm\n",
    "        X = sm.add_constant(x)\n",
    "        res = sm.Logit(y, X).fit(disp=0)\n",
    "        alpha = float(res.params[1])\n",
    "        ci = res.conf_int(alpha=0.05)\n",
    "        lo, hi = float(ci.iloc[1,0]), float(ci.iloc[1,1])\n",
    "        p = res.predict(X)\n",
    "        auc = roc_auc_score(y, p) if len(np.unique(y))==2 else np.nan\n",
    "        return alpha, lo, hi, auc\n",
    "    except Exception:\n",
    "        lr = LogisticRegression(solver=\"liblinear\")\n",
    "        lr.fit(x.reshape(-1,1), y)\n",
    "        p = lr.predict_proba(x.reshape(-1,1))[:,1]\n",
    "        auc = roc_auc_score(y, p)\n",
    "        alpha = float(lr.coef_.ravel()[0])\n",
    "        lo = hi = np.nan\n",
    "        return alpha, lo, hi, auc\n",
    "\n",
    "def topk_set(crc_dict, k):\n",
    "    items = sorted(crc_dict.items(), key=lambda kv: (-kv[1], kv[0]))\n",
    "    k = max(1, min(k, len(items)))\n",
    "    return set(a for a,_ in items[:k])\n",
    "\n",
    "def jaccard(a,b):\n",
    "    if not a and not b: return 1.0\n",
    "    u = len(a|b)\n",
    "    return len(a&b)/u if u>0 else 0.0\n",
    "\n",
    "# ---------- main loop ----------\n",
    "rows = []\n",
    "for ev_name, ev_folder in EVENTS.items():\n",
    "    AUTHORS_FILE   = os.path.join(ev_folder, \"network\", \"authors.txt\")\n",
    "    LABELS_FILE    = os.path.join(ev_folder, \"CRC_radicalization_analysis.csv\")\n",
    "    CSL_ALL_EDGES  = os.path.join(ev_folder, \"cslasl-pre\", \"edges\", \"edges.txt\")\n",
    "    UIL_FILE       = os.path.join(ev_folder, \"network\", \"uil\", \"edges.txt\")\n",
    "    TDL_FILE       = os.path.join(ev_folder, \"network\", \"tdl\", \"edges.txt\")\n",
    "    ASL_FILE       = os.path.join(ev_folder, \"network\", \"asl\", \"edges.txt\")\n",
    "\n",
    "    # Authors + labels\n",
    "    authors = load_authors(AUTHORS_FILE)\n",
    "    labs = pd.read_csv(LABELS_FILE, usecols=[\"author\",\"radical\"]).astype({\"author\":str})\n",
    "    lab_set = set(labs[\"author\"])\n",
    "    authors = [a for a in authors if a in lab_set]  # keep order, ensure labels exist\n",
    "    y = labs.set_index(\"author\").loc[authors, \"radical\"].astype(int).values\n",
    "    authors_set = set(authors)\n",
    "    if len(np.unique(y)) < 2:\n",
    "        # skip event if only one class\n",
    "        continue\n",
    "\n",
    "    # Fixed-layer strengths (streaming, sparse dicts)\n",
    "    str_uil = strength_from_stream_file(UIL_FILE, authors_set)\n",
    "    str_tdl = strength_from_stream_file(TDL_FILE, authors_set)\n",
    "    str_asl = strength_from_stream_file(ASL_FILE, authors_set)\n",
    "\n",
    "    # Percentiles for CSL via streaming digest\n",
    "    taus = get_tau_percentiles_stream(CSL_ALL_EDGES, P_SWEEP_ALPHA + P_SWEEP_JACC)\n",
    "    # Baseline top-1% set (stream CSL once at tau=90 for strengths + GCC not needed here)\n",
    "    str_csl_base, _, _ = csl_strength_and_gcc_stream(CSL_ALL_EDGES, authors, authors_set, taus[BASELINE])\n",
    "    crc_base = compute_crc_from_strengths(\n",
    "        {\"uil\":str_uil,\"csl\":str_csl_base,\"tdl\":str_tdl,\"asl\":str_asl},\n",
    "        authors, omega=omega, beta=beta\n",
    "    )\n",
    "    k1 = max(1, int(math.ceil(0.01 * len(authors))))\n",
    "    base_top1 = topk_set(crc_base, k1)\n",
    "    del str_csl_base, crc_base\n",
    "    gc.collect()\n",
    "\n",
    "    # Sweep α/AUC/GCC with single-pass per tau (low RAM)\n",
    "    alphas, los, his, aucs, gccs = [], [], [], [], []\n",
    "    for p in P_SWEEP_ALPHA:\n",
    "        tau = taus[p]\n",
    "        str_csl_tau, gcc_pct, _ = csl_strength_and_gcc_stream(CSL_ALL_EDGES, authors, authors_set, tau)\n",
    "        str_by_layer = {\"uil\":str_uil,\"csl\":str_csl_tau,\"tdl\":str_tdl,\"asl\":str_asl}\n",
    "        CRC = compute_crc_from_strengths(str_by_layer, authors, omega=omega, beta=beta)\n",
    "        x = np.array([CRC[a] for a in authors], dtype=float)\n",
    "        mu, sd = x.mean(), x.std(); sd = sd if sd>0 else 1.0\n",
    "        xz = (x - mu) / sd\n",
    "        alpha, lo, hi, auc = fit_logit_alpha_auc(xz, y)\n",
    "        alphas.append(alpha); los.append(lo); his.append(hi); aucs.append(auc); gccs.append(gcc_pct)\n",
    "        del str_csl_tau, str_by_layer, x, xz\n",
    "        gc.collect()\n",
    "\n",
    "    # Jaccard min (top-1%) on 80..95 — stream once per tau\n",
    "    jmins = []\n",
    "    for p in P_SWEEP_JACC:\n",
    "        tau = taus[p]\n",
    "        str_csl_tau, _, _ = csl_strength_and_gcc_stream(CSL_ALL_EDGES, authors, authors_set, tau)\n",
    "        crc_tau = compute_crc_from_strengths({\"uil\":str_uil,\"csl\":str_csl_tau,\"tdl\":str_tdl,\"asl\":str_asl},\n",
    "                                             authors, omega=omega, beta=beta)\n",
    "        s_tau = topk_set(crc_tau, k1)\n",
    "        jmins.append(jaccard(s_tau, base_top1))\n",
    "        del str_csl_tau, crc_tau, s_tau\n",
    "        gc.collect()\n",
    "    jmin = float(np.min(jmins)) if jmins else np.nan\n",
    "\n",
    "    # Baseline indices e metriche\n",
    "    idx90 = P_SWEEP_ALPHA.index(BASELINE)\n",
    "    alpha90, lo90, hi90 = alphas[idx90], los[idx90], his[idx90]\n",
    "    auc90 = aucs[idx90]\n",
    "\n",
    "    # Max drop AUC rel. vs baseline\n",
    "    auc_drop_rel = max([max(0.0, (auc90 - a)/auc90) for a in aucs if np.isfinite(a) and np.isfinite(auc90) and auc90>0] + [0.0])\n",
    "    auc_drop_pct = 100.0 * auc_drop_rel\n",
    "\n",
    "    # Min GCC% 70..95\n",
    "    gcc_min = float(np.nanmin(gccs)) if len(gccs)>0 else np.nan\n",
    "\n",
    "    # pass/fail\n",
    "    alpha_pass = all((not np.isnan(l) and l>0.0) for l in los)  # CI low > 0 in 70..95\n",
    "    auc_pass   = auc_drop_rel <= AUC_DROP_OK\n",
    "    net_pass   = (not np.isnan(jmin) and jmin >= JACC_MIN_OK) and (not np.isnan(gcc_min) and gcc_min >= GCC_MIN_OK)\n",
    "\n",
    "    rows.append({\n",
    "        \"Event\": ev_name,\n",
    "        \"alpha@90\": alpha90,\n",
    "        \"alpha_ci\": f\"[{lo90:.2f},{hi90:.2f}]\" if np.isfinite(lo90) and np.isfinite(hi90) else \"\",\n",
    "        \"AUC@90\": auc90,\n",
    "        \"Max AUC drop (%)\": auc_drop_pct,\n",
    "        \"Min Jaccard top-1%\": jmin,\n",
    "        \"Min GCC%\": gcc_min,\n",
    "        \"✓_alpha\": \"✓\" if alpha_pass else \"✗\",\n",
    "        \"✓_AUC\":   \"✓\" if auc_pass else \"✗\",\n",
    "        \"✓_Net\":   \"✓\" if net_pass else \"✗\",\n",
    "    })\n",
    "\n",
    "    # free per-event\n",
    "    del str_uil, str_tdl, str_asl, authors, authors_set, labs, y\n",
    "    gc.collect()\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved -> {OUT_CSV}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d21e8-48d9-497e-ad93-e9c536909413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
