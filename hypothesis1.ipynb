{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "5bb37ce9-4eb5-4438-b0ae-8c119a7952bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shared/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from igraph import Graph\n",
    "import pickle\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "891cbd06-9ce3-49f7-8dbc-14ee3e60b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def compute_strength_stream(edges_file, authors_set):\n",
    "    \"\"\"\n",
    "    Compute the weighted degree (strength) for nodes by streaming through the edges file.\n",
    "    Instead of summing raw weights, a concave transformation is applied to dampen extreme values.\n",
    "    Assumes each line in edges_file is in the format: node1;node2;weight.\n",
    "    \"\"\"\n",
    "    # Initialize strengths for nodes known from the authors file.\n",
    "    strengths = {node: 0.0 for node in authors_set}\n",
    "    with open(edges_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            u, v, w = parts[0], parts[1], float(parts[2])\n",
    "            # Apply concave transformation: log(1 + weight)\n",
    "            transformed_w = np.log(1 + w)\n",
    "            # Update both nodes' strength\n",
    "            if u in strengths:\n",
    "                strengths[u] += transformed_w\n",
    "            else:\n",
    "                strengths[u] = transformed_w\n",
    "            if v in strengths:\n",
    "                strengths[v] += transformed_w\n",
    "            else:\n",
    "                strengths[v] = transformed_w\n",
    "    return strengths\n",
    "\n",
    "def compute_crc(event_name):\n",
    "    \"\"\"\n",
    "    Compute the Composite Reinforced Centrality (CRC) for an event using a streaming approach.\n",
    "    The CRC aggregates the normalized effective influence across four behavioral layers.\n",
    "    The effective influence in each layer is normalized by the maximum possible connections (n-1),\n",
    "    where n is the total number of nodes.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    omega = 1.0  # Uniform interlayer coupling weight\n",
    "    layers_list = [\"uil\", \"csl\", \"tdl\", \"asl\"]\n",
    "    beta = {\n",
    "        \"uil\": 1.0,\n",
    "        \"csl\": 1.0,\n",
    "        \"tdl\": 1.0,\n",
    "        \"asl\": 1.0\n",
    "    }\n",
    "    \n",
    "    # Folder paths\n",
    "    base_folder = \"data\"\n",
    "    event_folder = os.path.join(base_folder, event_name)\n",
    "    network_folder = os.path.join(event_folder, \"network\")\n",
    "    \n",
    "    # Load nodes from authors.txt\n",
    "    nodes_file = os.path.join(network_folder, \"authors.txt\")\n",
    "    with open(nodes_file, \"r\") as f:\n",
    "        all_nodes = [line.strip() for line in f if line.strip()]\n",
    "    all_nodes_set = set(all_nodes)\n",
    "    n = len(all_nodes_set)  # Total number of nodes\n",
    "\n",
    "    # Compute strength (with concave transformation) for each layer using file streaming\n",
    "    phi = {}\n",
    "    for l in layers_list:\n",
    "        edges_file = os.path.join(network_folder, l, \"edges.txt\")\n",
    "        if os.path.exists(edges_file):\n",
    "            strengths = compute_strength_stream(edges_file, all_nodes_set)\n",
    "        else:\n",
    "            print(f\"Warning: No edges file found for layer {l}. Using 0 strength for all nodes.\")\n",
    "            strengths = {node: 0.0 for node in all_nodes_set}\n",
    "        phi[l] = strengths\n",
    "    \n",
    "    # Compute normalized effective influence for each user per layer:\n",
    "    # The effective influence for a node in layer l is defined as:\n",
    "    #   (phi[node] + omega * (total_phi - phi[node])) / (n-1)\n",
    "    effective_phi = {l: {} for l in layers_list}\n",
    "    for node in all_nodes_set:\n",
    "        # Total influence across layers for this node\n",
    "        total_phi = sum(phi[l].get(node, 0.0) for l in layers_list)\n",
    "        for l in layers_list:\n",
    "            phi_val = phi[l].get(node, 0.0)\n",
    "            effective_phi[l][node] = (phi_val + omega * (total_phi - phi_val)) / (n - 1)\n",
    "    \n",
    "    # Compute Composite Reinforced Centrality (CRC) for each user\n",
    "    # CRC is the product over layers of (1 + beta * normalized effective influence)\n",
    "    CRC = {}\n",
    "    for node in all_nodes_set:\n",
    "        prod = 1.0\n",
    "        for l in layers_list:\n",
    "            prod *= (1 + beta[l] * effective_phi[l].get(node, 0.0))\n",
    "        CRC[node] = prod\n",
    "    \n",
    "    # Save CRC values to disk\n",
    "    crc_file = os.path.join(event_folder, \"CRC_values.pkl\")\n",
    "    with open(crc_file, \"wb\") as f:\n",
    "        pickle.dump(CRC, f)\n",
    "    print(\"CRC values saved to:\", crc_file)\n",
    "    \n",
    "    return CRC\n",
    "\n",
    "# Example usage:\n",
    "# crc = compute_crc(\"2008_elections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "75bf1775-6193-400f-9d26-bbf20744962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_crc(crc, event_folder, radical_keywords):\n",
    "    \"\"\"\n",
    "    Given a CRC dictionary (mapping author -> CRC) and the event folder containing\n",
    "    \"authors.txt\" and \"contents.txt\", this function:\n",
    "    \n",
    "      1. Loads the radicalization content data.\n",
    "      2. Computes a radicalization score for each author based on the frequency of radical keywords.\n",
    "      3. Creates a binary radicalization label using the median radicalization score.\n",
    "      4. Merges the CRC values with the radicalization labels.\n",
    "      5. Generates visualizations (histogram and boxplot) comparing CRC distributions by radicalization status.\n",
    "      6. Performs logistic regression to assess the relationship between CRC and radicalization.\n",
    "      7. Computes the Pearson correlation between CRC values and the radicalization label.\n",
    "      \n",
    "    The results are saved in the event folder and the merged DataFrame, logistic regression result, \n",
    "    and correlation value are returned.\n",
    "    \n",
    "    Parameters:\n",
    "        crc (dict): Dictionary mapping each author (str) to their CRC value (float).\n",
    "        event_folder (str): Path to the event folder (e.g., \"data/2008_elections/\").\n",
    "        \n",
    "    Returns:\n",
    "        merged_df (DataFrame): Merged DataFrame with columns 'author', 'CRC', 'content', 'rad_score', and 'radical'.\n",
    "        logit_result: Fitted logistic regression result (statsmodels object).\n",
    "        correlation (float): Pearson correlation between CRC and the binary radicalization label.\n",
    "    \"\"\"\n",
    "    # --------------------------\n",
    "    # Load authors and contents\n",
    "    # --------------------------\n",
    "    authors_file = os.path.join(event_folder, \"network/authors.txt\")\n",
    "    contents_file = os.path.join(event_folder, \"cslasl-pre/contents.txt\")\n",
    "    \n",
    "    with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        authors = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    with open(contents_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents = [line for line in f if line]\n",
    "    \n",
    "    if len(authors) != len(contents):\n",
    "        raise ValueError(\"Mismatch between number of authors and contents.\")\n",
    "    \n",
    "    rad_df = pd.DataFrame({\"author\": authors, \"content\": contents})\n",
    "    \n",
    "    # --------------------------\n",
    "    # Compute radicalization score for each author\n",
    "    # --------------------------\n",
    "    \n",
    "    def compute_radical_score(text):\n",
    "        text_lower = text.lower()\n",
    "        score = 0\n",
    "        for kw in radical_keywords:\n",
    "            score += len(re.findall(r'\\b' + re.escape(kw) + r'\\b', text_lower))\n",
    "        words = text_lower.split()\n",
    "        return score / len(words) if words else 0.0\n",
    "    \n",
    "    rad_df[\"rad_score\"] = rad_df[\"content\"].apply(compute_radical_score)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Create binary radicalization label\n",
    "    # --------------------------\n",
    "    median_score = rad_df[\"rad_score\"].median()\n",
    "    rad_df[\"radical\"] = (rad_df[\"rad_score\"] > median_score).astype(int)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Merge CRC values with radicalization data\n",
    "    # --------------------------\n",
    "    crc_df = pd.DataFrame(list(crc.items()), columns=[\"author\", \"CRC\"])\n",
    "    merged_df = pd.merge(crc_df, rad_df, on=\"author\", how=\"inner\")\n",
    "    \n",
    "    # Ensure that the CRC column is numeric\n",
    "    merged_df[\"CRC\"] = pd.to_numeric(merged_df[\"CRC\"], errors=\"coerce\")\n",
    "    merged_df[\"radical\"] = merged_df[\"radical\"].astype(int)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Visualization: Distribution of CRC by radicalization status\n",
    "    # --------------------------\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(data=merged_df, x=\"CRC\", hue=\"radical\", kde=True, bins=30)\n",
    "    plt.title(\"Distribution of CRC Values by Radicalization Status\")\n",
    "    plt.xlabel(\"Composite Reinforced Centrality (CRC)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    hist_path = os.path.join(event_folder, \"CRC_distribution_radical.png\")\n",
    "    plt.savefig(hist_path)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxplot(x=\"radical\", y=\"CRC\", data=merged_df)\n",
    "    plt.title(\"CRC by Radicalization Status\")\n",
    "    plt.xlabel(\"Radicalization Status (0 = Non-Radicalized, 1 = Radicalized)\")\n",
    "    plt.ylabel(\"Composite Reinforced Centrality (CRC)\")\n",
    "    boxplot_path = os.path.join(event_folder, \"CRC_boxplot_radical.png\")\n",
    "    plt.savefig(boxplot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # --------------------------\n",
    "    # Logistic Regression Analysis\n",
    "    # --------------------------\n",
    "    merged_df[\"intercept\"] = 1.0\n",
    "    logit_model = sm.Logit(merged_df[\"radical\"], merged_df[[\"intercept\", \"CRC\"]])\n",
    "    logit_result = logit_model.fit(disp=False)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Pearson Correlation\n",
    "    # --------------------------\n",
    "    scaler = StandardScaler()\n",
    "    merged_df[['scaled_CRC', 'scaled_rad_score']] = scaler.fit_transform(merged_df[['CRC', 'rad_score']])\n",
    "    correlation = merged_df[\"CRC\"].corr(merged_df[\"radical\"])\n",
    "    \n",
    "    # Save merged DataFrame for further analysis.\n",
    "    merged_df.to_csv(os.path.join(event_folder, \"CRC_radicalization_analysis.csv\"), index=False)\n",
    "    \n",
    "    print(\"Logistic Regression Summary:\")\n",
    "    print(logit_result.summary())\n",
    "    print(\"Pearson correlation between CRC and Radicalization:\", correlation)\n",
    "\n",
    "\n",
    "    # Get the regression coefficients\n",
    "    intercept_value = logit_result.params[\"intercept\"]\n",
    "    crc_coef = logit_result.params[\"CRC\"]\n",
    "    \n",
    "    # Compute quartile values for CRC from the merged DataFrame\n",
    "    quartiles = merged_df[\"CRC\"].quantile([0, 0.25, 0.50, 0.75, 0.90])\n",
    "    print(\"CRC Quartiles:\")\n",
    "    print(quartiles)\n",
    "    \n",
    "    # Compute predicted probabilities at each quartile\n",
    "    def predicted_probability(crc_value, intercept, coef):\n",
    "        log_odds = intercept + coef * crc_value\n",
    "        return 1 / (1 + np.exp(-log_odds))\n",
    "    \n",
    "    pred_probs = quartiles.apply(lambda x: predicted_probability(x, intercept_value, crc_coef))\n",
    "    print(\"\\nPredicted Probabilities at CRC Quartiles:\")\n",
    "    print(pred_probs)\n",
    "    \n",
    "    return merged_df, logit_result, correlation\n",
    "\n",
    "# Example usage:\n",
    "# merged_df, logit_result, corr = analyze_crc(crc_2008, \"data/2008_elections/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a9609307-92cf-444b-ad28-715f8324463a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shared/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def compute_negative_word_distribution(crc, event_folder, authors_perc=0.1, top_k_words=20):\n",
    "    \"\"\"\n",
    "    For the top users by CRC, aggregate their content and compute the frequency distribution of\n",
    "    negative words. Negative words are determined using VADER's lexicon (words with sentiment scores < 0).\n",
    "    \n",
    "    Parameters:\n",
    "        crc (dict): Dictionary mapping each author (str) to their CRC value (float).\n",
    "        event_folder (str): Path to the event folder containing \"network/authors.txt\" and \"cslasl-pre/contents.txt\".\n",
    "        top_n_authors (int): Number of top users (by CRC) to consider.\n",
    "        top_k_words (int): Number of most frequent negative words to return.\n",
    "        \n",
    "    Returns:\n",
    "        neg_word_freq (list): A list of tuples (word, frequency) for the top negative words.\n",
    "        radical_keywords (list): A list of the top_k_words most frequent negative words longer than 4 characters.\n",
    "    \"\"\"\n",
    "    # Paths for authors and contents (adjust according to your folder structure)\n",
    "    authors_file = os.path.join(event_folder, \"network/authors.txt\")\n",
    "    contents_file = os.path.join(event_folder, \"cslasl-pre/contents.txt\")\n",
    "    \n",
    "    # Load authors and contents\n",
    "    with open(authors_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        authors_list = [line.strip() for line in f if line.strip()]\n",
    "    with open(contents_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        contents_list = [line for line in f if line]\n",
    "        \n",
    "    if len(authors_list) != len(contents_list):\n",
    "        raise ValueError(\"The number of authors and contents do not match.\")\n",
    "    \n",
    "    # Create a DataFrame mapping authors to their content (each line corresponds)\n",
    "    data_df = pd.DataFrame({\"author\": authors_list, \"content\": contents_list})\n",
    "    top_n_authors = int(len(authors_list) * authors_perc)\n",
    "    \n",
    "    # Sort authors by CRC in descending order and select the top_n_authors.\n",
    "    sorted_authors = sorted(crc.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_authors = [author for author, value in sorted_authors[:top_n_authors]]\n",
    "    \n",
    "    # Aggregate content for the top authors.\n",
    "    aggregated_text = \" \".join(data_df[data_df[\"author\"].isin(top_authors)][\"content\"].tolist())\n",
    "    \n",
    "    # Preprocess the text: lowercase and remove punctuation/numbers.\n",
    "    aggregated_text = aggregated_text.lower()\n",
    "    aggregated_text = re.sub(r'[^a-z\\s]', ' ', aggregated_text)\n",
    "    \n",
    "    # Tokenize the aggregated text.\n",
    "    tokens = word_tokenize(aggregated_text)\n",
    "    \n",
    "    # Initialize VADER sentiment analyzer and get its lexicon.\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    vader_lexicon = sia.lexicon  # Dictionary mapping word -> sentiment score.\n",
    "    \n",
    "    # Filter tokens: keep only those words that are in the VADER lexicon and have a negative sentiment score.\n",
    "    negative_tokens = [token for token in tokens if token in vader_lexicon and (vader_lexicon[token] < -0.8)]\n",
    "    \n",
    "    # Count the frequency of negative tokens.\n",
    "    freq_counter = Counter(negative_tokens)\n",
    "    \n",
    "    # Get the top_k_words most common negative words.\n",
    "    neg_word_freq = freq_counter.most_common(top_k_words)\n",
    "    \n",
    "    # Extract only words from neg_word_freq that are longer than 4 characters.\n",
    "    radical_keywords = [word for word, freq in neg_word_freq if len(word) > 3]\n",
    "    \n",
    "    return neg_word_freq, radical_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "b2707944-b781-4fa3-9b9f-c350100b4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression(event, authors_perc=0.1, top_k_words=40):\n",
    "    crc_file = os.path.join('data', event, 'CRC_values.pkl')\n",
    "    with open(crc_file, 'rb') as f:\n",
    "        crc_dict = pickle.load(f)\n",
    "    event_folder = f\"data/{event}\"\n",
    "    _, radical_keywords = compute_negative_word_distribution(crc_dict, event_folder, authors_perc=authors_perc, top_k_words=top_k_words)\n",
    "    print(f\"KEYWORDS: {radical_keywords}\")\n",
    "\n",
    "    merged_df, logit_result, corr = analyze_crc(crc_dict, event_folder, radical_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "97859985-61a8-419e-aa36-5a3a38cc7135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2008_elections/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2008 = compute_crc(\"2008_elections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4e0eb325-8ae1-4344-a92a-a7052a7e6232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['lies', 'wrong', 'shit', 'fuck', 'fraud', 'crisis', 'stop', 'stupid', 'hate', 'problem', 'anti', 'terrorist', 'attack', 'racist', 'crap', 'lose', 'poor', 'spammer', 'hell', 'fear', 'lost', 'attacks', 'bullshit', 'worse', 'argument', 'negative', 'racism', 'kill', 'problems', 'damn', 'crazy', 'dead', 'illegal']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:                13184\n",
      "Model:                          Logit   Df Residuals:                    13182\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Wed, 19 Mar 2025   Pseudo R-squ.:                  0.1603\n",
      "Time:                        14:32:51   Log-Likelihood:                -7526.5\n",
      "converged:                       True   LL-Null:                       -8963.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept    -11.0968      0.229    -48.407      0.000     -11.546     -10.647\n",
      "CRC            9.4850      0.201     47.169      0.000       9.091       9.879\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: 0.44779549750448866\n",
      "CRC Quartiles:\n",
      "0.00    1.000421\n",
      "0.25    1.023564\n",
      "0.50    1.112328\n",
      "0.75    1.220484\n",
      "0.90    1.266711\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.166894\n",
      "0.25    0.199681\n",
      "0.50    0.366709\n",
      "0.75    0.617627\n",
      "0.90    0.714625\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2008_elections', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "54e9480c-7b0a-4033-8849-5c8aed43dd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2011_wallstreet/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2011 = compute_crc(\"2011_wallstreet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "0aa1681f-aa8f-442c-9bc7-298c1ab5c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['protest', 'protesters', 'protests', 'protesting', 'problem', 'shit', 'wrong', 'stop', 'fuck', 'arrested', 'violence', 'violent', 'problems', 'stupid', 'anti', 'arrest', 'poor', 'fight', 'bullshit', 'hate', 'argument', 'debt', 'hell', 'blame', 'riot', 'greed', 'illegal', 'disagree', 'angry', 'lack', 'brutality', 'fail', 'fighting', 'worse', 'evil', 'lose']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:                31627\n",
      "Model:                          Logit   Df Residuals:                    31625\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 28 Mar 2025   Pseudo R-squ.:                  0.1585\n",
      "Time:                        08:09:28   Log-Likelihood:                -18447.\n",
      "converged:                       True   LL-Null:                       -21922.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept    -11.5154      0.155    -74.167      0.000     -11.820     -11.211\n",
      "CRC           10.3115      0.139     74.005      0.000      10.038      10.585\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: 0.44766594934388704\n",
      "CRC Quartiles:\n",
      "0.00    1.000175\n",
      "0.25    1.020084\n",
      "0.50    1.100516\n",
      "0.75    1.207077\n",
      "0.90    1.256360\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.231103\n",
      "0.25    0.269570\n",
      "0.50    0.458241\n",
      "0.75    0.717353\n",
      "0.90    0.808385\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2011_wallstreet', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "91b02790-27da-433e-991f-84dfa236b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2016_elections/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2016 = compute_crc(\"2016_elections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "47b6c498-92c1-47d9-87fa-d05f2a5cf971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['fake', 'shit', 'fuck', 'wrong', 'stop', 'lost', 'racist', 'hate', 'problem', 'anti', 'stupid', 'bullshit', 'rapist', 'worse', 'illegal', 'hell', 'lose', 'argument', 'blame', 'rape', 'damn', 'crazy', 'fraud', 'attack', 'conspiracy', 'rigged', 'lies', 'poor', 'lying', 'fucked', 'worst', 'doubt', 'fight', 'propaganda']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               295829\n",
      "Model:                          Logit   Df Residuals:                   295827\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 27 Mar 2025   Pseudo R-squ.:                  0.1762\n",
      "Time:                        21:22:53   Log-Likelihood:            -1.6892e+05\n",
      "converged:                       True   LL-Null:                   -2.0505e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept    -10.4465      0.044   -238.998      0.000     -10.532     -10.361\n",
      "CRC            9.1405      0.038    238.930      0.000       9.065       9.215\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: 0.4738617374766874\n",
      "CRC Quartiles:\n",
      "0.00    1.000019\n",
      "0.25    1.019890\n",
      "0.50    1.131329\n",
      "0.75    1.255559\n",
      "0.90    1.304697\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.213188\n",
      "0.25    0.245235\n",
      "0.50    0.473628\n",
      "0.75    0.736905\n",
      "0.90    0.814434\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2016_elections', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "ca8b2c75-7bb5-463a-bb63-92f51e97f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2017_rally/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2017 = compute_crc(\"2017_rally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "06580db1-8af6-43d2-a3ea-cae0ace6497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['racist', 'shit', 'violence', 'hate', 'fuck', 'racism', 'wrong', 'supremacists', 'stop', 'anti', 'violent', 'problem', 'protest', 'stupid', 'fake', 'bullshit', 'racists', 'attack', 'protesters', 'killed', 'argument', 'blame', 'worse', 'hell', 'fight', 'death', 'crime', 'fascist', 'terrorist', 'kill', 'lost', 'evil', 'murder', 'illegal', 'fire', 'terrorism']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               104125\n",
      "Model:                          Logit   Df Residuals:                   104123\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Wed, 19 Mar 2025   Pseudo R-squ.:                  0.1395\n",
      "Time:                        14:55:40   Log-Likelihood:                -62104.\n",
      "converged:                       True   LL-Null:                       -72174.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -8.4237      0.065   -129.673      0.000      -8.551      -8.296\n",
      "CRC            7.1831      0.055    130.105      0.000       7.075       7.291\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: 0.42671895621178574\n",
      "CRC Quartiles:\n",
      "0.00    1.000053\n",
      "0.25    1.037407\n",
      "0.50    1.167473\n",
      "0.75    1.298559\n",
      "0.90    1.347937\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.224397\n",
      "0.25    0.274501\n",
      "0.50    0.490593\n",
      "0.75    0.711764\n",
      "0.90    0.778795\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2017_rally', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dfa5ae5b-5df9-490b-974e-4e50371c9d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2020_covid19/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2020 = compute_crc(\"2020_covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "c1734b3b-495a-4107-b139-fc96974ef9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['infected', 'death', 'sick', 'shit', 'risk', 'stop', 'fuck', 'worse', 'problem', 'wrong', 'died', 'dead', 'panic', 'fear', 'stupid', 'worst', 'kill', 'severe', 'conspiracy', 'lower', 'negative', 'crisis', 'avoid', 'crazy', 'emergency', 'hell', 'poor', 'lack', 'worried', 'worry', 'alone', 'damn', 'lost', 'fake']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               371104\n",
      "Model:                          Logit   Df Residuals:                   371102\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 20 Mar 2025   Pseudo R-squ.:               4.782e-09\n",
      "Time:                        18:13:29   Log-Likelihood:            -2.5516e+05\n",
      "converged:                       True   LL-Null:                   -2.5516e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.9606\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -0.2102      0.036     -5.791      0.000      -0.281      -0.139\n",
      "CRC           -0.0016      0.032     -0.049      0.961      -0.065       0.062\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: -8.022799916073587e-05\n",
      "CRC Quartiles:\n",
      "0.00    1.000015\n",
      "0.25    1.019614\n",
      "0.50    1.088823\n",
      "0.75    1.196286\n",
      "0.90    1.243851\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.447248\n",
      "0.25    0.447240\n",
      "0.50    0.447213\n",
      "0.75    0.447171\n",
      "0.90    0.447153\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2020_covid19', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "cfcfaf89-9cb5-4cc0-b48a-d52bae505487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRC values saved to: data/2021_riot/CRC_values.pkl\n"
     ]
    }
   ],
   "source": [
    "crc_2021 = compute_crc(\"2021_riot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "0472d5d9-f564-4bc3-b9b5-c2395fa0aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORDS: ['shit', 'fuck', 'stop', 'terrorists', 'violence', 'riot', 'wrong', 'stupid', 'bullshit', 'attack', 'lost', 'hate', 'killed', 'terrorist', 'death', 'riots', 'protests', 'fraud', 'worse', 'conspiracy', 'violent', 'hell', 'crazy', 'arrested', 'protest', 'problem', 'damn', 'died', 'fight', 'lies', 'prison', 'fire', 'murder', 'dead', 'argument', 'crime']\n",
      "Logistic Regression Summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                radical   No. Observations:               254480\n",
      "Model:                          Logit   Df Residuals:                   254478\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Wed, 19 Mar 2025   Pseudo R-squ.:                  0.1206\n",
      "Time:                        17:30:57   Log-Likelihood:            -1.5323e+05\n",
      "converged:                       True   LL-Null:                   -1.7424e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -9.0798      0.047   -195.183      0.000      -9.171      -8.989\n",
      "CRC            7.6493      0.040    191.066      0.000       7.571       7.728\n",
      "==============================================================================\n",
      "Pearson correlation between CRC and Radicalization: 0.3975946092339529\n",
      "CRC Quartiles:\n",
      "0.00    1.000022\n",
      "0.25    1.029349\n",
      "0.50    1.138275\n",
      "0.75    1.256052\n",
      "0.90    1.302537\n",
      "Name: CRC, dtype: float64\n",
      "\n",
      "Predicted Probabilities at CRC Quartiles:\n",
      "0.00    0.193057\n",
      "0.25    0.230421\n",
      "0.50    0.407882\n",
      "0.75    0.629060\n",
      "0.90    0.707602\n",
      "Name: CRC, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "get_regression('2021_riot', authors_perc=0.1, top_k_words=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf25c70-20d7-4091-b61d-0e901c8d3158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
